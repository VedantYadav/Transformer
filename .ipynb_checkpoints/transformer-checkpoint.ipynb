{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arch units\n",
    "## Self Attention Unit\n",
    "## Multi Head Attention\n",
    "## Encode Decode Unit\n",
    "## Norm + Residual Layer\n",
    "## Feed Forward\n",
    "## Input Positional Encoding \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters from paper\n",
    "N = 4 # 6\n",
    "d_model = 512\n",
    "h = 8\n",
    "d_k = d_v = d_model//h\n",
    "d_ff = 2048 # 128\n",
    "vocab_size = 10000\n",
    "max_seq = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        self.W_O = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask, dropout=None):\n",
    "        b, q_seq, _ = Q.size()\n",
    "        b, k_seq, _ = K.size()\n",
    "        query = self.W_Q(Q).view(b, q_seq, h, d_k) # view (b, q_seq, h, d)\n",
    "        key = self.W_K(K).view(b, k_seq, h, d_k) # view (b, k_seq, h, d)\n",
    "        value  = self.W_V(V).view(b, k_seq, h, d_k) # view (b, k_seq, h, d)\n",
    "        \n",
    "        query = query.transpose(1, 2).contiguous() # view (b, h, q_seq, d)\n",
    "        key = key.transpose(1, 2).contiguous() # view (b, h, k_seq, d)\n",
    "        value = value.transpose(1, 2).contiguous() # view (b, h, k_seq, d)\n",
    "        \n",
    "        qk = query.matmul(key.transpose(-2,-1))\n",
    "        scale_qk = qk/(math.sqrt(d_k)) # shape (b, h, q_seq, k_seq)\n",
    "        \n",
    "        if mask is not None: # mask size (b, 1, k_seq)\n",
    "            mask = mask.unsqueeze(1) # mask size (b, 1, 1, k_seq)\n",
    "            scale_qk = scale_qk.masked_fill(mask==0, 1e-9)\n",
    "        \n",
    "        softmax_qk = nn.functional.softmax(scale_qk, dim=-1) # (b, h, q_seq, k_seq)\n",
    "        if dropout is not None:\n",
    "            softmax_qk = self.dropout(softmax_qk)\n",
    "        weighted_value = softmax_qk.matmul(value) # (b, h, q_seq, d)\n",
    "        return self.W_O(weighted_value.transpose(2,1).contiguous().view(b, q_seq, h*d_k)) # (b, h, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_mod=d_model):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.d_mod = d_mod\n",
    "        # https://stackoverflow.com/questions/39095252/fail-to-implement-layer-normalization-with-keras\n",
    "        # https://stackoverflow.com/questions/50935345/understanding-torch-nn-parameter\n",
    "        self.alpha = nn.Parameter(torch.ones(d_mod))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_mod))\n",
    "    def forward(self, x, eps=1e-6):\n",
    "        u = x.mean(-1, keepdim=True)\n",
    "        sigma = x.std(-1, keepdim=True)\n",
    "        return self.alpha * (x - u)/(sigma + eps) + self.beta\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCell(nn.Module):\n",
    "    def __init__(self, dropout=0.1, Adropout=0.1):\n",
    "        super(EncoderCell, self).__init__()\n",
    "        self.attn = MultiHeadAttention(Adropout)\n",
    "        self.norm_1 = LayerNorm()\n",
    "        self.pff = nn.Sequential(\n",
    "                    nn.Linear(d_model, d_ff),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                    nn.Linear(d_ff, d_model))\n",
    "        self.norm_2 = LayerNorm()\n",
    "    \n",
    "    def forward(self, x, src_mask=None):\n",
    "        x_norm_1 = self.norm_1(x + self.attn(x, x, x, src_mask))  # Layer 1\n",
    "        return self.norm_2(x_norm_1 + self.pff(x_norm_1)) # Layer 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderCell(nn.Module):\n",
    "    def __init__(self, dropout=0.1, Ddropout=0.1):\n",
    "        super(DecoderCell, self).__init__()\n",
    "        self.attn = MultiHeadAttention(Ddropout)\n",
    "        self.norm_1 = LayerNorm()\n",
    "        self.attn = MultiHeadAttention(Ddropout)\n",
    "        self.norm_2 = LayerNorm()\n",
    "        self.pff = nn.Sequential(\n",
    "                    nn.Linear(d_model, d_ff),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                    nn.Linear(d_ff, d_model))\n",
    "        self.norm_3 = LayerNorm()\n",
    "        \n",
    "    def forward(self, x, enc, src_mask=None, trg_mask=None):\n",
    "        x_norm_1 = self.norm_1(x + self.attn(x, x, x, trg_mask))\n",
    "        x_norm_2 = self.norm_2(x_norm_1 + self.attn(x_norm_1, enc, enc, src_mask))\n",
    "        return self.norm_3(x_norm_2 + self.pff(x_norm_2)) # (b, seq, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderStack(nn.Module):\n",
    "    def __init__(self, N, Edropout=0.1, Adropout=0.1):\n",
    "        super(EncoderStack, self).__init__()\n",
    "        self.N = N\n",
    "        self.Edropout = Edropout\n",
    "        self.Adropout = Adropout\n",
    "        self.encoders = nn.ModuleList([EncoderCell(self.Edropout, self.Adropout) \\\n",
    "                                       for _ in range(self.N)])\n",
    " \n",
    "    def forward(self, x, src_mask):\n",
    "        for enc in self.encoders:\n",
    "            x = enc(x, src_mask)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderStack(nn.Module):\n",
    "    def __init__(self, N, Ddropout=0.1, Adropout=0.1):\n",
    "        super(DecoderStack, self).__init__()\n",
    "        self.N = N\n",
    "        self.Ddropout = Ddropout\n",
    "        self.Adropout = Adropout\n",
    "        self.decoders = nn.ModuleList([DecoderCell(self.Ddropout, self.Adropout) \\\n",
    "                                       for _ in range(self.N)])\n",
    "        \n",
    "    def forward(self, x, enc, src_mask, trg_mask):\n",
    "        for decdr in self.decoders:\n",
    "            x = decdr(x, enc, src_mask, trg_mask)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dpout=0.1, max_seq=max_seq):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dpout)\n",
    "        \n",
    "        pe_matx = torch.zeros(max_seq, d_model, requires_grad=False)\n",
    "        position = torch.arange(0, max_seq, dtype=torch.float).unsqueeze(-1)\n",
    "        w_t = torch.exp(torch.arange(0, d_model, 2).float() * -math.log(10000)/d_model)\n",
    "        val = position * w_t\n",
    "        pe_matx[:, 0::2] = torch.sin(val)\n",
    "        pe_matx[:, 1::2] = torch.cos(val)\n",
    "        pe_matx = pe_matx.unsqueeze(1)\n",
    "        self.register_buffer(\"pe_matx\", pe_matx)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x - (batch, seq, emb), pe_matrix - (max_seq, 1, d_model)\n",
    "        x += self.pe_matx[:x.size(0), :]\n",
    "        return(self.dropout(x))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, embedd = True, x_vocab=vocab_size, y_vocab=vocab_size, dropout=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "#         self.W_in = nn.Linear(word_emb_dim, d_model)\n",
    "        self.encoderStack = EncoderStack(N, Adropout=dropout)\n",
    "        self.decoderStack = DecoderStack(N, Adropout=dropout)\n",
    "        # https://stats.stackexchange.com/questions/392213/understand-the-output-layer-of-transformer\n",
    "        self.W_out = nn.Linear(d_model, y_vocab)\n",
    "        self.embedd = embedd\n",
    "        if self.embedd:\n",
    "            embed_x = EmbeddingLayer(x_vocab, d_model)\n",
    "            embed_y = EmbeddingLayer(y_vocab, d_model)\n",
    "            pe_x = PositionalEncoding(d_model)\n",
    "            pe_y = copy.deepcopy(pe_x)\n",
    "            self.enc_x = nn.Sequential(embed_x, pe_x)\n",
    "            self.enc_y = nn.Sequential(embed_y, pe_y)\n",
    "        \n",
    "    def forward(self, inp_x, inp_y, src_mask, trg_mask, sftmx=True):\n",
    "        \n",
    "        dec_x = self.decoder(inp_y, self.encoder(inp_x, src_mask), \n",
    "                             src_mask, trg_mask)\n",
    "        \n",
    "        if sftmx:\n",
    "            return nn.functional.log_softmax(self.W_out(dec_x), dim=-1)\n",
    "        return self.W_out(dec_x)\n",
    "    \n",
    "    def encoder(self, inp_x, src_mask):\n",
    "        if self.embedd:\n",
    "            inp_x = self.enc_x(inp_x)\n",
    "        return self.encoderStack(inp_x, src_mask)\n",
    "    \n",
    "    def decoder(self, inp_y, enc_x, src_mask, trg_mask):\n",
    "        if self.embedd:\n",
    "            inp_y = self.enc_y(inp_y)\n",
    "        return self.decoderStack(inp_y, enc_x, src_mask, trg_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.reddit.com/r/MachineLearning/comments/bjgpt2/d_confused_about_using_masking_in_transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
    "class Batch:\n",
    "    def __init__(self, src, trg=None, pad=0): # size src, trg (b, seq)\n",
    "        self.src = src \n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        if trg is not None:\n",
    "            self.trg = trg[:,:-1] # size (b,0:seq-1)\n",
    "            self.trg_y = trg[:,1:] # size (b,1:seq)\n",
    "            self.trg_mask = self.std_mask(self.trg, pad)\n",
    "            self.ntokens = (self.trg_y != pad).data.sum() # size (1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def std_mask(tgt, pad):\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2) # size (b, 1, seq)\n",
    "        mask = torch.from_numpy(np.triu(np.ones((1,tgt.shape[-1],tgt.shape[-1])), k=1).astype('uint8')) == 0\n",
    "        return tgt_mask & mask # size (b, 1, seq) * (1, seq, seq) -> (b, seq, seq) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/pytorch/issues/7455    \n",
    "\n",
    "def labelSmoothingLoss(x, y, trg_pad_id, epsilon=0.0, cls=2, d=-1):\n",
    "    # concat x, y batch as index_fill_ don't support vector dim > 1\n",
    "    x=x.contiguous().view(-1, x.size(-1))\n",
    "    y=y.contiguous().view(-1)\n",
    "    \n",
    "    x_ = x.data.clone()\n",
    "    x_.fill_(epsilon / (x_.size(-1) - cls))\n",
    "    x_.scatter_(d, y.data.unsqueeze(-1), (1 - epsilon))\n",
    "    x_[:, trg_pad_id] = 0\n",
    "    mask = torch.where(target==p)[0]\n",
    "    if mask.dim() > 0:\n",
    "        x_.index_fill_(0, mask.squeeze(), 0.0)\n",
    "\n",
    "    return torch.mean(torch.sum(-x_*x), dim=d) # x_ is true distribution and x is prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "            (self.model_size ** (-0.5) *\n",
    "            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://nlp.seas.harvard.edu/2018/04/03/attention.html#synthetic-data\n",
    "\n",
    "def run_epoch(data_itr, model, opt, pad_id):\n",
    "    start = time.time()\n",
    "    total_token = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    \n",
    "    for i, batch in enumerate(data_itr):\n",
    "        opt.optimizer.zero_grad()\n",
    "#         opt.zero_grad()\n",
    "    \n",
    "        outp = model(batch.src, batch.trg, batch.src_mask, batch.trg_mask)\n",
    "        loss = labelSmoothingLoss(outp, batch.trg_y, pad_id)\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "\n",
    "        opt.step()\n",
    "\n",
    "        total_loss+=loss\n",
    "        total_token+=batch.ntokens\n",
    "        tokens+=batch.ntokens\n",
    "        \n",
    "        if i%50 == 1:\n",
    "            elapsed = time.time() - start\n",
    "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
    "                    (i, loss / batch.ntokens, tokens / elapsed))\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "    return total_loss/total_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://nlp.seas.harvard.edu/2018/04/03/attention.html#synthetic-data\n",
    "# def data_generation(V, batch, nbatches):\n",
    "#     for i in range(nbatches):\n",
    "#         data = torch.randint(1, V, size=(batch, 10))\n",
    "#         data[:, 0] = 1\n",
    "#         src = data.clone().detach()\n",
    "#         trg = data.clone().detach()\n",
    "#         yield Batch(src, trg, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Transformer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-9)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "# https://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
    "# model_opt = NoamOpt(d_model, 1, 400,\n",
    "#         torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init all parameters as we used deepcopy to save computation tym\n",
    "# for p in model.parameters():\n",
    "#     if p.dim() > 1:\n",
    "#         nn.init.xavier_uniform_(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(10):\n",
    "#     model.train()\n",
    "#     run_epoch(data_generation(vocab_size, 30, 20), model, model_opt)\n",
    "#     model.eval()\n",
    "#     print(run_epoch(data_generation(vocab_size, 30, 5), model, model_opt).data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data, datasets\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy_de = spacy.load(\"de\")\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "spacy_de = spacy.load('de_core_news_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STR_WORD = '<start>'\n",
    "END_WORD = '<end>'\n",
    "MAX_LEN = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = data.Field(tokenize=tokenize_de)\n",
    "TGT = data.Field(tokenize=tokenize_en, init_token=STR_WORD, eos_token=END_WORD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validation, test = datasets.IWSLT.splits(exts=(\".de\", \".en\"), fields=(SRC, TGT),\n",
    "            filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and len(vars(x)['trg']) <= MAX_LEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_FREQ = 2\n",
    "SRC.build_vocab(train.src, min_freq=MIN_FREQ)\n",
    "TGT.build_vocab(train.trg, min_freq=MIN_FREQ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/text/_modules/torchtext/data/iterator.html\n",
    "\n",
    "class IteratorWrapper(data.Iterator):\n",
    "    def create_batches(self):\n",
    "        if self.train:\n",
    "            self.batches = data.pool(data=self.data(), \n",
    "                                     batch_size=self.batch_size, \n",
    "                                     key=self.sort_key, \n",
    "                                     batch_size_fn=self.batch_size_fn, \n",
    "                                     random_shuffler=self.random_shuffler, \n",
    "                                     shuffle=True, \n",
    "                                     sort_within_batch=True)\n",
    "        else:\n",
    "            self.batches = data.batch(data=self.data(),\n",
    "                                       batch_size=self.batch_size,\n",
    "                                       batch_size_fn=self.batch_size_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
    "global max_src_in_batch, max_tgt_in_batch\n",
    "def batch_size_fn(new, count, sofar):\n",
    "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
    "    global max_src_in_batch, max_tgt_in_batch\n",
    "    if count == 1:\n",
    "        max_src_in_batch = 0\n",
    "        max_tgt_in_batch = 0\n",
    "    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
    "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
    "    src_elements = count * max_src_in_batch\n",
    "    tgt_elements = count * max_tgt_in_batch\n",
    "    return max(src_elements, tgt_elements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_wrapper(batch, pad_id):\n",
    "    return Batch(batch.src.transpose(1,0), batch.trg.transpose(1,0), pad_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_id = TGT.vocab.stoi['<pad>']\n",
    "# model = Transformer()\n",
    "NO_TOKEN = 100 # it's a number of token\n",
    "train_itr = IteratorWrapper(train,\n",
    "               batch_size=NO_TOKEN,\n",
    "               repeat=False,\n",
    "               sort_key=lambda x: (len(x.src), len(x.trg)),\n",
    "               batch_size_fn=batch_size_fn,\n",
    "               train=True)\n",
    "\n",
    "val_itr = IteratorWrapper(validation,\n",
    "               batch_size=NO_TOKEN,\n",
    "               repeat=False,\n",
    "               sort_key=lambda x: (len(x.src), len(x.trg)),\n",
    "               batch_size_fn=batch_size_fn,\n",
    "               train=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(x_vocab=len(SRC.vocab), y_vocab=len(TGT.vocab))\n",
    "model_opt = NoamOpt(d_model, 1, 400,\n",
    "        torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(1):\n",
    "# model.train()\n",
    "# run_epoch((batch_wrapper(t_b, pad_id) for t_b in train_itr), model, model_opt, pad_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://mc.ai/how-to-use-torchtext-for-neural-machine-translation-plus-hack-to-make-it-5x-faster/\n",
    "# https://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
