{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arch units\n",
    "## Self Attention Unit\n",
    "## Multi Head Attention\n",
    "## Encode Decode Unit\n",
    "## Norm + Residual Layer\n",
    "## Feed Forward\n",
    "## Input Positional Encoding \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters from paper\n",
    "word_emb_dim = 50\n",
    "N = 6\n",
    "d_model = 32 # 512\n",
    "h = 4 # 8\n",
    "d_k = d_v = d_model//h\n",
    "d_ff = 128 # 2048\n",
    "vocab_size = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        assert Q.shape[-1] == K.shape[-1] == V.shape[-1]\n",
    "        qk = torch.bmm(Q, K.transpose(-2, -1)) # Q & K size (b, seq, d_k)\n",
    "        scale_qk = qk/math.sqrt(d_k) # size (b, seq, seq)\n",
    "        if mask:\n",
    "            scale_qk.masked_fill(mask, 0) # where mask is True replaced with 0\n",
    "        softmax_qk = nn.functional.softmax(scale_qk, dim=-1) # size (b, seq, seq)\n",
    "        return torch.bmm(softmax_qk, V) # size (b, seq, d_k)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k)\n",
    "        self.W_K = nn.Linear(d_model, d_k)\n",
    "        self.W_V = nn.Linear(d_model, d_k)\n",
    "        self.attn = Attention()\n",
    "        self.W_O = nn.Linear(h*d_v, d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        head = None\n",
    "        for _ in range(h):\n",
    "            attn_head = self.attn(self.W_Q(Q), self.W_K(K), self.W_V(V), mask=None)\n",
    "            if head != None:\n",
    "                head = torch.cat((head, attn_head), dim=-1)\n",
    "            else:\n",
    "                head = attn_head\n",
    "        # head size (b, seq, d_k*h)\n",
    "        return self.W_O(head) # size (b, seq, d_model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_mod=d_model):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.d_mod = d_mod\n",
    "        # https://stackoverflow.com/questions/39095252/fail-to-implement-layer-normalization-with-keras\n",
    "        # https://stackoverflow.com/questions/50935345/understanding-torch-nn-parameter\n",
    "        self.alpha = nn.Parameter(torch.ones(d_mod))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_mod))\n",
    "    def forward(self, x, eps=1e-6):\n",
    "        u = x.mean(-1, keepdim=True)\n",
    "        sigma = x.std(-1, keepdim=True)\n",
    "        return self.alpha * (x - u)/(1/(sigma + eps)) + self.beta\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCell(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderCell, self).__init__()\n",
    "        self.attn = MultiHeadAttention()\n",
    "        self.norm_1 = LayerNorm()\n",
    "        self.pff = nn.Sequential(\n",
    "                    nn.Linear(d_model, d_ff),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(d_ff, d_model))\n",
    "        self.norm_2 = LayerNorm()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_norm_1 = self.norm_1(x + self.attn(x, x, x))  # Layer 1\n",
    "        return self.norm_2(x_norm_1 + self.pff(x_norm_1)) # Layer 2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderCell(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecoderCell, self).__init__()\n",
    "        self.attn = MultiHeadAttention()\n",
    "        self.norm_1 = LayerNorm()\n",
    "        self.attn = MultiHeadAttention()\n",
    "        self.norm_2 = LayerNorm()\n",
    "        self.pff = nn.Sequential(\n",
    "                    nn.Linear(d_model, d_ff),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(d_ff, d_model))\n",
    "        self.norm_3 = LayerNorm()\n",
    "        \n",
    "    def forward(self, x, enc, src_mask=None, trg_mask=None):\n",
    "        x_norm_1 = self.norm_1(x + self.attn(x, x, x, trg_mask))\n",
    "        x_norm_2 = self.norm_2(x_norm_1 + self.attn(x_norm_1, enc, enc, src_mask))\n",
    "        return self.norm_3(x_norm_2 + self.pff(x_norm_2)) # (b, seq, d_model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.W_in = nn.Linear(word_emb_dim, d_model)\n",
    "        # based on N values\n",
    "        self.encoder_unit = EncoderCell()\n",
    "        self.decoder_unit = DecoderCell()\n",
    "        # https://stats.stackexchange.com/questions/392213/understand-the-output-layer-of-transformer\n",
    "        self.W_out = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, inp_x, inp_y):\n",
    "        inp_x, inp_y = inp_x/math.sqrt(d_model), inp_y/math.sqrt(d_model)\n",
    "        inp_x, inp_y = self.W_in(inp_x), self.W_in(inp_y) # (b, seq, word_embedding) -> (b, seq, d_model)\n",
    "        enc_x = self.encoder_unit(inp_x)\n",
    "        dec_x = self.decoder_unit(inp_y, enc_x)\n",
    "        return self.W_out(dec_x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(5,4,word_emb_dim)\n",
    "b = torch.rand(5,3,word_emb_dim)\n",
    "t = Transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = t(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 100])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.reddit.com/r/MachineLearning/comments/bjgpt2/d_confused_about_using_masking_in_transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8239, 0.6005, 0.5948],\n",
      "        [0.5414, 0.9904, 0.6901],\n",
      "        [0.7687, 0.7560, 0.6091],\n",
      "        [0.8969, 0.8435, 0.7998]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.8239, 0.6005, 0.5948]],\n",
       "\n",
       "        [[0.5414, 0.9904, 0.6901]],\n",
       "\n",
       "        [[0.7687, 0.7560, 0.6091]],\n",
       "\n",
       "        [[0.8969, 0.8435, 0.7998]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.rand(4,3)\n",
    "print(c)\n",
    "c.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2774, 0.3314, 0.7153],\n",
       "        [0.4680, 0.3764, 0.4510],\n",
       "        [0.3530, 0.0466, 0.2651],\n",
       "        [0.4527, 0.1481, 0.5217]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.view(-1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = torch.randint(0,4,(3,4,5))\n",
    "t = torch.randint(0,4,(3,4,5))\n",
    "p = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (s != p).unsqueeze(-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 1, 3, 1],\n",
       "         [0, 3, 2, 1, 2],\n",
       "         [1, 0, 1, 0, 2],\n",
       "         [1, 0, 1, 3, 1]],\n",
       "\n",
       "        [[2, 3, 1, 3, 1],\n",
       "         [2, 0, 1, 2, 1],\n",
       "         [1, 2, 3, 1, 0],\n",
       "         [3, 1, 0, 2, 0]],\n",
       "\n",
       "        [[0, 1, 2, 3, 3],\n",
       "         [1, 3, 1, 3, 0],\n",
       "         [2, 3, 1, 1, 0],\n",
       "         [2, 0, 1, 3, 3]]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_1 = t[:, :-1]\n",
    "t_y = t[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 1, 3, 1],\n",
       "         [0, 3, 2, 1, 2],\n",
       "         [1, 0, 1, 0, 2]],\n",
       "\n",
       "        [[2, 3, 1, 3, 1],\n",
       "         [2, 0, 1, 2, 1],\n",
       "         [1, 2, 3, 1, 0]],\n",
       "\n",
       "        [[0, 1, 2, 3, 3],\n",
       "         [1, 3, 1, 3, 0],\n",
       "         [2, 3, 1, 1, 0]]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 3, 2, 1, 2],\n",
       "         [1, 0, 1, 0, 2],\n",
       "         [1, 0, 1, 3, 1]],\n",
       "\n",
       "        [[2, 0, 1, 2, 1],\n",
       "         [1, 2, 3, 1, 0],\n",
       "         [3, 1, 0, 2, 0]],\n",
       "\n",
       "        [[1, 3, 1, 3, 0],\n",
       "         [2, 3, 1, 1, 0],\n",
       "         [2, 0, 1, 3, 3]]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vedantyadav/opt/anaconda3/envs/nmt/lib/python3.5/site-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "t_mask = make_std_mask(t_1, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True, False, False],\n",
       "         [ True,  True, False],\n",
       "         [ True,  True,  True]]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
    "class Batch:\n",
    "    def __init__(self, src, trg=None, pad=0):\n",
    "        self.src = src\n",
    "        if trg is not None:\n",
    "            self.trg = trg[:,:-1]\n",
    "            self.trg_y = trg[:,1:]\n",
    "#             self.trg_mask = \n",
    "    \n",
    "    @staticmethod\n",
    "    def std_mask(tgt, pad):\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & self.subsequent_mask(tgt, pad)\n",
    "         \n",
    "    def subsequent_mask(self.size):\n",
    "        return torch.from_numpy(np.triu(np.ones((1,size,size)), k=1).astype('uint8')) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_q = (t_1 != p).unsqueeze(-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    return torch.from_numpy(np.triu(np.ones((1,size,size)), k=1).astype('uint8')) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 1, 5])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 5])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsequent_mask(t_q.shape[-1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ True,  True,  True,  True,  True]],\n",
       "\n",
       "         [[False,  True,  True,  True,  True]],\n",
       "\n",
       "         [[ True, False,  True, False,  True]]],\n",
       "\n",
       "\n",
       "        [[[ True,  True,  True,  True,  True]],\n",
       "\n",
       "         [[ True, False,  True,  True,  True]],\n",
       "\n",
       "         [[ True,  True,  True,  True, False]]],\n",
       "\n",
       "\n",
       "        [[[False,  True,  True,  True,  True]],\n",
       "\n",
       "         [[ True,  True,  True,  True, False]],\n",
       "\n",
       "         [[ True,  True,  True,  True, False]]]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 1, 3, 1],\n",
       "         [0, 3, 2, 1, 2],\n",
       "         [1, 0, 1, 0, 2],\n",
       "         [1, 0, 1, 3, 1]],\n",
       "\n",
       "        [[2, 3, 1, 3, 1],\n",
       "         [2, 0, 1, 2, 1],\n",
       "         [1, 2, 3, 1, 0],\n",
       "         [3, 1, 0, 2, 0]],\n",
       "\n",
       "        [[0, 1, 2, 3, 3],\n",
       "         [1, 3, 1, 3, 0],\n",
       "         [2, 3, 1, 1, 0],\n",
       "         [2, 0, 1, 3, 3]]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
