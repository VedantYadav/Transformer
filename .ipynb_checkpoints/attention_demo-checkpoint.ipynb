{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arch units\n",
    "## Self Attention Unit\n",
    "## Multi Head Attention\n",
    "## Encode Decode Unit\n",
    "## Norm + Residual Layer\n",
    "## Feed Forward\n",
    "## Input Positional Encoding \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters from paper\n",
    "N = 2 # 6\n",
    "d_model = 512\n",
    "h = 8\n",
    "d_k = d_v = d_model//h\n",
    "d_ff = 2048 #128\n",
    "vocab_size = 11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        self.W_O = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask):\n",
    "        b, q_seq, _ = Q.size()\n",
    "        b, k_seq, _ = K.size()\n",
    "        query = self.W_Q(Q).view(b, q_seq, h, d_k) # view (b, q_seq, h, d)\n",
    "        key = self.W_K(K).view(b, k_seq, h, d_k) # view (b, k_seq, h, d)\n",
    "        value  = self.W_V(V).view(b, k_seq, h, d_k) # view (b, k_seq, h, d)\n",
    "        \n",
    "        query = query.transpose(1, 2).contiguous() # view (b, h, q_seq, d)\n",
    "        key = key.transpose(1, 2).contiguous() # view (b, h, k_seq, d)\n",
    "        value = value.transpose(1, 2).contiguous() # view (b, h, k_seq, d)\n",
    "        \n",
    "        qk = query.matmul(key.transpose(-2,-1))\n",
    "        scale_qk = qk/(math.sqrt(d_k)) # shape (b, h, q_seq, k_seq)\n",
    "        \n",
    "        if mask is not None: # mask size (b, 1, k_seq)\n",
    "            mask = mask.unsqueeze(1) # mask size (b, 1, 1, k_seq)\n",
    "            scale_qk = scale_qk.masked_fill(mask==0, 1e-9)\n",
    "        \n",
    "        softmax_qk = nn.functional.softmax(scale_qk, dim=-1) # (b, h, q_seq, k_seq)\n",
    "        weighted_value = softmax_qk.matmul(value) # (b, h, q_seq, d)\n",
    "        return self.W_O(weighted_value.transpose(2,1).contiguous().view(b, q_seq, h*d_k)) # (b, h, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_mod=d_model):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.d_mod = d_mod\n",
    "        # https://stackoverflow.com/questions/39095252/fail-to-implement-layer-normalization-with-keras\n",
    "        # https://stackoverflow.com/questions/50935345/understanding-torch-nn-parameter\n",
    "        self.alpha = nn.Parameter(torch.ones(d_mod))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_mod))\n",
    "    def forward(self, x, eps=1e-6):\n",
    "        u = x.mean(-1, keepdim=True)\n",
    "        sigma = x.std(-1, keepdim=True)\n",
    "        return self.alpha * (x - u)/(sigma + eps) + self.beta\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCell(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderCell, self).__init__()\n",
    "        self.attn = MultiHeadAttention()\n",
    "        self.norm_1 = LayerNorm()\n",
    "        self.pff = nn.Sequential(\n",
    "                    nn.Linear(d_model, d_ff),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(d_ff, d_model))\n",
    "        self.norm_2 = LayerNorm()\n",
    "    \n",
    "    def forward(self, x, src_mask=None):\n",
    "        x_norm_1 = self.norm_1(x + self.attn(x, x, x, src_mask))  # Layer 1\n",
    "        return self.norm_2(x_norm_1 + self.pff(x_norm_1)) # Layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderCell(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecoderCell, self).__init__()\n",
    "        self.attn = MultiHeadAttention()\n",
    "        self.norm_1 = LayerNorm()\n",
    "        self.attn = MultiHeadAttention()\n",
    "        self.norm_2 = LayerNorm()\n",
    "        self.pff = nn.Sequential(\n",
    "                    nn.Linear(d_model, d_ff),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(d_ff, d_model))\n",
    "        self.norm_3 = LayerNorm()\n",
    "        \n",
    "    def forward(self, x, enc, src_mask=None, trg_mask=None):\n",
    "        x_norm_1 = self.norm_1(x + self.attn(x, x, x, trg_mask))\n",
    "        x_norm_2 = self.norm_2(x_norm_1 + self.attn(x_norm_1, enc, enc, src_mask))\n",
    "        return self.norm_3(x_norm_2 + self.pff(x_norm_2)) # (b, seq, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderStack(nn.Module):\n",
    "    def __init__(self, N):\n",
    "        super(EncoderStack, self).__init__()\n",
    "        self.N = N\n",
    "        \n",
    "    def forward(self, x, src_mask):\n",
    "        cell = EncoderCell()\n",
    "        encoders = nn.ModuleList([copy.deepcopy(cell) for _ in range(self.N)])\n",
    "        \n",
    "        for enc in encoders:\n",
    "            x = enc(x, src_mask)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderStack(nn.Module):\n",
    "    def __init__(self, N):\n",
    "        super(DecoderStack, self).__init__()\n",
    "        self.N = N\n",
    "    \n",
    "    def forward(self, x, enc, src_mask, trg_mask):\n",
    "        cell = DecoderCell()\n",
    "        decoders = nn.ModuleList([copy.deepcopy(cell) for _ in range(self.N)])\n",
    "        for decdr in decoders:\n",
    "            x = decdr(x, enc, src_mask, trg_mask)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dpout=0.1, max_seq=50):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dpout)\n",
    "        \n",
    "        pe_matx = torch.zeros(max_seq, d_model)\n",
    "        position = torch.arange(0, max_seq, dtype=torch.float).unsqueeze(-1)\n",
    "        w_t = torch.exp(torch.arange(0, d_model, 2).float() * -math.log(10000)/d_model)\n",
    "        val = position * w_t\n",
    "        pe_matx[:, 0::2] = torch.sin(val)\n",
    "        pe_matx[:, 1::2] = torch.cos(val)\n",
    "        pe_matx = pe_matx.unsqueeze(1)\n",
    "        self.register_buffer(\"pe_matx\", pe_matx)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x - (batch, seq, emb), pe_matrix - (max_seq, 1, d_model)\n",
    "        x += self.pe_matx[:x.size(0), :]\n",
    "        return(self.dropout(x))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, embedd = True, log_softmx=True):\n",
    "        super(Transformer, self).__init__()\n",
    "#         self.W_in = nn.Linear(word_emb_dim, d_model)\n",
    "        self.encoder = EncoderStack(N)\n",
    "        self.decoder = DecoderStack(N)\n",
    "        # https://stats.stackexchange.com/questions/392213/understand-the-output-layer-of-transformer\n",
    "        self.W_out = nn.Linear(d_model, vocab_size)\n",
    "        self.sftmx = log_softmx\n",
    "        self.embedd = embedd\n",
    "        if self.embedd:\n",
    "            embed_x = EmbeddingLayer(vocab_size, d_model)\n",
    "            embed_y = EmbeddingLayer(vocab_size, d_model)\n",
    "            pe_x = PositionalEncoding(d_model)\n",
    "            pe_y = copy.deepcopy(pe_x)\n",
    "            self.enc_x = nn.Sequential(embed_x, pe_x)\n",
    "            self.enc_y = nn.Sequential(embed_y, pe_y)\n",
    "        \n",
    "    def forward(self, inp_x, inp_y, src_mask, trg_mask):\n",
    "        if self.embedd:\n",
    "            inp_x, inp_y = self.enc_x(inp_x), self.enc_y(inp_y)\n",
    "        enc_x = self.encoder(inp_x, src_mask)\n",
    "        dec_x = self.decoder(inp_y, enc_x, src_mask, trg_mask)\n",
    "        if self.sftmx:\n",
    "            return nn.functional.log_softmax(self.W_out(dec_x), dim=-1)\n",
    "        return self.W_out(dec_x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.reddit.com/r/MachineLearning/comments/bjgpt2/d_confused_about_using_masking_in_transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
    "class Batch:\n",
    "    def __init__(self, src, trg=None, pad=0): # size src, trg (b, seq)\n",
    "        self.src = src \n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        if trg is not None:\n",
    "            self.trg = trg[:,:-1] # size (b,0:seq-1)\n",
    "            self.trg_y = trg[:,1:] # size (b,1:seq)\n",
    "            self.trg_mask = self.std_mask(self.trg, pad)\n",
    "            self.ntokens = (self.trg_y != pad).data.sum() # size (1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def std_mask(tgt, pad):\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2) # size (b, 1, seq)\n",
    "        mask = torch.from_numpy(np.triu(np.ones((1,tgt.shape[-1],tgt.shape[-1])), k=1).astype('uint8')) == 0\n",
    "        return tgt_mask & mask # size (b, 1, seq) * (1, seq, seq) -> (b, seq, seq) \n",
    "    \n",
    "#     @staticmethod\n",
    "#     def subsequent_mask(size):\n",
    "#         return torch.from_numpy(np.triu(np.ones((1,size,size)), k=1).astype('uint8')) == 0 # size (1, seq, seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vedantyadav/opt/anaconda3/envs/nmt/lib/python3.5/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/pytorch/pytorch/issues/7455    \n",
    "# kldivLoss = nn.KLDivLoss(size_average=False)\n",
    "\n",
    "def labelSmoothingLoss(x, y, epsilon, padding_value=0, cls=1, d=-1):\n",
    "    # concat x, y batch as index_fill_ don't support vector dim > 1\n",
    "#     x = x.view(-1, x.size(-1))    \n",
    "    x=x.contiguous().view(-1, x.size(-1))\n",
    "    y=y.contiguous().view(-1)\n",
    "    \n",
    "    x_ = x.data.clone()\n",
    "    x_.fill_(epsilon / (x_.size(-1) - cls))\n",
    "    x_.scatter_(d, y.data.unsqueeze(-1), (1 - epsilon))\n",
    "    x_[:, padding_value] = 0\n",
    "    mask = torch.nonzero(y.data == padding_value)\n",
    "    if mask.dim() > 0:\n",
    "        x_.index_fill_(0, mask.squeeze(), 0.0)\n",
    "    return torch.mean(torch.sum(-x_*x), dim=d) # x_ is true distribution and x is prediction\n",
    "#     return kldivLoss(x, copy.deepcopy(x_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init all parameters as we used deepcopy to save computation tym\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://nlp.seas.harvard.edu/2018/04/03/attention.html#synthetic-data\n",
    "def data_generation(V, batch, nbatches):\n",
    "    for i in range(nbatches):\n",
    "        data = torch.randint(1, V, size=(batch, 10))\n",
    "        data[:, 0] = 1\n",
    "        src = data.clone().detach()\n",
    "        trg = data.clone().detach()\n",
    "        yield Batch(src, trg, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://nlp.seas.harvard.edu/2018/04/03/attention.html#synthetic-data\n",
    "def run_epoch(data_itr, model, optimizer):\n",
    "    start = time.time()\n",
    "    total_token = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    \n",
    "    for i, batch in enumerate(data_itr):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outp = model(batch.src, batch.trg, batch.src_mask, batch.trg_mask)\n",
    "        loss = labelSmoothingLoss(outp, batch.trg_y, batch.ntokens)\n",
    "        print(outp, batch.trg_y)\n",
    "        print(loss)\n",
    "        \n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss+=loss\n",
    "        total_token+=batch.ntokens\n",
    "        tokens+=batch.ntokens\n",
    "        \n",
    "        if i%30 == 1:\n",
    "            elapsed = time.time() - start\n",
    "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
    "                    (i, loss / batch.ntokens, tokens / elapsed))\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "#     return total_loss/total_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-7.4237, -3.1665, -2.4552,  ..., -2.6077, -2.3816, -2.3433],\n",
      "         [-6.7839, -2.2102, -2.8418,  ..., -2.3309, -3.0578, -2.5213],\n",
      "         [-7.3292, -3.4833, -2.2683,  ..., -3.5495, -2.4031, -2.3797],\n",
      "         ...,\n",
      "         [-7.9818, -3.2699, -2.4587,  ..., -1.8401, -2.7156, -2.7715],\n",
      "         [-6.5371, -3.1272, -2.6855,  ..., -2.9109, -2.2116, -2.9183],\n",
      "         [-6.9132, -3.5621, -3.2908,  ..., -3.3836, -2.3981, -2.7933]],\n",
      "\n",
      "        [[-7.1767, -3.2429, -1.5856,  ..., -2.7775, -2.2431, -1.8857],\n",
      "         [-6.0338, -2.9686, -1.8795,  ..., -2.6771, -1.9329, -1.6746],\n",
      "         [-6.2772, -3.1710, -1.4743,  ..., -2.0955, -2.8640, -2.0553],\n",
      "         ...,\n",
      "         [-6.0521, -2.5058, -2.6947,  ..., -2.5138, -2.8490, -1.9991],\n",
      "         [-6.2784, -3.9501, -2.2114,  ..., -3.4146, -2.5325, -1.9739],\n",
      "         [-6.1703, -2.6596, -2.9692,  ..., -2.5299, -2.2412, -1.7625]],\n",
      "\n",
      "        [[-8.1208, -2.5282, -1.8604,  ..., -3.5268, -2.6839, -1.9251],\n",
      "         [-6.7543, -3.3216, -3.1664,  ..., -2.8497, -3.0009, -1.9410],\n",
      "         [-7.3760, -2.5935, -2.3890,  ..., -3.7362, -3.1577, -0.8012],\n",
      "         ...,\n",
      "         [-7.7833, -3.5958, -2.4042,  ..., -3.5991, -2.7741, -1.9848],\n",
      "         [-7.4459, -3.1156, -3.1113,  ..., -3.1998, -4.1687, -1.1438],\n",
      "         [-7.6372, -3.8653, -2.4809,  ..., -4.6635, -3.5931, -1.5002]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-9.4630, -2.5965, -2.3159,  ..., -3.7581, -1.3734, -1.8078],\n",
      "         [-8.5151, -3.0271, -2.8626,  ..., -3.0658, -2.1671, -1.9516],\n",
      "         [-7.7171, -3.0688, -3.3808,  ..., -2.4407, -1.5478, -1.7483],\n",
      "         ...,\n",
      "         [-7.2478, -2.3561, -3.0969,  ..., -2.5526, -1.7213, -2.6378],\n",
      "         [-8.2119, -1.7541, -4.4725,  ..., -2.7151, -2.4083, -1.8127],\n",
      "         [-7.8485, -2.6366, -2.9908,  ..., -2.1391, -2.1156, -1.4654]],\n",
      "\n",
      "        [[-8.3039, -2.8896, -2.1016,  ..., -2.9119, -1.3547, -1.2117],\n",
      "         [-6.9203, -3.0755, -2.8698,  ..., -3.4324, -1.8774, -1.6578],\n",
      "         [-6.7167, -3.2097, -1.9629,  ..., -2.3252, -1.8804, -1.0939],\n",
      "         ...,\n",
      "         [-7.5899, -4.0029, -1.9407,  ..., -3.3124, -2.2041, -1.1662],\n",
      "         [-7.4002, -4.2014, -2.6160,  ..., -3.7944, -1.2886, -1.3754],\n",
      "         [-7.1987, -3.3705, -2.0752,  ..., -2.9366, -1.3772, -1.5172]],\n",
      "\n",
      "        [[-7.4064, -1.9867, -1.4089,  ..., -3.4732, -2.1777, -2.3956],\n",
      "         [-7.3043, -3.2014, -2.5998,  ..., -3.6736, -1.7715, -1.2277],\n",
      "         [-6.6668, -2.5246, -2.4191,  ..., -3.3205, -1.9667, -2.0021],\n",
      "         ...,\n",
      "         [-7.5332, -2.7419, -2.4494,  ..., -2.8707, -2.3036, -2.1716],\n",
      "         [-8.0713, -2.5654, -2.0333,  ..., -3.1544, -2.1901, -1.6130],\n",
      "         [-7.4237, -2.8506, -2.0950,  ..., -3.0555, -2.1320, -1.1611]]],\n",
      "       grad_fn=<LogSoftmaxBackward>) tensor([[ 5,  4, 10,  2,  6,  1,  2,  4,  6],\n",
      "        [ 8,  9,  5,  4,  8, 10,  4, 10,  9],\n",
      "        [10,  8,  9, 10,  5,  4,  7,  3,  3],\n",
      "        [ 1,  2, 10,  6,  3,  3,  1,  1,  4],\n",
      "        [ 2,  1,  9,  3,  3,  8,  9, 10,  3],\n",
      "        [10,  6,  5, 10,  5, 10, 10,  3,  4],\n",
      "        [ 4,  9,  1,  6,  4,  4,  8,  8,  7],\n",
      "        [ 1,  9,  3,  3,  9,  2,  2,  5,  5],\n",
      "        [ 9,  8,  7,  3,  7,  1,  2, 10,  2],\n",
      "        [ 7,  4,  4,  2,  3,  5,  6,  2,  8],\n",
      "        [ 1,  5,  4,  8,  5,  9,  1, 10, 10],\n",
      "        [ 2,  3,  1,  4,  9,  2, 10,  9,  7],\n",
      "        [ 1,  6,  9,  2,  3,  1,  2,  9,  8],\n",
      "        [ 5,  3,  4,  6,  5,  1,  4,  4,  1],\n",
      "        [ 7,  2,  8,  3, 10,  3,  5,  4,  2],\n",
      "        [ 4,  1,  1,  7,  4,  1,  4,  8,  6],\n",
      "        [ 6,  1,  6,  1,  2,  6,  2,  8,  3],\n",
      "        [ 1,  8,  8,  7,  8,  5,  9,  7, 10],\n",
      "        [ 4,  2,  1, 10,  7,  4,  2,  7,  8],\n",
      "        [ 2,  7,  9,  3,  4,  2,  3, 10,  5],\n",
      "        [10,  3,  3,  9, 10,  4,  4,  5,  3],\n",
      "        [ 8,  1,  6,  8,  7,  1,  1,  6, 10],\n",
      "        [ 4,  6, 10, 10,  2,  2,  6,  4,  2],\n",
      "        [ 1,  3,  3,  7,  6,  2,  5,  9,  8],\n",
      "        [ 5,  3,  6,  1,  8,  8,  9,  6,  6],\n",
      "        [10,  1,  8,  1,  1,  5,  3,  9,  5],\n",
      "        [ 8, 10,  2, 10,  1, 10,  3,  6,  6],\n",
      "        [ 6,  2,  4,  6,  6, 10,  5,  9,  9],\n",
      "        [ 3,  9,  8,  5,  9,  8,  8,  9,  7],\n",
      "        [ 4,  4,  5,  3, 10,  6,  1,  3,  3]])\n",
      "tensor(382598., grad_fn=<KlDivBackward>)\n",
      "tensor([[[ -9.5694,  -3.5488,  -2.5638,  ...,  -2.3751,  -2.2344,  -2.7752],\n",
      "         [ -7.6218,  -2.8823,  -1.7325,  ...,  -3.0343,  -1.6391,  -3.0810],\n",
      "         [ -9.7278,  -3.7864,  -2.4716,  ...,  -2.6086,  -2.8965,  -3.1875],\n",
      "         ...,\n",
      "         [ -8.6136,  -3.8564,  -2.0562,  ...,  -2.1228,  -3.4867,  -2.0339],\n",
      "         [ -7.3243,  -3.6204,  -2.5711,  ...,  -1.7270,  -2.2161,  -3.1602],\n",
      "         [ -7.7988,  -3.5522,  -2.5280,  ...,  -1.8515,  -2.2430,  -2.9461]],\n",
      "\n",
      "        [[ -9.8855,  -2.8853,  -2.3462,  ...,  -1.8781,  -1.8171,  -3.8173],\n",
      "         [ -7.9371,  -3.2543,  -2.4965,  ...,  -1.5809,  -1.9419,  -3.4620],\n",
      "         [ -9.4739,  -3.2406,  -3.1138,  ...,  -1.7884,  -1.8377,  -3.4996],\n",
      "         ...,\n",
      "         [ -8.8304,  -3.8848,  -2.9279,  ...,  -3.3694,  -2.9024,  -2.4229],\n",
      "         [ -8.5988,  -3.7186,  -2.7069,  ...,  -1.1975,  -2.5602,  -4.5469],\n",
      "         [ -7.0354,  -3.1531,  -3.1232,  ...,  -0.8939,  -2.0705,  -4.4509]],\n",
      "\n",
      "        [[ -7.9516,  -3.0357,  -2.3187,  ...,  -2.5849,  -1.1858,  -2.9028],\n",
      "         [ -7.9440,  -3.1311,  -2.5378,  ...,  -2.2895,  -1.8026,  -2.9458],\n",
      "         [ -6.8255,  -3.2599,  -1.9830,  ...,  -3.8867,  -1.3666,  -2.9772],\n",
      "         ...,\n",
      "         [ -6.3269,  -3.8879,  -2.1553,  ...,  -2.3775,  -1.9496,  -2.8718],\n",
      "         [ -6.5685,  -2.9330,  -1.5997,  ...,  -3.4379,  -1.5043,  -2.7163],\n",
      "         [ -6.3218,  -3.2947,  -1.9431,  ...,  -3.3150,  -1.7199,  -2.8038]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-11.5493,  -3.9657,  -1.5738,  ...,  -1.3658,  -1.8412,  -4.3542],\n",
      "         [ -9.5597,  -3.0569,  -2.1192,  ...,  -1.1812,  -1.1718,  -3.6229],\n",
      "         [ -9.3967,  -4.3719,  -1.8576,  ...,  -1.5996,  -1.7689,  -2.9374],\n",
      "         ...,\n",
      "         [-10.4067,  -3.5398,  -1.5251,  ...,  -1.2813,  -2.0450,  -4.0965],\n",
      "         [ -9.9089,  -3.5077,  -2.6600,  ...,  -0.4830,  -2.3809,  -4.6993],\n",
      "         [-11.1751,  -4.2575,  -1.0959,  ...,  -1.3090,  -1.9195,  -3.3636]],\n",
      "\n",
      "        [[-11.4843,  -2.7861,  -2.3578,  ...,  -0.9238,  -1.9723,  -3.2459],\n",
      "         [-10.6637,  -3.5081,  -1.9329,  ...,  -0.7065,  -2.3594,  -3.0810],\n",
      "         [-11.1336,  -3.6779,  -3.2083,  ...,  -0.4192,  -2.5721,  -4.3645],\n",
      "         ...,\n",
      "         [-10.8149,  -3.6668,  -2.1904,  ...,  -0.9495,  -1.8320,  -3.7086],\n",
      "         [-10.5244,  -3.2564,  -3.0548,  ...,  -0.7388,  -2.0363,  -3.2817],\n",
      "         [ -9.7356,  -3.5352,  -2.7674,  ...,  -0.7912,  -2.1232,  -3.7065]],\n",
      "\n",
      "        [[ -9.4576,  -2.2047,  -1.3589,  ...,  -2.0076,  -1.5588,  -3.0730],\n",
      "         [ -9.9207,  -4.3145,  -1.6649,  ...,  -0.9920,  -2.7412,  -2.6345],\n",
      "         [ -7.9885,  -3.1991,  -2.0498,  ...,  -0.6905,  -2.4452,  -3.9557],\n",
      "         ...,\n",
      "         [ -9.0539,  -2.7886,  -1.6768,  ...,  -2.0529,  -2.4230,  -3.1833],\n",
      "         [ -9.1744,  -3.1533,  -1.7840,  ...,  -1.1915,  -1.8156,  -2.8405],\n",
      "         [ -9.0952,  -2.4523,  -1.2132,  ...,  -2.7627,  -1.2478,  -3.4635]]],\n",
      "       grad_fn=<LogSoftmaxBackward>) tensor([[ 8,  4,  6,  2,  3,  9,  2,  2,  1],\n",
      "        [ 2,  1,  8,  7,  1,  6,  4, 10,  2],\n",
      "        [ 3,  8,  5,  2,  5,  2,  8,  8,  7],\n",
      "        [ 6,  9, 10,  3,  7,  6, 10,  9, 10],\n",
      "        [ 2,  1,  2,  4,  5,  2,  7,  7,  1],\n",
      "        [ 7,  3,  4,  4,  9,  1,  5,  9,  1],\n",
      "        [ 3, 10,  5,  3,  9, 10,  8,  6,  2],\n",
      "        [ 2,  7,  4,  4,  4,  9,  4,  2,  5],\n",
      "        [ 8,  3,  7, 10,  7, 10,  8,  1,  8],\n",
      "        [10,  3,  2,  2,  6,  7,  1,  5,  1],\n",
      "        [ 8,  5,  9,  8,  2,  6,  9,  6,  9],\n",
      "        [ 8,  3,  9,  4,  6,  9,  2, 10,  3],\n",
      "        [ 7,  9,  5,  5,  3,  2,  2,  6,  3],\n",
      "        [ 5,  1,  1,  3,  5,  4,  3,  1,  7],\n",
      "        [ 7,  9,  6,  8,  5,  9,  7,  6,  5],\n",
      "        [ 6,  3,  1,  8,  3,  5,  5,  5,  9],\n",
      "        [ 4,  6,  5,  2, 10,  5,  8,  7,  2],\n",
      "        [10,  2,  5,  2,  2,  4,  1,  6,  2],\n",
      "        [ 5,  9,  5,  6,  3,  1,  2,  7,  7],\n",
      "        [ 7,  1,  6,  2,  8,  7,  6,  1,  3],\n",
      "        [ 2,  6,  7, 10, 10,  6,  6,  7,  8],\n",
      "        [ 3,  6,  8,  4,  1,  1,  9,  5,  7],\n",
      "        [ 4,  8,  8,  8,  8, 10,  8,  2,  1],\n",
      "        [ 5,  5,  9,  4, 10,  9,  4, 10,  4],\n",
      "        [ 1,  3, 10,  5,  1,  3,  3,  6,  5],\n",
      "        [ 8,  3,  1,  8,  3,  1,  1,  4,  1],\n",
      "        [ 1,  8,  8,  7,  1,  3,  6,  9,  4],\n",
      "        [10,  6, 10,  7,  3,  4,  2,  5,  1],\n",
      "        [ 9,  5, 10,  1, 10,  1,  2,  2, 10],\n",
      "        [ 9, 10,  9,  6,  3,  2,  2,  8,  6]])\n",
      "tensor(395739.5625, grad_fn=<KlDivBackward>)\n",
      "Epoch Step: 1 Loss: 1465.702026 Tokens per Sec: 724.160706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-7.7950, -2.5703, -2.9710,  ..., -2.1655, -2.3503, -2.3699],\n",
      "         [-7.4835, -3.0805, -0.9570,  ..., -2.8210, -1.9875, -4.0076],\n",
      "         [-6.4127, -2.9519, -1.3419,  ..., -2.0622, -2.1407, -3.8500],\n",
      "         ...,\n",
      "         [-7.4673, -2.6674, -1.4339,  ..., -2.4560, -2.0066, -3.3499],\n",
      "         [-7.6415, -2.7999, -2.3931,  ..., -2.0003, -1.9770, -3.2338],\n",
      "         [-6.9788, -3.3380, -0.8468,  ..., -2.9426, -2.3153, -4.9298]],\n",
      "\n",
      "        [[-8.2784, -2.3322, -1.5016,  ..., -1.7233, -2.5958, -3.1344],\n",
      "         [-6.5740, -3.2061, -1.9602,  ..., -1.7172, -3.4880, -3.1112],\n",
      "         [-5.8225, -3.5218, -0.9733,  ..., -2.8371, -2.1779, -2.9079],\n",
      "         ...,\n",
      "         [-6.2010, -3.0233, -1.2086,  ..., -1.9640, -2.9815, -4.0497],\n",
      "         [-6.2395, -3.3574, -0.8612,  ..., -2.0572, -3.3959, -3.9298],\n",
      "         [-7.9357, -2.8254, -1.8276,  ..., -2.1613, -1.5951, -4.3203]],\n",
      "\n",
      "        [[-7.8032, -2.4187, -2.5121,  ..., -1.8451, -2.2766, -2.6166],\n",
      "         [-5.8621, -2.7302, -1.9287,  ..., -3.5599, -0.8827, -2.8092],\n",
      "         [-7.5826, -1.9480, -2.3744,  ..., -1.8100, -2.0343, -2.9997],\n",
      "         ...,\n",
      "         [-7.3799, -2.0053, -1.9145,  ..., -1.9600, -2.1938, -2.2995],\n",
      "         [-7.2881, -2.7776, -1.9329,  ..., -2.0834, -2.3750, -3.3271],\n",
      "         [-7.9742, -3.4821, -2.0419,  ..., -1.0940, -2.2232, -3.4850]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-9.5445, -3.5072, -4.0239,  ..., -1.1476, -2.1905, -1.8122],\n",
      "         [-6.7737, -4.0447, -3.1061,  ..., -1.0518, -3.0055, -1.7930],\n",
      "         [-7.7457, -3.4059, -3.2611,  ..., -2.0545, -1.6275, -2.5565],\n",
      "         ...,\n",
      "         [-7.5252, -4.3395, -2.6239,  ..., -2.1428, -1.5966, -1.9564],\n",
      "         [-7.7274, -3.4463, -2.4388,  ..., -2.3756, -1.6913, -3.7823],\n",
      "         [-7.9255, -3.3355, -2.4218,  ..., -1.7367, -2.1597, -2.0891]],\n",
      "\n",
      "        [[-8.9623, -2.4885, -3.1726,  ..., -1.6183, -1.5815, -2.2161],\n",
      "         [-7.7978, -3.8135, -2.3222,  ..., -1.9139, -2.0084, -1.8094],\n",
      "         [-7.3642, -3.1074, -2.4950,  ..., -2.0640, -3.0075, -1.7369],\n",
      "         ...,\n",
      "         [-7.2871, -4.1200, -2.9491,  ..., -1.3635, -2.7626, -1.7876],\n",
      "         [-8.0796, -3.5162, -2.2467,  ..., -2.0868, -2.8402, -1.3745],\n",
      "         [-8.7814, -3.8304, -2.3914,  ..., -1.0869, -3.2589, -2.9550]],\n",
      "\n",
      "        [[-8.0125, -2.8939, -3.4265,  ..., -1.4871, -2.9413, -1.7257],\n",
      "         [-7.0815, -4.6565, -2.4104,  ..., -1.8069, -2.0532, -1.5958],\n",
      "         [-7.1246, -3.2821, -2.7551,  ..., -1.6780, -3.1120, -1.5730],\n",
      "         ...,\n",
      "         [-5.9359, -3.4935, -2.9118,  ..., -2.4802, -1.8465, -2.1614],\n",
      "         [-6.1843, -3.8634, -2.9603,  ..., -3.0819, -1.7973, -1.5370],\n",
      "         [-8.4940, -3.3638, -3.3922,  ..., -1.4385, -2.4065, -2.0280]]],\n",
      "       grad_fn=<LogSoftmaxBackward>) tensor([[ 3,  8,  1,  5,  2,  4,  1,  3, 10],\n",
      "        [ 7,  9, 10,  3,  1,  7,  7,  4,  7],\n",
      "        [10,  1,  1,  3, 10,  8,  5,  2,  8],\n",
      "        [ 5,  1,  8,  9, 10,  1,  8, 10,  9],\n",
      "        [ 3,  4,  3,  3, 10,  7,  5,  9,  4],\n",
      "        [ 9,  3,  4,  9,  7,  6,  1,  4, 10],\n",
      "        [ 7,  7,  1,  6,  1,  5,  7,  6,  8],\n",
      "        [10,  8,  1,  4,  1,  5,  6,  7,  8],\n",
      "        [ 2,  2,  9, 10,  4,  8,  8,  2, 10],\n",
      "        [ 6,  9, 10,  1, 10,  6,  2,  8,  5],\n",
      "        [ 4,  8, 10,  9,  2,  5,  1,  1,  1],\n",
      "        [ 7,  8,  2,  2,  9,  6,  5,  5,  9],\n",
      "        [ 8,  6, 10,  4,  3, 10,  4,  6,  1],\n",
      "        [ 3,  5,  7, 10,  9,  6,  9,  5,  6],\n",
      "        [ 5,  3,  1,  9,  4,  9,  1,  2,  8],\n",
      "        [ 3,  6,  1,  2,  5, 10,  4,  8,  8],\n",
      "        [ 2,  7,  7,  7,  8,  2,  2,  7,  9],\n",
      "        [ 3,  6,  3,  5,  1,  9,  6,  1, 10],\n",
      "        [ 7,  4,  8,  1,  2,  2,  1,  3,  5],\n",
      "        [ 5,  6,  1,  6, 10,  2, 10,  6,  2],\n",
      "        [ 6,  4,  1,  2, 10,  4,  8,  3,  6],\n",
      "        [ 8,  2,  9,  4,  7,  4,  2,  4,  5],\n",
      "        [ 5,  6, 10,  4, 10,  7,  8,  5,  4],\n",
      "        [ 5,  8,  1,  3,  6,  3,  1,  1,  2],\n",
      "        [ 6,  4,  8,  1,  5,  6,  9,  3,  1],\n",
      "        [ 6,  4,  4,  4,  6,  6,  6,  4,  6],\n",
      "        [ 1,  5,  7,  1,  1,  6,  9, 10,  6],\n",
      "        [ 7,  4,  4,  3,  1,  2,  3,  8,  7],\n",
      "        [ 2,  9,  7,  3,  3,  7,  9,  5,  7],\n",
      "        [ 2,  9,  4,  7,  2, 10, 10,  5,  4]])\n",
      "tensor(381558.7188, grad_fn=<KlDivBackward>)\n",
      "tensor([[[-8.8432, -3.0407, -4.1459,  ..., -2.3113, -3.9993, -3.9934],\n",
      "         [-7.5625, -3.3916, -4.4269,  ..., -3.3221, -5.1345, -3.6867],\n",
      "         [-8.6329, -2.4588, -4.7788,  ..., -2.3060, -4.2005, -3.7931],\n",
      "         ...,\n",
      "         [-9.7679, -2.8589, -3.4987,  ..., -2.9195, -5.3077, -4.4743],\n",
      "         [-8.2894, -2.9172, -3.1748,  ..., -2.6577, -4.6907, -4.4542],\n",
      "         [-7.4533, -3.1134, -3.3367,  ..., -2.5823, -4.0211, -4.0415]],\n",
      "\n",
      "        [[-8.8018, -2.5881, -4.5861,  ..., -2.2679, -3.7753, -3.4316],\n",
      "         [-6.8718, -3.5685, -3.7374,  ..., -2.3990, -4.8153, -3.4334],\n",
      "         [-6.0481, -3.1224, -3.4122,  ..., -2.6365, -4.2284, -2.9793],\n",
      "         ...,\n",
      "         [-7.4900, -4.5273, -5.0857,  ..., -3.2571, -5.4229, -5.7677],\n",
      "         [-6.3892, -2.3533, -3.4183,  ..., -2.0791, -3.3987, -3.1297],\n",
      "         [-7.4797, -2.4317, -4.3255,  ..., -2.1832, -4.2297, -3.9642]],\n",
      "\n",
      "        [[-9.0082, -2.9079, -4.9010,  ..., -2.8634, -4.1148, -2.6432],\n",
      "         [-7.8355, -2.7612, -3.7467,  ..., -2.5997, -4.2913, -2.7593],\n",
      "         [-7.7987, -3.7347, -4.0833,  ..., -2.6222, -5.3252, -4.0709],\n",
      "         ...,\n",
      "         [-8.6039, -2.4736, -3.9952,  ..., -1.4829, -5.0976, -3.7388],\n",
      "         [-8.6384, -3.4156, -4.8289,  ..., -2.0667, -3.9414, -3.9388],\n",
      "         [-8.7063, -2.9823, -5.0229,  ..., -2.7762, -4.5535, -3.4928]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-7.2169, -3.1976, -3.2145,  ..., -1.3303, -2.9412, -3.0768],\n",
      "         [-7.4559, -4.5075, -3.2999,  ..., -0.7631, -3.8427, -3.6463],\n",
      "         [-7.9447, -3.6531, -3.9371,  ..., -1.9002, -4.2304, -2.4750],\n",
      "         ...,\n",
      "         [-5.7966, -2.9663, -4.0717,  ..., -1.5059, -3.4869, -2.5394],\n",
      "         [-7.6593, -3.4042, -4.2103,  ..., -1.3405, -3.5241, -3.0317],\n",
      "         [-8.0014, -1.4313, -3.6301,  ..., -1.2257, -4.6171, -3.0471]],\n",
      "\n",
      "        [[-7.5533, -2.9351, -4.2614,  ..., -1.3565, -3.3934, -3.0404],\n",
      "         [-7.1793, -3.2922, -4.4554,  ..., -1.5051, -3.8269, -2.5876],\n",
      "         [-7.0986, -3.0541, -3.8608,  ..., -1.6630, -3.3618, -2.6622],\n",
      "         ...,\n",
      "         [-6.2129, -3.3766, -2.9685,  ..., -2.0254, -4.4515, -2.9710],\n",
      "         [-7.7402, -3.1163, -4.4484,  ..., -1.7215, -3.8076, -3.3620],\n",
      "         [-6.9335, -3.0243, -3.6637,  ..., -1.3123, -4.8558, -2.8227]],\n",
      "\n",
      "        [[-7.5442, -2.5405, -4.0098,  ..., -1.5546, -3.4499, -3.2131],\n",
      "         [-8.3922, -2.8411, -4.0864,  ..., -2.1756, -4.6965, -2.5289],\n",
      "         [-7.2906, -3.0875, -4.2113,  ..., -1.6299, -4.2990, -3.6436],\n",
      "         ...,\n",
      "         [-8.5459, -3.3324, -5.0135,  ..., -1.2404, -3.8359, -3.1196],\n",
      "         [-8.5347, -2.6509, -3.7218,  ..., -1.1916, -4.2736, -3.2029],\n",
      "         [-7.9021, -1.9900, -3.1680,  ..., -1.4905, -3.1109, -2.1863]]],\n",
      "       grad_fn=<LogSoftmaxBackward>) tensor([[ 8,  1,  4,  8,  9,  6,  6,  9,  2],\n",
      "        [ 9, 10,  2,  2,  5,  8,  2,  6,  8],\n",
      "        [ 2,  8,  7,  4,  4,  3,  4,  1,  8],\n",
      "        [10,  2,  6, 10,  4,  7,  9,  2,  8],\n",
      "        [ 3,  2,  1,  1,  4,  9,  4,  8,  5],\n",
      "        [ 1,  6,  5,  9,  8,  6,  1,  9,  8],\n",
      "        [ 9,  3,  7,  4,  7,  9,  5,  2,  6],\n",
      "        [ 4,  6,  1,  5,  4,  5,  6,  2,  4],\n",
      "        [ 6,  3,  7,  5,  7,  8,  4, 10,  4],\n",
      "        [10,  5, 10,  6,  5,  5,  7,  7,  9],\n",
      "        [ 6,  3,  1,  8,  9,  4,  4,  1,  3],\n",
      "        [ 8,  1,  8,  1,  9, 10,  2,  9,  8],\n",
      "        [ 2,  8,  8,  8,  5, 10,  8,  3,  5],\n",
      "        [ 2,  1,  9,  5,  8,  3,  9,  7, 10],\n",
      "        [ 3,  2,  5,  3,  8,  4,  3,  2,  6],\n",
      "        [10,  1,  7,  9,  5,  7,  6,  9,  2],\n",
      "        [ 2,  1,  6,  4,  2,  8,  4,  5,  4],\n",
      "        [10,  3,  1,  8,  5,  2,  3,  4,  8],\n",
      "        [ 8,  8,  3,  3,  2,  8,  7,  2,  5],\n",
      "        [10,  7,  8,  7,  8,  9,  4,  9, 10],\n",
      "        [ 1,  4,  8, 10,  2,  6,  8,  4,  8],\n",
      "        [ 3,  8,  4,  8,  6,  8,  5,  2,  5],\n",
      "        [ 7,  9,  8,  5,  2, 10,  6,  4,  1],\n",
      "        [ 3,  6,  8,  7,  1, 10,  3,  4,  2],\n",
      "        [ 4, 10,  9,  6,  5,  9,  6,  7,  3],\n",
      "        [10,  8,  8,  4,  1,  2,  3,  7,  4],\n",
      "        [ 5,  3,  1,  7,  3,  8,  2,  5,  9],\n",
      "        [ 4,  6,  2, 10,  2, 10,  4,  3,  5],\n",
      "        [10,  1,  9,  2,  4, 10,  5,  3, 10],\n",
      "        [ 6,  8,  4,  4, 10,  4,  3,  2,  9]])\n",
      "tensor(404343.0312, grad_fn=<KlDivBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-9.0640, -1.9357, -2.1684,  ..., -2.5488, -4.0397, -3.4942],\n",
      "         [-9.9760, -2.7557, -2.7528,  ..., -3.1590, -4.3148, -3.2182],\n",
      "         [-8.2828, -2.1472, -2.5048,  ..., -2.6651, -4.5133, -3.1216],\n",
      "         ...,\n",
      "         [-8.2096, -2.7888, -4.3449,  ..., -3.1330, -3.6702, -2.7656],\n",
      "         [-8.2211, -3.0613, -1.8829,  ..., -2.8156, -4.4655, -2.0563],\n",
      "         [-9.4004, -2.8567, -3.2573,  ..., -3.0598, -3.3100, -2.8578]],\n",
      "\n",
      "        [[-8.5595, -1.6066, -2.0826,  ..., -2.9281, -3.1487, -3.9539],\n",
      "         [-7.7073, -2.4613, -1.9801,  ..., -2.3729, -4.2752, -3.4054],\n",
      "         [-7.7665, -2.7319, -3.0793,  ..., -2.4929, -2.5588, -3.5737],\n",
      "         ...,\n",
      "         [-9.4756, -1.8293, -3.1520,  ..., -2.3735, -3.5188, -3.5549],\n",
      "         [-9.4577, -2.7827, -2.4907,  ..., -2.4375, -3.2882, -2.9545],\n",
      "         [-8.7958, -2.1797, -1.5928,  ..., -3.1574, -4.4527, -2.8628]],\n",
      "\n",
      "        [[-8.4212, -1.0725, -3.0453,  ..., -2.1283, -3.8493, -3.7909],\n",
      "         [-8.6560, -1.8614, -2.9651,  ..., -1.6389, -3.7834, -4.2898],\n",
      "         [-8.9711, -1.2366, -2.1040,  ..., -2.2607, -3.4890, -4.0102],\n",
      "         ...,\n",
      "         [-8.3267, -2.3675, -1.7909,  ..., -2.7013, -4.1412, -3.3534],\n",
      "         [-8.2891, -1.4925, -3.3463,  ..., -2.4158, -3.7118, -3.9173],\n",
      "         [-7.6526, -2.1961, -2.2739,  ..., -2.6186, -3.1630, -3.6489]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-8.5352, -1.0090, -2.6745,  ..., -2.6399, -3.9897, -3.1815],\n",
      "         [-8.5368, -2.2832, -2.5856,  ..., -2.6029, -3.2984, -2.7643],\n",
      "         [-8.6184, -2.6888, -1.7178,  ..., -2.1308, -4.2296, -2.0125],\n",
      "         ...,\n",
      "         [-9.1730, -1.3399, -1.3889,  ..., -3.2471, -4.2262, -2.6631],\n",
      "         [-8.3911, -1.4111, -1.3244,  ..., -2.6951, -3.8915, -2.5942],\n",
      "         [-8.6619, -1.5258, -1.5410,  ..., -2.8430, -4.1675, -2.4019]],\n",
      "\n",
      "        [[-8.4575, -1.3731, -2.1575,  ..., -2.0275, -3.8675, -1.6654],\n",
      "         [-9.3198, -2.0231, -2.1055,  ..., -2.5677, -3.9283, -0.9996],\n",
      "         [-8.9720, -1.1376, -3.3619,  ..., -2.0804, -3.8780, -2.0961],\n",
      "         ...,\n",
      "         [-8.9387, -1.2318, -3.2633,  ..., -2.9382, -2.9330, -2.2715],\n",
      "         [-9.6624, -1.7674, -2.8632,  ..., -1.8475, -3.4487, -1.5923],\n",
      "         [-8.8812, -1.8911, -2.5177,  ..., -1.9171, -2.5344, -1.4691]],\n",
      "\n",
      "        [[-8.6730, -0.7306, -2.4704,  ..., -2.5776, -4.3644, -2.7647],\n",
      "         [-9.2004, -2.1050, -1.5627,  ..., -2.4127, -3.6045, -2.5259],\n",
      "         [-8.8696, -0.9428, -2.3310,  ..., -2.3299, -4.4569, -2.2389],\n",
      "         ...,\n",
      "         [-6.7269, -1.4028, -2.1777,  ..., -2.7010, -4.8367, -1.7945],\n",
      "         [-8.3564, -1.6383, -2.9157,  ..., -1.2570, -3.9023, -2.5344],\n",
      "         [-8.3920, -1.5428, -1.9164,  ..., -2.0100, -3.8106, -2.4027]]],\n",
      "       grad_fn=<LogSoftmaxBackward>) tensor([[ 4,  4,  1,  1,  8,  9,  6,  5,  5],\n",
      "        [ 6,  8,  4,  3,  1,  4,  4,  7,  8],\n",
      "        [ 8,  1,  1,  7,  9,  6,  9,  2,  6],\n",
      "        [ 5,  1,  9,  3,  8,  1, 10,  2, 10],\n",
      "        [ 5,  8,  5,  5,  3, 10,  7,  6,  1],\n",
      "        [ 2,  7,  2, 10,  7, 10, 10,  6,  8],\n",
      "        [ 6,  9,  9,  2,  5,  2,  6,  1,  2],\n",
      "        [ 6,  1,  7,  7,  2,  9,  3,  2,  1],\n",
      "        [ 5,  3,  8, 10,  5,  1,  5,  5,  2],\n",
      "        [ 6,  4,  5,  5,  6,  5,  3,  4,  9],\n",
      "        [ 1, 10,  2,  7,  2,  7,  3,  6,  3],\n",
      "        [10,  1,  1, 10,  9,  4,  5, 10,  5],\n",
      "        [ 6,  8,  2,  1,  3,  9,  4,  7,  3],\n",
      "        [ 9,  7,  2,  5,  3,  8,  2,  6, 10],\n",
      "        [ 9,  7,  3,  4, 10,  2,  4,  6,  5],\n",
      "        [ 7,  5,  1,  8,  3,  8,  8,  8,  6],\n",
      "        [ 2,  9,  2,  5,  5,  9,  7,  6,  3],\n",
      "        [10,  2,  3,  1,  5,  7,  4,  5,  1],\n",
      "        [ 3,  5,  7,  2,  3,  9,  3,  1,  3],\n",
      "        [10, 10,  1,  5,  1,  9,  8,  8,  2],\n",
      "        [ 8, 10,  6,  8, 10,  5,  9,  8,  4],\n",
      "        [ 5,  6, 10,  5,  6,  1,  5,  7,  9],\n",
      "        [ 1,  6,  7,  2,  9,  3,  1,  8,  8],\n",
      "        [ 2, 10,  9,  3,  9,  4,  1,  4,  3],\n",
      "        [ 1,  1,  9,  6,  6,  5,  1,  5,  6],\n",
      "        [ 9,  6,  4,  1,  3,  6,  7,  9,  5],\n",
      "        [ 1,  1,  4,  3,  4,  4,  7,  2,  5],\n",
      "        [ 3,  6,  8,  3,  7,  7,  6,  6,  9],\n",
      "        [ 6,  5,  1,  7,  7,  3,  5,  5,  7],\n",
      "        [ 4,  1,  5,  9,  1, 10,  5,  5,  9]])\n",
      "tensor(392247.5000, grad_fn=<KlDivBackward>)\n",
      "tensor([[[-7.4315, -3.1436, -1.7515,  ..., -1.3715, -1.7531, -2.7834],\n",
      "         [-6.8188, -2.5693, -2.0156,  ..., -0.9003, -2.0471, -3.6254],\n",
      "         [-7.0212, -2.1519, -1.2001,  ..., -1.6436, -3.1239, -2.9956],\n",
      "         ...,\n",
      "         [-6.6568, -3.7122, -2.6972,  ..., -0.9936, -2.3955, -1.9301],\n",
      "         [-7.4100, -3.3654, -2.3904,  ..., -1.1151, -2.2178, -2.1567],\n",
      "         [-6.6876, -2.4535, -1.4178,  ..., -1.8684, -1.7303, -3.2417]],\n",
      "\n",
      "        [[-7.9636, -3.1676, -2.6259,  ..., -1.2938, -0.9102, -4.0024],\n",
      "         [-7.5681, -2.9300, -2.1701,  ..., -1.2441, -2.4306, -3.3792],\n",
      "         [-7.5642, -3.4821, -3.1037,  ..., -3.1049, -0.7990, -3.6931],\n",
      "         ...,\n",
      "         [-8.2444, -2.5348, -2.9149,  ..., -2.1416, -1.1477, -3.4389],\n",
      "         [-6.4265, -2.4314, -2.1705,  ..., -1.3456, -1.3759, -3.8241],\n",
      "         [-6.0783, -3.1362, -1.8763,  ..., -1.6749, -1.8083, -3.2059]],\n",
      "\n",
      "        [[-7.3213, -3.0063, -2.3908,  ..., -2.0884, -1.0072, -4.1594],\n",
      "         [-6.9766, -1.9516, -2.5636,  ..., -1.7749, -1.5050, -3.9592],\n",
      "         [-6.4303, -2.4674, -2.1211,  ..., -1.8194, -1.9922, -3.0351],\n",
      "         ...,\n",
      "         [-7.7054, -2.0155, -2.0509,  ..., -1.2305, -1.4829, -4.7678],\n",
      "         [-7.4070, -2.2397, -2.1272,  ..., -1.2653, -2.1351, -3.8560],\n",
      "         [-8.4937, -3.1325, -2.3304,  ..., -1.3722, -1.0998, -4.1175]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-8.7695, -3.0048, -2.7749,  ..., -1.4422, -1.0357, -4.0312],\n",
      "         [-7.2115, -3.1990, -2.8768,  ..., -1.1142, -2.6084, -2.8335],\n",
      "         [-7.7155, -1.4415, -3.1629,  ..., -2.2954, -1.8347, -3.4304],\n",
      "         ...,\n",
      "         [-7.2717, -2.3319, -2.7720,  ..., -0.9267, -2.3540, -5.1416],\n",
      "         [-7.0638, -2.3135, -3.5597,  ..., -1.2213, -1.2960, -2.7139],\n",
      "         [-6.8612, -3.1762, -2.1020,  ..., -1.7391, -2.6392, -2.7063]],\n",
      "\n",
      "        [[-7.8572, -2.6346, -2.8212,  ..., -2.2762, -0.8510, -3.1783],\n",
      "         [-6.7474, -2.6660, -3.0029,  ..., -1.2574, -1.5834, -3.6792],\n",
      "         [-7.3072, -3.0214, -3.5892,  ..., -2.2080, -0.8579, -2.5693],\n",
      "         ...,\n",
      "         [-5.9916, -2.5636, -2.0281,  ..., -2.1991, -1.4817, -2.6480],\n",
      "         [-6.8817, -2.3608, -3.1618,  ..., -1.7592, -1.6514, -3.9447],\n",
      "         [-7.8419, -2.8588, -3.2358,  ..., -1.6367, -1.5199, -3.3360]],\n",
      "\n",
      "        [[-7.2865, -3.1484, -2.0088,  ..., -1.1191, -1.6828, -4.1757],\n",
      "         [-6.6170, -2.8087, -2.8371,  ..., -1.1146, -1.9616, -3.9974],\n",
      "         [-5.6366, -1.9808, -2.9195,  ..., -1.1082, -2.0060, -4.0961],\n",
      "         ...,\n",
      "         [-6.0122, -2.9927, -2.7377,  ..., -1.7215, -1.0338, -3.1532],\n",
      "         [-7.0011, -2.2865, -2.2330,  ..., -1.0689, -2.5432, -3.7532],\n",
      "         [-7.7122, -2.3444, -4.0085,  ..., -0.4931, -3.5130, -5.0459]]],\n",
      "       grad_fn=<LogSoftmaxBackward>) tensor([[ 4,  9,  9,  4,  9, 10,  7,  3,  4],\n",
      "        [ 6,  2,  9,  8,  9,  1,  4, 10,  9],\n",
      "        [ 4, 10,  8,  4,  2,  8,  4,  1,  5],\n",
      "        [ 1,  6,  3,  1,  3,  3,  7,  2,  7],\n",
      "        [ 1,  5, 10,  1,  9, 10,  5,  9,  1],\n",
      "        [ 9,  4, 10,  5,  3,  9,  4,  3,  8],\n",
      "        [ 8,  6,  4,  4,  2,  1,  1,  4,  8],\n",
      "        [ 9,  7, 10,  9,  8,  1, 10,  5,  4],\n",
      "        [ 8,  3,  6,  2,  1,  1, 10,  9,  3],\n",
      "        [10,  2,  4,  6,  7,  3,  5,  5,  1],\n",
      "        [ 5,  8,  8,  8,  2,  1,  4,  6,  2],\n",
      "        [ 6,  4, 10,  3,  6,  4,  4,  6,  9],\n",
      "        [ 3,  5,  3, 10,  9,  2,  1,  4,  6],\n",
      "        [ 9,  6,  9,  5,  1,  3,  5,  8,  3],\n",
      "        [ 2,  2,  6,  7,  7,  6,  7,  9,  9],\n",
      "        [ 9,  9,  2,  1,  5,  2,  8,  4,  4],\n",
      "        [ 6,  9, 10,  3,  3,  7, 10,  5,  5],\n",
      "        [ 4, 10,  3,  4,  5,  8,  9,  4,  8],\n",
      "        [ 9,  1,  2,  3,  7,  1,  6,  6,  5],\n",
      "        [ 2,  8,  9,  7,  9,  9,  3,  4,  1],\n",
      "        [ 9,  3,  3,  1,  1,  1,  1,  9,  4],\n",
      "        [ 3,  1,  9,  6,  9,  7,  9,  6,  8],\n",
      "        [ 5,  8,  1,  4,  3,  9,  5,  7,  1],\n",
      "        [ 5,  1,  2,  4,  3,  2,  1,  2, 10],\n",
      "        [ 5,  7,  2,  8,  7, 10,  2,  3,  8],\n",
      "        [ 2,  3,  6,  8,  3,  1,  5,  2,  9],\n",
      "        [ 2,  8,  8, 10,  6,  4,  8,  9,  6],\n",
      "        [10,  3,  1,  4, 10,  5,  9,  6,  7],\n",
      "        [ 8,  9,  4,  8,  5,  4,  5,  5,  8],\n",
      "        [ 1,  4,  4,  7,  3,  2,  3,  5,  3]])\n",
      "tensor(395286.4688, grad_fn=<KlDivBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ -7.6686,  -2.6692,  -2.2077,  ...,  -3.0155,  -1.4579,  -4.0962],\n",
      "         [ -8.3326,  -1.9635,  -2.1802,  ...,  -2.8757,  -3.7400,  -4.5951],\n",
      "         [ -7.2284,  -2.2436,  -2.0601,  ...,  -2.1644,  -2.2278,  -4.6842],\n",
      "         ...,\n",
      "         [ -7.8346,  -2.9236,  -1.7084,  ...,  -2.8776,  -2.1188,  -4.2384],\n",
      "         [ -7.0040,  -2.8205,  -2.5351,  ...,  -3.1265,  -2.2237,  -4.0277],\n",
      "         [ -6.3820,  -3.1449,  -2.2299,  ...,  -3.4557,  -2.7186,  -4.6734]],\n",
      "\n",
      "        [[ -8.1564,  -3.0022,  -1.4018,  ...,  -3.0190,  -2.3795,  -4.5552],\n",
      "         [ -7.9691,  -3.3696,  -1.9625,  ...,  -2.8768,  -2.4125,  -3.9694],\n",
      "         [ -6.5091,  -3.2751,  -2.9469,  ...,  -2.5798,  -2.2792,  -4.0519],\n",
      "         ...,\n",
      "         [ -6.0019,  -3.1605,  -2.6372,  ...,  -3.9005,  -3.9120,  -4.6277],\n",
      "         [ -7.7918,  -2.4856,  -2.5246,  ...,  -2.0425,  -3.0443,  -3.4174],\n",
      "         [ -6.9321,  -2.4207,  -2.1239,  ...,  -2.3858,  -2.6688,  -3.7237]],\n",
      "\n",
      "        [[ -8.5404,  -3.6106,  -3.1058,  ...,  -2.8739,  -2.4137,  -4.0087],\n",
      "         [ -8.8269,  -3.4821,  -3.0057,  ...,  -2.8767,  -4.4709,  -3.9034],\n",
      "         [ -6.9276,  -3.0067,  -2.5306,  ...,  -2.8456,  -2.7973,  -3.4712],\n",
      "         ...,\n",
      "         [ -7.6652,  -4.0008,  -2.3373,  ...,  -4.0871,  -3.4053,  -3.3442],\n",
      "         [ -7.3573,  -4.4965,  -3.4435,  ...,  -3.8479,  -3.6049,  -4.1094],\n",
      "         [ -7.4809,  -3.4305,  -2.6795,  ...,  -3.7662,  -3.2227,  -4.1194]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ -8.9002,  -3.7359,  -3.0619,  ...,  -3.4015,  -1.6264,  -2.7582],\n",
      "         [ -9.8301,  -2.9587,  -2.9788,  ...,  -2.9806,  -1.9718,  -1.9218],\n",
      "         [ -8.2257,  -2.8970,  -3.0539,  ...,  -3.9418,  -1.7799,  -2.2948],\n",
      "         ...,\n",
      "         [ -8.1554,  -3.0139,  -2.1341,  ...,  -3.6236,  -2.5533,  -2.1067],\n",
      "         [ -8.8390,  -2.4554,  -3.0552,  ...,  -3.3286,  -2.7709,  -2.2938],\n",
      "         [ -9.0709,  -3.7608,  -2.8738,  ...,  -2.8483,  -1.5620,  -2.5999]],\n",
      "\n",
      "        [[ -8.5199,  -3.5812,  -3.6095,  ...,  -2.8199,  -2.2824,  -3.4397],\n",
      "         [ -8.5813,  -2.8944,  -2.6137,  ...,  -2.6243,  -3.1216,  -2.9638],\n",
      "         [ -6.9909,  -3.0563,  -3.5400,  ...,  -2.8511,  -2.8020,  -3.3992],\n",
      "         ...,\n",
      "         [ -9.5907,  -2.9219,  -3.2474,  ...,  -3.7137,  -2.7675,  -3.5306],\n",
      "         [ -7.7027,  -3.2801,  -3.2202,  ...,  -2.8494,  -1.9560,  -3.9578],\n",
      "         [ -8.5242,  -2.8432,  -3.9931,  ...,  -2.2700,  -2.6714,  -3.4253]],\n",
      "\n",
      "        [[ -8.7483,  -4.4503,  -3.7446,  ...,  -3.8042,  -1.4647,  -4.0755],\n",
      "         [ -7.3016,  -4.3815,  -3.4549,  ...,  -4.3938,  -2.4546,  -3.7677],\n",
      "         [ -8.8002,  -4.1597,  -3.0999,  ...,  -2.9852,  -1.5560,  -2.9414],\n",
      "         ...,\n",
      "         [ -7.3477,  -3.4641,  -4.0270,  ...,  -3.1164,  -1.9371,  -3.6670],\n",
      "         [-10.2488,  -3.4112,  -3.2279,  ...,  -4.0614,  -2.5599,  -3.5015],\n",
      "         [ -8.3084,  -3.0139,  -3.9969,  ...,  -3.4415,  -2.2008,  -3.1898]]],\n",
      "       grad_fn=<LogSoftmaxBackward>) tensor([[ 6,  1,  5,  8,  4,  1,  8, 10, 10],\n",
      "        [ 1,  8,  3,  3,  6,  3,  6,  5,  1],\n",
      "        [10,  2,  7,  1,  1,  3,  3,  2,  6],\n",
      "        [ 1,  5,  2,  4,  3,  5,  6,  3,  1],\n",
      "        [ 1, 10, 10,  2,  9, 10,  2,  9,  2],\n",
      "        [ 4, 10,  6,  5,  4,  7,  1,  7,  1],\n",
      "        [ 9,  9,  2,  8,  9,  2,  4,  5,  3],\n",
      "        [ 1,  3,  6,  7,  5,  1, 10,  5,  7],\n",
      "        [ 8,  4,  4,  3,  4,  6,  8,  2, 10],\n",
      "        [ 6,  3,  3,  5,  4,  4,  9,  1,  1],\n",
      "        [ 5,  8,  1,  3,  1,  5,  3,  3,  1],\n",
      "        [ 4,  4,  5,  2,  2,  4,  6, 10,  3],\n",
      "        [ 1,  6,  2,  1,  5, 10,  5, 10,  6],\n",
      "        [10,  6,  5,  6,  1,  7,  8, 10,  4],\n",
      "        [ 3,  7,  8,  3,  5,  8,  2,  4,  1],\n",
      "        [ 8,  6,  2,  4,  8,  3,  9, 10,  8],\n",
      "        [ 5,  5,  3,  1,  7,  2,  5,  4,  3],\n",
      "        [ 8,  9,  2,  2, 10,  8,  9,  6,  4],\n",
      "        [ 6,  8, 10,  6,  3,  7,  4,  1,  1],\n",
      "        [ 4,  1,  2,  2,  1, 10,  6,  8, 10],\n",
      "        [ 3,  8,  7,  8,  3,  6,  6,  5,  8],\n",
      "        [ 6,  4,  3, 10,  8,  3,  2,  1,  9],\n",
      "        [10,  1,  4,  1,  1,  1,  6,  3,  3],\n",
      "        [ 7,  4,  8,  1,  4,  6,  2,  8,  2],\n",
      "        [ 2,  2,  9,  7,  5,  3,  3,  9, 10],\n",
      "        [ 3,  4,  1,  7,  3, 10,  5,  5,  6],\n",
      "        [ 7,  2,  8,  9,  3,  8,  4,  5,  1],\n",
      "        [ 6,  3,  9,  7,  5, 10,  5,  1,  9],\n",
      "        [ 6,  8,  1, 10,  1,  2,  8,  9,  9],\n",
      "        [10,  1,  1,  4,  6,  3,  6,  8,  7]])\n",
      "tensor(394607.8750, grad_fn=<KlDivBackward>)\n",
      "tensor([[[-8.0503, -1.8535, -1.4993,  ..., -1.4437, -3.8514, -3.5857],\n",
      "         [-7.3668, -2.4919, -1.0473,  ..., -2.1650, -3.8071, -3.5696],\n",
      "         [-6.4732, -1.5238, -1.7232,  ..., -1.8138, -4.7526, -3.2808],\n",
      "         ...,\n",
      "         [-6.5682, -1.4624, -1.5390,  ..., -1.9295, -3.6239, -2.9331],\n",
      "         [-7.7444, -1.1392, -1.5057,  ..., -2.4650, -3.1861, -2.1774],\n",
      "         [-7.2067, -1.9339, -1.1472,  ..., -2.0724, -4.4535, -3.1347]],\n",
      "\n",
      "        [[-8.6597, -2.0789, -1.8296,  ..., -2.2179, -3.2875, -2.4754],\n",
      "         [-7.4475, -2.5335, -1.2725,  ..., -1.8759, -3.8933, -3.1511],\n",
      "         [-7.6209, -1.6138, -2.3294,  ..., -1.2426, -4.4684, -3.5931],\n",
      "         ...,\n",
      "         [-8.0623, -1.4160, -1.1788,  ..., -1.9214, -3.4394, -3.3186],\n",
      "         [-6.4287, -1.9858, -0.8525,  ..., -2.5561, -3.4099, -3.2791],\n",
      "         [-6.1801, -1.9040, -1.9705,  ..., -1.2642, -3.8573, -3.1640]],\n",
      "\n",
      "        [[-9.2226, -2.3089, -1.7149,  ..., -2.0764, -3.5869, -1.6121],\n",
      "         [-7.3233, -1.4916, -1.8743,  ..., -1.9486, -2.4113, -2.9105],\n",
      "         [-8.7633, -1.2607, -1.4213,  ..., -2.2579, -3.8083, -3.0328],\n",
      "         ...,\n",
      "         [-8.3795, -1.8433, -1.2101,  ..., -1.7127, -3.4000, -3.4942],\n",
      "         [-8.2079, -1.3471, -2.0331,  ..., -2.4667, -2.5547, -3.0336],\n",
      "         [-6.4019, -1.5508, -2.6521,  ..., -2.6689, -1.8262, -2.2122]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-9.1538, -2.6331, -2.5492,  ..., -2.0358, -2.0180, -1.4094],\n",
      "         [-7.2004, -1.5125, -1.9118,  ..., -1.9444, -2.8649, -1.8842],\n",
      "         [-7.5202, -2.5533, -2.1645,  ..., -2.1786, -2.3740, -1.2051],\n",
      "         ...,\n",
      "         [-7.9791, -3.3492, -1.5628,  ..., -1.3798, -2.0999, -1.6221],\n",
      "         [-7.6244, -2.2509, -2.1458,  ..., -1.4268, -2.5887, -1.9700],\n",
      "         [-7.0184, -1.7265, -2.0405,  ..., -2.3289, -2.6378, -1.4593]],\n",
      "\n",
      "        [[-9.8087, -3.2837, -2.3091,  ..., -1.8956, -2.5677, -2.0193],\n",
      "         [-7.4807, -2.2984, -2.4105,  ..., -1.4802, -2.4986, -2.3673],\n",
      "         [-7.6821, -2.6353, -1.6796,  ..., -1.1536, -2.0466, -3.0020],\n",
      "         ...,\n",
      "         [-7.4439, -2.2905, -2.3096,  ..., -1.4555, -2.0479, -1.7076],\n",
      "         [-7.2351, -2.2449, -2.4101,  ..., -1.3845, -2.5700, -2.2838],\n",
      "         [-7.8688, -2.4837, -1.4911,  ..., -2.1046, -2.0806, -2.1489]],\n",
      "\n",
      "        [[-9.7809, -2.9758, -2.0870,  ..., -1.8815, -2.4055, -2.6200],\n",
      "         [-8.7079, -1.7034, -2.2112,  ..., -2.0531, -2.9901, -1.9037],\n",
      "         [-7.8832, -1.4096, -2.0621,  ..., -2.0755, -2.9047, -3.3511],\n",
      "         ...,\n",
      "         [-7.4248, -2.5836, -2.3872,  ..., -1.3633, -2.4823, -3.2991],\n",
      "         [-7.2676, -1.6336, -1.6847,  ..., -2.6117, -2.2980, -2.5233],\n",
      "         [-8.3142, -1.4665, -2.1246,  ..., -2.8061, -2.0129, -2.3685]]],\n",
      "       grad_fn=<LogSoftmaxBackward>) tensor([[ 6,  8,  8,  5,  2, 10, 10,  8,  8],\n",
      "        [ 6,  8,  2,  3,  3, 10, 10,  9,  8],\n",
      "        [ 2,  6,  9,  1,  1,  6,  5,  7, 10],\n",
      "        [ 2,  1,  8,  7,  3,  6,  1,  4,  7],\n",
      "        [ 7,  2,  8,  8,  4,  9,  3,  4,  4],\n",
      "        [ 6,  5,  1,  3,  4, 10,  4,  5,  2],\n",
      "        [ 2, 10,  9, 10,  6,  4,  8,  3,  8],\n",
      "        [ 2,  5,  2,  7,  2,  6,  9,  5,  7],\n",
      "        [ 1,  4,  4,  3,  8,  7,  1,  1,  9],\n",
      "        [ 7,  1,  7, 10,  6,  7,  6,  7,  2],\n",
      "        [ 6,  9,  2,  1, 10,  1,  2,  7,  5],\n",
      "        [ 2,  2,  6, 10,  9,  6,  2,  8,  5],\n",
      "        [ 2,  9,  5,  4,  6,  5,  8,  1,  6],\n",
      "        [ 3,  7,  6,  8,  8,  1,  4,  2,  8],\n",
      "        [ 3,  9,  2,  9, 10,  4,  2,  6,  1],\n",
      "        [10,  7,  9,  9,  6,  9,  5,  2,  6],\n",
      "        [10,  4,  1,  1,  1,  2,  2,  5,  6],\n",
      "        [ 7,  6,  1,  6,  7,  2,  3,  9,  9],\n",
      "        [ 6,  8,  2,  6,  4,  9,  2,  5,  1],\n",
      "        [10,  3,  5,  1,  5,  3,  9,  1,  1],\n",
      "        [ 3,  2,  1,  5,  5,  6,  9,  1,  9],\n",
      "        [ 2,  4,  9,  1, 10,  7,  2,  3,  6],\n",
      "        [ 6,  2,  1,  6,  2,  7,  3,  6,  3],\n",
      "        [ 4,  2,  5,  7,  9,  5, 10,  9,  8],\n",
      "        [ 4,  6,  4,  2, 10,  9,  4,  2,  6],\n",
      "        [ 5,  2,  1,  7,  6, 10,  4,  4,  8],\n",
      "        [ 9,  6,  5,  3,  5,  1,  2,  2, 10],\n",
      "        [10,  2,  8,  8,  1,  6,  2,  3,  8],\n",
      "        [ 2,  6,  3,  9,  4, 10,  3, 10,  2],\n",
      "        [ 9, 10,  5,  5,  4,  2,  9,  4,  3]])\n",
      "tensor(386022.3125, grad_fn=<KlDivBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-8.9883, -2.5696, -1.4298,  ..., -3.8510, -2.1397, -2.0918],\n",
      "         [-8.5362, -2.6937, -3.1370,  ..., -1.8854, -2.6877, -2.0839],\n",
      "         [-8.9282, -2.1887, -2.2971,  ..., -3.8569, -0.9835, -2.2690],\n",
      "         ...,\n",
      "         [-7.2700, -1.8914, -3.2496,  ..., -2.5491, -2.3772, -3.2633],\n",
      "         [-7.2238, -2.2511, -2.3577,  ..., -2.6042, -2.1268, -2.8295],\n",
      "         [-8.6966, -2.1429, -2.5223,  ..., -3.4619, -1.3798, -1.9817]],\n",
      "\n",
      "        [[-8.8524, -2.2490, -1.5240,  ..., -4.0613, -2.0026, -2.3724],\n",
      "         [-8.9724, -2.6095, -1.3805,  ..., -3.7779, -2.7258, -1.8968],\n",
      "         [-7.4153, -1.3709, -2.1509,  ..., -2.3789, -3.8681, -3.4361],\n",
      "         ...,\n",
      "         [-8.0366, -1.3491, -2.1945,  ..., -2.3130, -2.4094, -2.3112],\n",
      "         [-6.8883, -1.0840, -2.8858,  ..., -2.1855, -3.3781, -3.0761],\n",
      "         [-7.4506, -1.7626, -3.4523,  ..., -2.0498, -3.3335, -1.9650]],\n",
      "\n",
      "        [[-7.8095, -2.7104, -1.4204,  ..., -3.8801, -2.1621, -2.0319],\n",
      "         [-8.9938, -1.9252, -3.0991,  ..., -2.7373, -2.6813, -2.5672],\n",
      "         [-6.5967, -2.0477, -1.4002,  ..., -3.4125, -3.1406, -2.5000],\n",
      "         ...,\n",
      "         [-7.2150, -1.4422, -2.3215,  ..., -2.5865, -1.9977, -2.4121],\n",
      "         [-8.9488, -2.4117, -1.6014,  ..., -3.7897, -1.2045, -1.9526],\n",
      "         [-7.6313, -2.6343, -2.0337,  ..., -3.3817, -3.2629, -1.8230]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-9.0492, -2.9577, -1.6408,  ..., -3.9556, -1.1137, -2.8738],\n",
      "         [-6.1825, -2.4258, -2.7100,  ..., -2.3936, -1.9410, -3.0463],\n",
      "         [-8.2947, -3.1078, -1.6106,  ..., -3.6849, -1.1855, -2.5190],\n",
      "         ...,\n",
      "         [-7.2903, -1.8020, -1.9921,  ..., -2.5434, -1.9395, -3.8280],\n",
      "         [-8.1689, -1.6621, -2.1706,  ..., -2.7334, -2.9055, -3.2215],\n",
      "         [-5.6860, -2.9058, -2.4378,  ..., -2.5101, -1.5650, -2.9622]],\n",
      "\n",
      "        [[-9.4669, -3.3264, -2.4476,  ..., -3.2486, -0.6557, -2.2933],\n",
      "         [-7.8079, -3.2666, -3.2969,  ..., -3.3788, -1.6120, -1.9232],\n",
      "         [-8.3053, -3.2025, -2.9071,  ..., -3.0310, -1.0120, -2.4592],\n",
      "         ...,\n",
      "         [-9.7582, -3.2906, -2.4248,  ..., -3.2280, -0.5893, -2.1129],\n",
      "         [-8.5462, -2.6942, -4.1550,  ..., -2.4081, -1.3477, -2.9155],\n",
      "         [-7.6608, -3.0675, -2.8984,  ..., -2.8425, -1.6293, -2.0069]],\n",
      "\n",
      "        [[-8.7371, -2.4134, -1.7337,  ..., -2.9649, -1.8176, -2.2180],\n",
      "         [-8.8380, -2.3050, -1.6291,  ..., -1.9777, -2.1574, -2.6788],\n",
      "         [-8.1381, -1.6738, -1.7108,  ..., -2.5092, -2.6809, -3.3093],\n",
      "         ...,\n",
      "         [-8.6380, -2.1527, -1.9298,  ..., -2.5758, -2.5827, -3.3974],\n",
      "         [-7.3486, -3.0049, -2.4606,  ..., -2.6872, -2.7396, -2.0739],\n",
      "         [-8.0849, -2.4551, -2.3279,  ..., -2.3945, -1.9122, -2.0992]]],\n",
      "       grad_fn=<LogSoftmaxBackward>) tensor([[ 5,  1,  7, 10,  4,  3,  7,  1,  4],\n",
      "        [ 1,  8,  5,  4,  3,  5, 10,  6,  3],\n",
      "        [ 6,  9,  5,  6, 10,  8,  1,  9, 10],\n",
      "        [ 7,  1,  2,  6,  6,  2,  1,  8,  3],\n",
      "        [ 1,  3,  3,  6,  6,  9,  7,  4,  8],\n",
      "        [ 7,  3,  3,  7,  4,  5, 10,  5,  7],\n",
      "        [10, 10,  9,  3,  9, 10, 10,  8,  9],\n",
      "        [ 2,  3,  6,  9,  5,  4,  5,  2,  9],\n",
      "        [ 4,  1,  4,  8,  2,  4,  4,  7,  1],\n",
      "        [ 8,  5,  7,  2,  1,  9,  8,  6,  4],\n",
      "        [ 7,  8,  8,  3,  9,  2,  8,  4,  5],\n",
      "        [ 5,  1,  4,  2,  4,  1,  2,  6,  8],\n",
      "        [ 5, 10,  1, 10,  2,  8,  7,  7,  7],\n",
      "        [ 4,  1,  8,  4, 10,  4,  2,  8,  7],\n",
      "        [ 4,  8,  2,  1,  9,  1,  8,  3,  6],\n",
      "        [ 3,  7,  2,  9,  9, 10,  5,  6,  1],\n",
      "        [ 3,  1,  5,  2,  4,  4,  4,  4, 10],\n",
      "        [ 5,  4,  5,  8,  3,  6,  9,  8,  2],\n",
      "        [ 3,  4,  4, 10,  2,  5,  7, 10, 10],\n",
      "        [ 9,  4,  7,  6,  1, 10,  1,  3,  1],\n",
      "        [10,  3,  8,  4, 10,  8, 10,  3,  2],\n",
      "        [ 7,  2,  3,  9,  1,  4,  6, 10,  3],\n",
      "        [ 1,  3, 10,  6,  5,  9,  2,  5,  8],\n",
      "        [ 3,  5, 10,  2,  1,  6,  8,  9,  6],\n",
      "        [ 2,  4,  4,  3,  8,  8,  4,  3,  7],\n",
      "        [ 5,  3,  9,  7,  6,  2,  6,  9,  5],\n",
      "        [ 3,  1,  8, 10,  9, 10,  5,  7,  9],\n",
      "        [ 7,  1,  5,  3, 10,  8,  8, 10,  9],\n",
      "        [ 9,  2,  7,  7,  5,  1,  4,  9,  7],\n",
      "        [ 5,  8,  8,  5,  8,  3,  9,  4,  1]])\n",
      "tensor(392599.0938, grad_fn=<KlDivBackward>)\n",
      "tensor([[[-7.7852, -2.7273, -2.9498,  ..., -2.4749, -3.5835, -3.3187],\n",
      "         [-7.3244, -2.4850, -4.3903,  ..., -2.9391, -3.1568, -2.7360],\n",
      "         [-7.1062, -2.6277, -2.4723,  ..., -2.4995, -3.3820, -3.3653],\n",
      "         ...,\n",
      "         [-7.3273, -1.9463, -3.2645,  ..., -2.4729, -2.0124, -3.3369],\n",
      "         [-8.1753, -3.0537, -2.9266,  ..., -2.4794, -3.0226, -4.1696],\n",
      "         [-6.5459, -2.2106, -3.4041,  ..., -3.5052, -3.0107, -1.8073]],\n",
      "\n",
      "        [[-9.3653, -2.3541, -1.7314,  ..., -2.2735, -3.4253, -3.3140],\n",
      "         [-9.3806, -2.6367, -1.5183,  ..., -3.3814, -3.7320, -3.4205],\n",
      "         [-8.0898, -1.6829, -2.0536,  ..., -2.6093, -2.6932, -2.5550],\n",
      "         ...,\n",
      "         [-9.4908, -1.9553, -1.7089,  ..., -2.8822, -3.0471, -2.7250],\n",
      "         [-8.8803, -1.9695, -1.7767,  ..., -3.3240, -3.1510, -2.8818],\n",
      "         [-8.7879, -2.3505, -2.6873,  ..., -3.5950, -4.8986, -2.9948]],\n",
      "\n",
      "        [[-9.2761, -1.7777, -2.2233,  ..., -2.2000, -2.8836, -2.8605],\n",
      "         [-9.2966, -2.4898, -2.0858,  ..., -2.3408, -3.4243, -3.3306],\n",
      "         [-7.9560, -2.0776, -2.1264,  ..., -2.7369, -3.0542, -2.7475],\n",
      "         ...,\n",
      "         [-8.1392, -1.7325, -1.9515,  ..., -1.9352, -3.1479, -3.0767],\n",
      "         [-8.4096, -1.8741, -1.7389,  ..., -1.5415, -2.5009, -2.9804],\n",
      "         [-8.0545, -1.9744, -1.7479,  ..., -2.4977, -2.4413, -1.9033]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-8.1470, -2.1506, -1.8386,  ..., -2.7007, -3.0013, -2.6218],\n",
      "         [-7.9012, -1.8933, -2.5453,  ..., -2.3157, -2.5322, -2.5214],\n",
      "         [-6.5114, -2.1612, -1.9493,  ..., -2.3669, -3.3545, -1.8880],\n",
      "         ...,\n",
      "         [-9.0475, -1.1843, -2.9516,  ..., -3.3407, -2.1680, -2.5335],\n",
      "         [-7.1221, -1.6122, -1.5824,  ..., -2.8116, -2.9856, -2.8788],\n",
      "         [-8.1545, -1.8410, -2.3185,  ..., -3.1372, -2.7847, -1.9914]],\n",
      "\n",
      "        [[-8.4919, -2.0963, -1.8048,  ..., -2.4191, -3.2238, -2.7437],\n",
      "         [-8.5611, -2.7449, -1.6252,  ..., -1.7291, -3.1938, -2.6432],\n",
      "         [-8.5119, -2.4627, -2.3078,  ..., -2.7124, -3.1364, -2.6448],\n",
      "         ...,\n",
      "         [-8.8617, -1.8466, -2.0351,  ..., -2.5802, -2.2899, -2.2628],\n",
      "         [-7.3595, -2.7140, -1.7880,  ..., -2.9155, -2.9610, -3.6173],\n",
      "         [-8.5233, -2.4672, -2.7568,  ..., -3.3163, -3.7145, -2.7629]],\n",
      "\n",
      "        [[-8.2327, -2.3371, -2.7637,  ..., -2.0701, -3.4077, -2.4463],\n",
      "         [-7.8161, -2.4250, -2.8039,  ..., -2.9890, -3.1095, -3.0644],\n",
      "         [-7.9526, -2.7592, -1.7430,  ..., -2.7115, -2.7861, -2.6029],\n",
      "         ...,\n",
      "         [-7.8990, -1.3886, -3.4801,  ..., -4.0743, -3.1726, -2.0553],\n",
      "         [-7.5586, -2.1701, -1.8258,  ..., -2.6463, -2.8163, -2.7170],\n",
      "         [-7.9922, -2.4697, -2.1235,  ..., -2.7231, -3.0401, -3.0160]]],\n",
      "       grad_fn=<LogSoftmaxBackward>) tensor([[ 7,  9,  1,  1,  2,  5,  1,  7,  2],\n",
      "        [ 2,  5,  2,  7,  8,  5,  5,  2,  5],\n",
      "        [ 1, 10,  6,  7,  1,  9,  8,  8,  3],\n",
      "        [ 1,  2,  3,  5,  7,  3,  3,  7,  5],\n",
      "        [ 4,  3,  6,  5,  2,  6,  9, 10,  6],\n",
      "        [ 4,  1, 10,  9,  6,  7,  9,  3,  1],\n",
      "        [ 1, 10,  3,  8,  9,  1,  3,  3,  9],\n",
      "        [ 9,  8,  7,  3,  3, 10,  5,  1,  5],\n",
      "        [ 1,  2,  4,  8,  5,  9,  4,  2,  3],\n",
      "        [ 4,  1,  5, 10,  2,  2,  6,  9,  3],\n",
      "        [ 4,  5,  7,  4,  5,  3,  5,  5,  6],\n",
      "        [ 1,  4,  7,  5,  5,  3,  4,  3,  9],\n",
      "        [ 7,  3,  3,  2,  5,  2, 10,  7,  3],\n",
      "        [ 7,  4,  9,  4,  7,  9,  4,  3,  9],\n",
      "        [ 8,  7,  5,  2,  9, 10,  3,  2,  2],\n",
      "        [ 9, 10,  1, 10,  1,  6,  5, 10,  6],\n",
      "        [ 5,  7,  8,  7,  6,  6,  6,  3,  6],\n",
      "        [ 2,  7,  8,  1, 10,  9,  2,  3,  4],\n",
      "        [ 1, 10,  2,  1, 10,  3,  3,  6,  8],\n",
      "        [ 6,  2,  3,  2, 10,  1, 10,  4,  5],\n",
      "        [ 7,  6, 10,  2,  4,  5,  6,  3,  3],\n",
      "        [ 5,  4,  1,  2,  7,  3, 10,  4,  3],\n",
      "        [10,  4,  5,  6,  2,  6,  1,  1,  3],\n",
      "        [10,  9,  1,  2,  6,  2,  7, 10,  5],\n",
      "        [ 5,  9,  3,  4,  8,  9, 10,  7,  9],\n",
      "        [ 4,  4, 10,  1,  9,  6, 10,  9,  6],\n",
      "        [10,  7,  4,  3,  9,  8,  1,  7, 10],\n",
      "        [ 1,  9,  8,  1,  1,  5,  8,  2,  1],\n",
      "        [ 8, 10,  7,  4,  7,  5,  3,  2,  5],\n",
      "        [10,  9,  9,  4,  5,  7,  8,  4,  3]])\n",
      "tensor(385715.5625, grad_fn=<KlDivBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ -8.0309,  -2.0771,  -2.4560,  ...,  -2.1631,  -3.7331,  -3.2290],\n",
      "         [ -6.8165,  -2.6509,  -1.8513,  ...,  -1.1613,  -3.6555,  -3.9478],\n",
      "         [ -7.0170,  -2.4088,  -1.4648,  ...,  -2.1647,  -4.4594,  -3.7020],\n",
      "         ...,\n",
      "         [ -7.7841,  -2.5428,  -2.2696,  ...,  -1.9898,  -3.9101,  -2.4256],\n",
      "         [ -7.4984,  -1.6807,  -2.5428,  ...,  -2.3319,  -3.5676,  -2.8616],\n",
      "         [ -5.7282,  -2.6225,  -2.0321,  ...,  -1.3262,  -4.5095,  -3.6193]],\n",
      "\n",
      "        [[ -7.3972,  -2.5764,  -1.9637,  ...,  -2.8853,  -3.1784,  -3.4155],\n",
      "         [ -8.3359,  -2.9705,  -2.7824,  ...,  -1.9058,  -2.9187,  -3.5370],\n",
      "         [ -5.9456,  -2.1443,  -1.9955,  ...,  -2.2690,  -3.4628,  -3.5615],\n",
      "         ...,\n",
      "         [ -6.1778,  -1.4154,  -2.1805,  ...,  -2.4596,  -3.2713,  -3.9701],\n",
      "         [ -5.9148,  -1.7623,  -2.1031,  ...,  -1.6275,  -3.3959,  -3.8403],\n",
      "         [ -6.7613,  -3.0064,  -1.7406,  ...,  -1.8410,  -3.7613,  -2.8214]],\n",
      "\n",
      "        [[ -7.8922,  -1.3879,  -1.5823,  ...,  -2.3706,  -3.3901,  -3.5123],\n",
      "         [ -7.0886,  -1.6606,  -2.4097,  ...,  -2.0119,  -4.0240,  -3.0952],\n",
      "         [ -7.7252,  -2.2547,  -1.6890,  ...,  -2.7317,  -3.1909,  -4.1088],\n",
      "         ...,\n",
      "         [ -7.6013,  -1.0765,  -2.3146,  ...,  -2.7738,  -4.8862,  -3.7597],\n",
      "         [ -7.0423,  -1.8367,  -2.5127,  ...,  -1.4163,  -3.8038,  -4.1636],\n",
      "         [ -7.6518,  -1.8648,  -2.3731,  ...,  -1.7636,  -4.1871,  -3.3400]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ -9.2683,  -2.3095,  -2.4334,  ...,  -1.7917,  -3.3862,  -3.1584],\n",
      "         [ -7.0524,  -2.1785,  -1.7190,  ...,  -2.0455,  -4.1288,  -2.7064],\n",
      "         [ -6.6606,  -3.0634,  -3.1951,  ...,  -1.2311,  -4.2892,  -3.6678],\n",
      "         ...,\n",
      "         [ -7.8327,  -2.1705,  -3.1171,  ...,  -1.5987,  -3.1058,  -3.3780],\n",
      "         [ -7.2341,  -0.8234,  -2.6103,  ...,  -2.0840,  -3.0958,  -3.4102],\n",
      "         [ -7.5967,  -3.1162,  -3.2420,  ...,  -0.4921,  -4.1663,  -3.3112]],\n",
      "\n",
      "        [[-10.0736,  -1.8590,  -1.7436,  ...,  -2.2969,  -3.2359,  -4.1258],\n",
      "         [ -8.3925,  -1.8999,  -2.2829,  ...,  -2.2683,  -4.2859,  -2.8192],\n",
      "         [ -8.1793,  -1.4233,  -2.2850,  ...,  -1.6966,  -3.9263,  -3.5595],\n",
      "         ...,\n",
      "         [ -7.5097,  -1.7594,  -1.3559,  ...,  -2.3618,  -3.9972,  -3.2008],\n",
      "         [ -7.9140,  -1.2079,  -2.4804,  ...,  -1.1996,  -4.0727,  -3.7641],\n",
      "         [ -9.0766,  -2.3014,  -2.5622,  ...,  -1.0036,  -2.7490,  -3.4646]],\n",
      "\n",
      "        [[ -9.8668,  -2.2797,  -2.5283,  ...,  -2.3978,  -3.0760,  -2.8076],\n",
      "         [ -7.9700,  -2.2964,  -2.1362,  ...,  -1.8955,  -4.2835,  -2.7622],\n",
      "         [ -6.9574,  -1.9586,  -2.3630,  ...,  -1.7927,  -4.1738,  -3.3944],\n",
      "         ...,\n",
      "         [ -9.2582,  -2.0269,  -2.0465,  ...,  -1.5528,  -2.7337,  -3.3403],\n",
      "         [ -8.0325,  -2.3466,  -2.7020,  ...,  -2.7392,  -3.9518,  -2.1236],\n",
      "         [ -8.0889,  -2.0481,  -2.2826,  ...,  -1.9942,  -4.3818,  -2.1252]]],\n",
      "       grad_fn=<LogSoftmaxBackward>) tensor([[ 5,  3,  5,  3,  6,  2,  1,  3,  6],\n",
      "        [ 1,  9,  5,  1,  2,  4,  5,  2,  3],\n",
      "        [ 2,  1,  5,  6,  9,  4,  5,  2, 10],\n",
      "        [ 8,  6,  3,  6,  2,  4,  5,  4,  9],\n",
      "        [ 7,  7,  8,  6,  1,  2,  5,  1,  6],\n",
      "        [10,  6,  3,  4, 10,  2, 10,  5,  6],\n",
      "        [ 5,  1,  9,  4, 10,  6,  5,  3,  3],\n",
      "        [ 3,  3,  2,  1,  6, 10, 10,  5,  3],\n",
      "        [ 1,  7,  8,  2,  8,  2,  1,  4,  3],\n",
      "        [ 4,  2,  5,  7, 10,  8,  3,  1,  3],\n",
      "        [ 6,  5,  1,  9,  8,  2,  6,  6, 10],\n",
      "        [ 6,  2, 10,  5,  6,  1,  2,  2,  8],\n",
      "        [ 8,  3,  5,  5,  6,  1,  9,  9, 10],\n",
      "        [ 7,  4,  7,  1,  9,  5,  7,  3,  3],\n",
      "        [ 6,  8,  3,  4,  5,  2,  4,  2,  1],\n",
      "        [ 7,  8, 10,  4,  2,  1,  4,  3,  1],\n",
      "        [ 7,  8,  1,  4,  1,  2,  1,  5,  2],\n",
      "        [ 2,  2,  1,  3,  6,  6,  2,  8,  4],\n",
      "        [ 5,  2,  6,  8,  7,  5,  8, 10,  4],\n",
      "        [ 6,  5,  7,  9,  1,  4, 10,  6,  1],\n",
      "        [10,  1,  7,  8,  5,  5,  5,  9,  6],\n",
      "        [ 6,  1,  1,  8,  9,  9,  4, 10,  4],\n",
      "        [ 6,  9,  2,  9,  6,  2,  1,  3,  3],\n",
      "        [10,  7,  2,  1,  9,  8,  7,  8,  9],\n",
      "        [ 3,  7,  3, 10,  4,  2,  4,  5,  9],\n",
      "        [ 1,  2,  6,  6,  6, 10, 10,  9,  6],\n",
      "        [ 9,  3,  7,  3,  5,  1,  6,  5,  4],\n",
      "        [ 9,  7,  9,  5,  2,  5,  4, 10,  3],\n",
      "        [ 2,  5,  6,  4,  1,  3, 10,  5,  3],\n",
      "        [ 9, 10,  1,  5, 10,  5,  2,  9,  5]])\n",
      "tensor(393919.3125, grad_fn=<KlDivBackward>)\n",
      "tensor([[[-8.4099, -2.5970, -1.8274,  ..., -1.3236, -3.8851, -1.7881],\n",
      "         [-7.2315, -2.3929, -1.2250,  ..., -1.7103, -4.1115, -2.3001],\n",
      "         [-6.3211, -1.8790, -2.1793,  ..., -1.2183, -5.2450, -2.4898],\n",
      "         ...,\n",
      "         [-5.7972, -2.2400, -1.6863,  ..., -2.2607, -3.8611, -2.1976],\n",
      "         [-6.2639, -2.1969, -1.9933,  ..., -1.7752, -4.0642, -2.3982],\n",
      "         [-7.0591, -1.5880, -1.7036,  ..., -1.4923, -3.8495, -2.8083]],\n",
      "\n",
      "        [[-8.0169, -1.5044, -1.9224,  ..., -2.6408, -3.0693, -2.3981],\n",
      "         [-7.3594, -1.4448, -2.0535,  ..., -1.6635, -4.2913, -3.3583],\n",
      "         [-7.4415, -1.0984, -1.9291,  ..., -2.5949, -3.4386, -2.8942],\n",
      "         ...,\n",
      "         [-7.3200, -1.5971, -2.1631,  ..., -1.8576, -4.2812, -2.8821],\n",
      "         [-7.1227, -1.7465, -2.3774,  ..., -2.2950, -3.6936, -3.0675],\n",
      "         [-7.3400, -1.9896, -2.5581,  ..., -1.6596, -5.1581, -3.1699]],\n",
      "\n",
      "        [[-7.5478, -2.3658, -2.2947,  ..., -1.7437, -3.6150, -1.9683],\n",
      "         [-7.0373, -1.2876, -3.4575,  ..., -1.7686, -3.3958, -1.8361],\n",
      "         [-7.4671, -1.5033, -3.2743,  ..., -1.5732, -3.0246, -2.1561],\n",
      "         ...,\n",
      "         [-7.0304, -0.7471, -2.7590,  ..., -2.0699, -3.6773, -3.0386],\n",
      "         [-7.6812, -1.1618, -2.9894,  ..., -1.2377, -3.8182, -2.7686],\n",
      "         [-6.0117, -1.3517, -1.8540,  ..., -1.8789, -3.2093, -2.2257]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-9.5758, -3.4304, -2.8445,  ..., -1.7850, -2.9629, -1.2195],\n",
      "         [-8.3478, -2.5452, -2.7304,  ..., -1.1871, -3.7743, -1.5829],\n",
      "         [-8.8399, -3.3269, -1.6352,  ..., -1.6345, -2.7554, -1.0418],\n",
      "         ...,\n",
      "         [-9.6545, -2.8952, -2.2115,  ..., -2.1902, -2.2144, -1.8084],\n",
      "         [-8.8065, -3.0367, -2.5906,  ..., -1.4084, -3.3191, -1.4057],\n",
      "         [-9.0261, -2.3221, -2.9876,  ..., -1.9575, -3.6474, -1.1693]],\n",
      "\n",
      "        [[-9.1440, -2.5572, -2.0040,  ..., -2.9616, -2.9543, -1.7862],\n",
      "         [-7.7532, -1.9778, -2.1403,  ..., -1.7790, -3.3011, -2.6578],\n",
      "         [-8.5122, -2.0253, -1.6649,  ..., -2.3171, -2.6426, -1.7958],\n",
      "         ...,\n",
      "         [-8.2227, -1.6263, -2.6889,  ..., -2.0518, -2.8228, -1.5260],\n",
      "         [-7.2856, -2.5553, -2.0945,  ..., -1.2230, -2.5333, -2.1077],\n",
      "         [-8.1217, -2.9023, -2.0147,  ..., -1.6815, -4.1407, -2.2889]],\n",
      "\n",
      "        [[-8.3135, -1.8620, -2.5362,  ..., -1.6212, -3.4373, -2.1256],\n",
      "         [-7.4293, -1.7772, -1.8065,  ..., -1.7092, -2.7873, -2.1177],\n",
      "         [-8.4791, -2.5320, -2.0681,  ..., -2.1059, -2.4960, -1.7783],\n",
      "         ...,\n",
      "         [-7.9489, -1.3749, -2.2658,  ..., -1.7465, -3.4423, -2.7335],\n",
      "         [-8.8338, -1.9830, -1.2893,  ..., -3.1541, -2.9759, -2.9773],\n",
      "         [-8.9040, -1.0089, -2.6704,  ..., -1.8802, -3.2373, -2.0313]]],\n",
      "       grad_fn=<LogSoftmaxBackward>) tensor([[ 6,  8,  2,  2,  6,  3, 10,  9,  8],\n",
      "        [ 8,  9,  1,  8,  6,  8,  5,  8,  9],\n",
      "        [ 9,  1,  5,  8,  6,  7,  9,  2,  9],\n",
      "        [ 7,  1,  8,  2,  8,  3,  6,  9,  1],\n",
      "        [ 5,  5,  9,  5,  7,  2,  3,  3,  6],\n",
      "        [10,  8,  5,  8, 10,  7,  7,  8,  2],\n",
      "        [ 9,  5,  1,  2,  2,  1,  2,  9,  9],\n",
      "        [ 5,  9,  4,  8,  2,  2,  7, 10, 10],\n",
      "        [ 6,  8,  8,  4,  1,  8,  9, 10, 10],\n",
      "        [10,  8,  1, 10,  6,  7,  4, 10,  4],\n",
      "        [ 8,  2,  7,  2,  3,  4,  7,  6,  2],\n",
      "        [ 9, 10,  2,  4,  7,  3,  8,  1,  4],\n",
      "        [ 3,  9,  7,  7,  4,  5,  3,  6,  2],\n",
      "        [ 9, 10, 10, 10,  7,  8,  8, 10,  9],\n",
      "        [ 1,  2,  3, 10,  5,  2,  4,  1,  3],\n",
      "        [ 7, 10,  2,  8,  2, 10, 10,  1,  2],\n",
      "        [ 1,  3,  9,  7,  5,  2, 10,  7,  4],\n",
      "        [10, 10,  5,  7, 10,  9,  2,  3,  5],\n",
      "        [10,  6,  4,  4,  7,  8,  9, 10,  4],\n",
      "        [ 3,  9, 10,  7,  4,  1,  1,  9,  2],\n",
      "        [ 8,  1,  8,  1,  9,  3,  8,  2,  5],\n",
      "        [10,  5,  9,  9,  3,  2,  3,  5,  9],\n",
      "        [ 5,  1,  4,  7,  5,  4,  9,  5, 10],\n",
      "        [ 5,  9,  7,  4,  6,  9,  4,  4,  9],\n",
      "        [ 8,  4,  2,  4,  6,  3,  1,  7,  5],\n",
      "        [ 6,  4,  7,  5,  6,  6,  2,  7,  8],\n",
      "        [ 9,  2,  4,  7,  5,  9,  9,  3,  5],\n",
      "        [ 5,  6,  4,  6,  1,  2,  5,  5,  3],\n",
      "        [ 3,  4,  9,  2,  2,  9,  3,  3,  3],\n",
      "        [10,  1,  5,  1,  4,  9,  2,  9,  4]])\n",
      "tensor(389881.2188, grad_fn=<KlDivBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-8.4550, -2.9835, -3.4971,  ..., -1.6640, -2.3634, -3.4419],\n",
      "         [-6.8296, -2.8738, -3.0319,  ..., -1.4398, -1.8754, -3.4103],\n",
      "         [-5.5514, -3.9574, -2.1337,  ..., -1.8429, -1.2302, -2.7686],\n",
      "         ...,\n",
      "         [-7.3497, -4.2878, -2.3761,  ..., -1.7713, -1.1571, -2.4241],\n",
      "         [-6.9080, -2.4289, -3.0508,  ..., -1.8231, -1.5831, -3.7087],\n",
      "         [-5.2932, -2.8746, -2.5184,  ..., -2.5834, -1.8484, -2.4638]],\n",
      "\n",
      "        [[-7.4699, -1.8588, -2.8951,  ..., -1.9002, -3.7966, -3.6139],\n",
      "         [-6.2276, -2.1028, -1.8719,  ..., -2.5294, -1.6711, -3.0962],\n",
      "         [-6.9636, -1.9278, -2.9781,  ..., -2.5948, -1.4958, -3.8909],\n",
      "         ...,\n",
      "         [-8.0812, -2.9461, -2.1905,  ..., -1.3258, -1.9487, -3.0307],\n",
      "         [-6.4973, -2.9118, -3.2967,  ..., -1.8283, -2.0779, -3.3045],\n",
      "         [-7.6328, -2.1274, -2.9064,  ..., -1.1728, -2.6317, -3.0612]],\n",
      "\n",
      "        [[-8.6435, -2.4947, -2.5999,  ..., -1.6422, -2.8852, -3.5315],\n",
      "         [-7.0771, -2.1398, -2.1403,  ..., -1.9357, -1.6637, -2.5192],\n",
      "         [-8.0802, -2.0826, -2.9881,  ..., -1.8127, -2.1977, -2.5130],\n",
      "         ...,\n",
      "         [-7.6874, -3.3889, -2.3245,  ..., -1.6302, -1.7528, -2.9387],\n",
      "         [-7.8297, -3.2457, -2.7380,  ..., -1.2743, -1.7050, -2.8057],\n",
      "         [-6.7218, -2.2370, -2.1279,  ..., -2.4711, -2.5302, -2.6539]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-8.6260, -2.8275, -2.7246,  ..., -1.5044, -2.5995, -4.4422],\n",
      "         [-7.9298, -2.8652, -2.4508,  ..., -1.9782, -1.3156, -4.2899],\n",
      "         [-8.4789, -2.7527, -1.8151,  ..., -1.9223, -1.8423, -3.9149],\n",
      "         ...,\n",
      "         [-7.6635, -2.6156, -1.9882,  ..., -2.5964, -1.2501, -3.8796],\n",
      "         [-8.7601, -3.4738, -2.1507,  ..., -1.7197, -1.8173, -3.9945],\n",
      "         [-8.6384, -3.2334, -2.1418,  ..., -2.3673, -1.1116, -3.8689]],\n",
      "\n",
      "        [[-8.0308, -2.0176, -2.7999,  ..., -2.0755, -2.1465, -3.4618],\n",
      "         [-7.8456, -2.1508, -2.2640,  ..., -3.6501, -0.7386, -4.3185],\n",
      "         [-6.4895, -2.3536, -1.5800,  ..., -2.4485, -1.5926, -3.8775],\n",
      "         ...,\n",
      "         [-7.6811, -3.3992, -2.6275,  ..., -3.0607, -1.4846, -4.1394],\n",
      "         [-7.6481, -3.2927, -1.1564,  ..., -2.2778, -1.3717, -3.9443],\n",
      "         [-7.4978, -2.5613, -1.3968,  ..., -3.3318, -1.0772, -4.5148]],\n",
      "\n",
      "        [[-8.4042, -2.6880, -2.8652,  ..., -2.4080, -2.6360, -4.3324],\n",
      "         [-7.0192, -2.3808, -2.4352,  ..., -2.2377, -1.5850, -4.1448],\n",
      "         [-6.8364, -2.5568, -2.5720,  ..., -1.9243, -2.4392, -4.3081],\n",
      "         ...,\n",
      "         [-6.7739, -2.5805, -2.1381,  ..., -2.7233, -1.4302, -3.8395],\n",
      "         [-6.6066, -2.1781, -1.9744,  ..., -2.4377, -2.4480, -4.1195],\n",
      "         [-6.9795, -2.4582, -2.0735,  ..., -2.2752, -1.3982, -4.5242]]],\n",
      "       grad_fn=<LogSoftmaxBackward>) tensor([[ 8,  4,  8,  2,  8,  6,  8,  2,  1],\n",
      "        [ 4,  8,  6,  6,  7,  5,  9,  5,  3],\n",
      "        [ 4,  1, 10,  3,  3,  5,  9, 10,  5],\n",
      "        [ 7,  3,  3,  4,  9,  7,  9, 10,  6],\n",
      "        [ 6,  1, 10,  5,  1,  6,  2,  6,  6],\n",
      "        [ 4,  8,  1,  3,  8,  6,  8,  9,  7],\n",
      "        [ 2,  4,  1,  9,  8,  4,  5,  1,  3],\n",
      "        [ 7, 10,  8,  6,  9,  3,  9,  3,  8],\n",
      "        [ 9,  1,  5,  6, 10, 10,  8, 10,  2],\n",
      "        [ 5,  7,  3,  1, 10,  8, 10,  7,  2],\n",
      "        [ 4,  7,  9,  3,  3,  9,  2,  5,  3],\n",
      "        [ 8,  7,  3, 10,  1,  4,  9, 10,  4],\n",
      "        [ 3,  8,  4,  7,  3,  7,  6,  1,  9],\n",
      "        [ 2,  1,  9, 10,  9,  4,  6, 10,  1],\n",
      "        [ 4,  4,  2,  7,  6,  5,  6,  2,  9],\n",
      "        [ 3,  1,  4,  5,  4,  1,  6,  6,  9],\n",
      "        [ 1,  7, 10,  8,  9,  6,  3,  4,  4],\n",
      "        [ 1,  5,  7,  3,  6,  3,  6,  7,  3],\n",
      "        [ 5,  2,  3,  8,  8,  4,  8,  4,  7],\n",
      "        [ 9,  9,  3,  5,  3,  7,  1,  9,  6],\n",
      "        [ 9,  4,  3,  3,  1,  4, 10,  7,  6],\n",
      "        [10,  2,  4,  3,  1,  8,  1,  7,  1],\n",
      "        [ 4,  6,  3,  2,  9,  8,  6,  3,  7],\n",
      "        [10,  6,  5,  9,  5,  7,  2,  1,  2],\n",
      "        [ 8,  8,  2,  8,  6,  5,  9,  2,  4],\n",
      "        [10,  5,  9,  4,  1,  1,  9,  1,  3],\n",
      "        [ 5,  6, 10,  4,  3,  8,  8,  9,  6],\n",
      "        [ 3,  5, 10,  7,  1,  2,  5,  9,  3],\n",
      "        [ 7,  2,  9,  6,  3,  9,  4,  7,  3],\n",
      "        [ 8,  9,  9,  2,  7,  2, 10,  7,  7]])\n",
      "tensor(387302.5938, grad_fn=<KlDivBackward>)\n",
      "tensor([[[-6.8834, -4.1182, -1.4188,  ..., -4.3726, -1.9241, -1.7366],\n",
      "         [-6.1588, -4.8622, -1.5441,  ..., -2.8282, -2.9733, -2.5028],\n",
      "         [-5.9603, -4.7085, -0.9804,  ..., -3.7443, -2.6041, -3.1185],\n",
      "         ...,\n",
      "         [-6.6345, -3.8867, -0.8868,  ..., -3.6220, -3.0888, -2.7356],\n",
      "         [-5.8233, -3.4417, -1.4280,  ..., -3.6455, -3.2134, -1.9027],\n",
      "         [-6.1269, -3.6395, -1.1957,  ..., -3.8591, -3.1351, -2.1465]],\n",
      "\n",
      "        [[-7.0252, -2.7452, -1.7466,  ..., -3.7205, -2.1108, -2.2720],\n",
      "         [-6.1144, -2.9979, -1.7182,  ..., -3.4512, -2.9980, -2.8466],\n",
      "         [-6.3195, -3.9239, -1.3293,  ..., -2.8217, -2.5221, -1.9405],\n",
      "         ...,\n",
      "         [-6.0075, -2.9085, -1.3737,  ..., -2.7708, -3.2613, -2.2141],\n",
      "         [-6.6341, -2.9706, -0.9719,  ..., -4.3057, -2.2380, -2.7508],\n",
      "         [-6.6951, -2.5839, -1.8103,  ..., -3.8345, -2.9165, -2.9175]],\n",
      "\n",
      "        [[-6.6927, -3.1472, -1.3743,  ..., -3.8510, -2.1974, -2.2602],\n",
      "         [-7.1872, -2.1734, -2.0449,  ..., -2.7467, -3.3785, -2.7272],\n",
      "         [-6.9548, -2.5013, -1.7012,  ..., -2.5795, -3.1834, -3.0533],\n",
      "         ...,\n",
      "         [-7.3953, -3.5970, -1.0460,  ..., -3.0846, -3.2451, -3.0698],\n",
      "         [-5.8931, -3.3129, -1.5631,  ..., -2.7782, -3.2634, -2.4018],\n",
      "         [-5.7058, -2.2894, -1.9294,  ..., -3.3907, -3.7759, -2.9565]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-7.9403, -3.5851, -2.2871,  ..., -4.1413, -2.4496, -2.2185],\n",
      "         [-8.8926, -3.7984, -3.4596,  ..., -4.3079, -2.6107, -2.1860],\n",
      "         [-7.7611, -3.7813, -2.4085,  ..., -3.7791, -2.0720, -2.4115],\n",
      "         ...,\n",
      "         [-8.2347, -3.3633, -2.4519,  ..., -4.5889, -1.4703, -2.1972],\n",
      "         [-8.0468, -3.8178, -3.9674,  ..., -3.5767, -2.2903, -3.3039],\n",
      "         [-7.9924, -3.8496, -2.2618,  ..., -3.4917, -2.3704, -2.6910]],\n",
      "\n",
      "        [[-8.8620, -3.5433, -2.4377,  ..., -4.4239, -1.4034, -2.0596],\n",
      "         [-7.4142, -3.4202, -1.6493,  ..., -3.6602, -2.7366, -1.9088],\n",
      "         [-7.1001, -3.4978, -3.0445,  ..., -4.4034, -2.0721, -2.6873],\n",
      "         ...,\n",
      "         [-7.8381, -2.9627, -2.8232,  ..., -3.9429, -1.9821, -2.8743],\n",
      "         [-7.5782, -3.6393, -2.4806,  ..., -3.1113, -1.8734, -2.4514],\n",
      "         [-6.9481, -3.2295, -2.2454,  ..., -3.1111, -1.9341, -2.3158]],\n",
      "\n",
      "        [[-7.9908, -3.5016, -1.1662,  ..., -4.6251, -2.8576, -1.8625],\n",
      "         [-7.2327, -2.5397, -2.7190,  ..., -3.3840, -2.3985, -1.4476],\n",
      "         [-7.6629, -3.2208, -1.1627,  ..., -4.0596, -2.6752, -2.4169],\n",
      "         ...,\n",
      "         [-5.8510, -3.3096, -1.5627,  ..., -4.5182, -3.3583, -1.7879],\n",
      "         [-6.9374, -2.6930, -1.8248,  ..., -2.9687, -3.0535, -2.1641],\n",
      "         [-7.0900, -3.1875, -1.4346,  ..., -3.6329, -3.0383, -2.1435]]],\n",
      "       grad_fn=<LogSoftmaxBackward>) tensor([[ 3,  9,  2,  2,  6,  4,  5,  2,  6],\n",
      "        [ 2,  3,  5,  3,  8,  4,  9,  7,  8],\n",
      "        [ 7,  7,  4,  1,  8,  5,  3,  7,  7],\n",
      "        [ 7,  9, 10,  6,  2,  9, 10,  9,  3],\n",
      "        [ 8,  5,  4,  7,  7,  8,  2,  1,  1],\n",
      "        [ 7,  4,  3,  3,  9,  2,  5, 10,  1],\n",
      "        [ 7,  2,  7, 10,  4,  7,  2, 10,  4],\n",
      "        [ 6,  7,  7,  3,  3,  8,  5, 10,  1],\n",
      "        [ 7, 10,  7,  9,  1,  3,  2, 10, 10],\n",
      "        [ 1,  2,  7,  5,  7,  1, 10,  4,  1],\n",
      "        [ 1, 10,  2,  8,  6,  1,  9,  1,  4],\n",
      "        [ 9,  5, 10,  6, 10, 10,  8,  7,  9],\n",
      "        [ 1, 10,  8,  2,  4,  5,  6,  2,  1],\n",
      "        [ 1,  2,  5,  2,  3,  5,  6,  2,  3],\n",
      "        [ 7,  1,  3,  3, 10,  1, 10,  4,  8],\n",
      "        [ 8,  3,  7,  2, 10,  7,  8,  6,  7],\n",
      "        [10,  5,  5,  2,  5,  5,  2,  8,  9],\n",
      "        [ 3,  2,  3,  3,  3,  1,  7,  1,  5],\n",
      "        [ 3,  1,  9,  2,  1,  3,  1,  7,  4],\n",
      "        [10,  9,  1, 10,  7,  5,  5,  1,  2],\n",
      "        [ 8,  5,  5,  9,  5,  7,  4,  2,  5],\n",
      "        [ 5,  5,  1,  7,  9,  5,  6,  1, 10],\n",
      "        [ 9,  2,  9,  9,  2,  5,  5,  2,  1],\n",
      "        [ 2,  8,  6,  5,  1,  8,  4,  6,  4],\n",
      "        [ 4,  8,  8,  9,  8,  4,  6,  3,  8],\n",
      "        [ 5,  2,  3,  4,  7,  3,  3,  1,  8],\n",
      "        [ 6,  5,  4, 10,  6,  5,  5,  3,  1],\n",
      "        [ 5,  2, 10,  1,  6,  1,  7,  5,  1],\n",
      "        [ 8,  7,  1,  1,  4,  7,  6,  2,  5],\n",
      "        [ 2,  5,  9,  4,  6,  8,  7,  9,  6]])\n",
      "tensor(393173.6875, grad_fn=<KlDivBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ -8.5835,  -3.4841,  -3.0031,  ...,  -1.0436,  -3.4863,  -2.4134],\n",
      "         [ -6.8819,  -3.1626,  -2.7444,  ...,  -1.5908,  -2.7158,  -1.9327],\n",
      "         [ -7.3672,  -2.6947,  -2.6577,  ...,  -1.8993,  -2.7553,  -1.9024],\n",
      "         ...,\n",
      "         [ -7.7022,  -4.2047,  -2.7235,  ...,  -2.4368,  -3.7119,  -3.0456],\n",
      "         [ -7.1616,  -3.1409,  -1.9512,  ...,  -1.3642,  -4.8656,  -2.6074],\n",
      "         [ -8.2057,  -3.8415,  -2.5858,  ...,  -1.6895,  -3.1653,  -1.9008]],\n",
      "\n",
      "        [[ -8.8131,  -3.7407,  -2.7678,  ...,  -2.0190,  -3.5917,  -2.6485],\n",
      "         [ -9.6056,  -2.5318,  -2.5042,  ...,  -1.9846,  -3.1428,  -2.0980],\n",
      "         [ -9.5626,  -3.5737,  -2.9097,  ...,  -1.9179,  -3.1275,  -3.0009],\n",
      "         ...,\n",
      "         [ -8.5078,  -4.0170,  -2.8542,  ...,  -3.0236,  -2.7475,  -2.1927],\n",
      "         [ -9.2287,  -3.2094,  -2.1290,  ...,  -2.8714,  -2.8302,  -2.2562],\n",
      "         [ -7.0902,  -3.6871,  -2.9169,  ...,  -3.1065,  -2.8225,  -1.8487]],\n",
      "\n",
      "        [[ -8.8308,  -3.3480,  -2.4984,  ...,  -1.4774,  -3.4055,  -3.2403],\n",
      "         [ -7.4518,  -2.6951,  -1.8044,  ...,  -1.8955,  -3.0697,  -2.7292],\n",
      "         [ -8.4301,  -2.3787,  -1.9767,  ...,  -2.0494,  -2.4124,  -2.9006],\n",
      "         ...,\n",
      "         [ -7.1776,  -2.4625,  -1.5525,  ...,  -2.1089,  -3.8618,  -2.5757],\n",
      "         [ -7.6131,  -2.6290,  -1.8888,  ...,  -3.2775,  -2.8488,  -2.5868],\n",
      "         [ -9.5726,  -2.7380,  -2.5725,  ...,  -2.5990,  -3.0639,  -3.3501]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-10.2619,  -3.8078,  -3.1690,  ...,  -2.3922,  -2.6363,  -1.0156],\n",
      "         [ -9.6103,  -3.5461,  -2.9160,  ...,  -2.3207,  -2.2924,  -0.9840],\n",
      "         [ -8.6829,  -3.4365,  -2.5737,  ...,  -1.7608,  -3.2578,  -0.8428],\n",
      "         ...,\n",
      "         [ -9.0359,  -4.1913,  -2.2657,  ...,  -1.9235,  -1.5557,  -1.3475],\n",
      "         [ -9.3904,  -4.0999,  -3.4719,  ...,  -2.8159,  -2.6734,  -1.0257],\n",
      "         [ -8.3802,  -3.2388,  -2.7593,  ...,  -2.7088,  -2.2953,  -0.8188]],\n",
      "\n",
      "        [[ -9.8236,  -3.3446,  -2.6547,  ...,  -1.9808,  -2.7270,  -1.0211],\n",
      "         [ -8.7186,  -3.5128,  -2.4739,  ...,  -2.4929,  -1.9465,  -0.9998],\n",
      "         [ -9.3780,  -3.6000,  -2.2825,  ...,  -2.0310,  -1.7384,  -1.4634],\n",
      "         ...,\n",
      "         [ -9.3153,  -3.5351,  -1.6069,  ...,  -1.9378,  -1.8052,  -1.9980],\n",
      "         [ -9.2120,  -2.9813,  -2.0047,  ...,  -2.6886,  -2.1686,  -1.1218],\n",
      "         [ -7.8812,  -3.0671,  -1.3221,  ...,  -1.9519,  -3.3915,  -1.8287]],\n",
      "\n",
      "        [[ -9.6743,  -3.1954,  -2.9929,  ...,  -1.3977,  -2.4148,  -1.8704],\n",
      "         [ -9.5494,  -3.0305,  -1.8422,  ...,  -1.7493,  -2.2227,  -2.3521],\n",
      "         [ -9.1003,  -2.7495,  -1.9169,  ...,  -1.8775,  -2.7125,  -1.2501],\n",
      "         ...,\n",
      "         [ -9.2034,  -2.8209,  -2.4025,  ...,  -2.7211,  -2.9331,  -1.2789],\n",
      "         [ -8.8196,  -3.2998,  -2.3590,  ...,  -3.0325,  -2.5047,  -0.7351],\n",
      "         [ -8.4536,  -3.1106,  -1.5762,  ...,  -2.1710,  -2.5469,  -1.4934]]],\n",
      "       grad_fn=<LogSoftmaxBackward>) tensor([[ 5,  4,  7,  7,  1,  3,  8,  4,  6],\n",
      "        [ 6,  1,  3,  5,  5,  4,  4,  2,  6],\n",
      "        [10, 10,  5,  7,  3,  8,  2,  5, 10],\n",
      "        [ 6,  3,  9,  7,  4,  9,  5,  6,  8],\n",
      "        [ 9,  7,  9,  3,  9,  7,  1,  3, 10],\n",
      "        [ 5,  2,  5,  4,  9,  6,  6,  7,  8],\n",
      "        [ 1,  3,  7,  3,  3,  2,  3,  5,  3],\n",
      "        [ 1,  3,  8, 10,  2,  7,  4,  8,  7],\n",
      "        [ 8,  5,  6,  8,  6,  7,  7,  1,  5],\n",
      "        [ 2,  3,  9,  9,  7,  3,  4,  4, 10],\n",
      "        [ 7,  5,  7,  8,  6,  2,  4,  3,  1],\n",
      "        [ 2,  1,  3,  9,  6,  8,  4, 10, 10],\n",
      "        [10,  4,  6,  8,  6,  9,  5,  3,  3],\n",
      "        [10,  3,  9, 10,  2,  1,  4,  7,  5],\n",
      "        [ 6,  9,  3,  4,  1,  3,  3,  7, 10],\n",
      "        [ 1,  8,  6,  4,  7,  4,  3,  9,  7],\n",
      "        [ 3, 10,  7,  6,  4,  4,  9,  6,  2],\n",
      "        [ 6,  9,  1,  2,  9,  4,  7,  8, 10],\n",
      "        [ 9,  3,  2,  7,  5,  3,  8,  3,  9],\n",
      "        [ 3, 10,  2,  5,  2,  9,  6,  6, 10],\n",
      "        [10,  5,  7,  5, 10,  6,  3,  3,  4],\n",
      "        [ 3,  1,  1,  1,  5,  3,  3,  6,  2],\n",
      "        [ 9,  1,  7, 10,  4,  1,  5,  7,  1],\n",
      "        [10, 10, 10,  3,  8, 10,  7,  7,  7],\n",
      "        [10, 10,  5,  3,  2,  6,  2,  4, 10],\n",
      "        [ 7,  9,  6,  9,  1,  3,  5,  8,  7],\n",
      "        [ 8,  5,  2,  2,  5,  7,  6,  3,  3],\n",
      "        [ 5,  6,  5,  3,  4,  7,  5,  2,  9],\n",
      "        [ 4,  5,  4,  9,  1,  3,  2,  8,  7],\n",
      "        [ 7,  6,  3,  7,  3,  2,  2, 10,  9]])\n",
      "tensor(392427.0625, grad_fn=<KlDivBackward>)\n",
      "tensor([[[-4.8927, -3.5073, -2.2707,  ..., -3.1617, -2.9175, -2.5120],\n",
      "         [-5.1556, -4.2633, -2.6473,  ..., -2.9378, -2.0149, -2.4572],\n",
      "         [-4.9433, -4.4809, -1.9448,  ..., -2.6361, -1.9888, -1.8062],\n",
      "         ...,\n",
      "         [-4.5402, -4.4568, -1.5696,  ..., -2.2077, -3.0393, -1.9738],\n",
      "         [-3.9886, -3.4149, -2.5076,  ..., -2.3538, -2.8568, -3.6392],\n",
      "         [-6.3548, -3.3815, -2.0767,  ..., -1.9001, -2.6693, -1.6275]],\n",
      "\n",
      "        [[-6.0068, -2.8105, -2.7346,  ..., -3.2712, -3.2953, -3.3895],\n",
      "         [-5.3005, -3.1389, -2.3128,  ..., -3.3014, -2.3591, -3.4629],\n",
      "         [-6.6221, -3.2763, -3.3499,  ..., -3.1755, -2.7411, -3.5813],\n",
      "         ...,\n",
      "         [-5.0642, -3.0973, -2.3510,  ..., -2.5096, -2.3094, -3.4820],\n",
      "         [-6.4268, -3.1607, -2.4746,  ..., -3.1296, -2.9196, -3.3544],\n",
      "         [-5.1402, -4.2415, -2.6121,  ..., -2.7916, -2.5675, -3.6554]],\n",
      "\n",
      "        [[-5.8046, -3.5067, -3.1037,  ..., -3.2659, -2.2721, -2.2524],\n",
      "         [-5.0138, -3.7881, -2.7764,  ..., -2.0445, -1.9826, -2.3874],\n",
      "         [-4.7264, -3.3528, -2.6498,  ..., -1.7472, -2.8433, -3.4990],\n",
      "         ...,\n",
      "         [-5.8639, -3.3657, -1.4826,  ..., -2.9513, -2.3510, -3.3835],\n",
      "         [-5.5655, -2.8090, -1.7919,  ..., -2.6040, -2.7021, -2.3725],\n",
      "         [-5.4387, -3.4022, -2.3018,  ..., -2.5634, -2.4228, -1.9758]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-8.2768, -3.7546, -3.1955,  ..., -2.3393, -3.3028, -3.5840],\n",
      "         [-7.4453, -5.1022, -1.9512,  ..., -2.5631, -1.9041, -3.2496],\n",
      "         [-6.5312, -4.1617, -2.3787,  ..., -2.5648, -1.6601, -3.0024],\n",
      "         ...,\n",
      "         [-7.4498, -3.7442, -2.2954,  ..., -2.5807, -2.7185, -4.4973],\n",
      "         [-7.0763, -4.7309, -1.9581,  ..., -2.9613, -1.6637, -2.8075],\n",
      "         [-7.0602, -4.5452, -1.9454,  ..., -2.1270, -2.1763, -3.4296]],\n",
      "\n",
      "        [[-6.2594, -2.6893, -2.4157,  ..., -2.9900, -4.8058, -3.7558],\n",
      "         [-7.4496, -2.8505, -3.3355,  ..., -3.2644, -3.8591, -4.3616],\n",
      "         [-7.6234, -3.1665, -2.3983,  ..., -3.0964, -3.8868, -3.8801],\n",
      "         ...,\n",
      "         [-7.7559, -3.7169, -2.6124,  ..., -2.4848, -3.2108, -2.0367],\n",
      "         [-8.6015, -3.9693, -2.0767,  ..., -4.2725, -3.7038, -4.1976],\n",
      "         [-7.8380, -3.5180, -1.7108,  ..., -2.2248, -3.9124, -2.7037]],\n",
      "\n",
      "        [[-6.3315, -3.0637, -1.6308,  ..., -2.7883, -4.7537, -3.4935],\n",
      "         [-7.8867, -3.2208, -1.8579,  ..., -2.9247, -4.5830, -2.7328],\n",
      "         [-6.5803, -2.8204, -2.0553,  ..., -3.0046, -3.6697, -3.6277],\n",
      "         ...,\n",
      "         [-6.7677, -2.9464, -2.1867,  ..., -2.9294, -3.8965, -3.0461],\n",
      "         [-7.2439, -3.1925, -2.4785,  ..., -3.1869, -3.3984, -4.2404],\n",
      "         [-6.0144, -2.9465, -1.5805,  ..., -2.1261, -4.6660, -3.8825]]],\n",
      "       grad_fn=<LogSoftmaxBackward>) tensor([[ 9,  8, 10,  4,  9,  3,  2,  6, 10],\n",
      "        [ 9,  4,  5,  8,  5,  7,  6,  4,  5],\n",
      "        [10,  2,  2,  5,  8,  8,  7, 10,  8],\n",
      "        [ 1,  1,  9,  6,  1,  6,  5,  1,  5],\n",
      "        [ 8,  8,  6,  5,  8,  9,  4, 10,  3],\n",
      "        [ 7,  6,  3,  6,  9,  9,  8,  5,  6],\n",
      "        [10,  3,  4,  8,  4,  3,  6,  9,  6],\n",
      "        [ 2,  4, 10,  3,  3,  4,  6,  3,  5],\n",
      "        [ 9,  4,  9, 10,  6,  6,  9,  3,  9],\n",
      "        [ 3, 10,  4,  3,  2,  3, 10,  3,  8],\n",
      "        [ 8,  8,  2,  3,  8,  4,  3,  6,  5],\n",
      "        [10,  1,  8,  8, 10,  2, 10,  4,  2],\n",
      "        [ 5,  2,  9,  9,  8,  3,  1,  5,  5],\n",
      "        [ 2,  9,  3,  4,  4,  1,  9,  7,  5],\n",
      "        [ 1,  6, 10,  5,  5,  8,  6,  8,  9],\n",
      "        [ 8,  5,  2,  7,  9,  2,  2,  2, 10],\n",
      "        [10,  9,  3,  2,  6, 10,  4,  7,  6],\n",
      "        [ 3,  1,  1, 10, 10,  9,  8,  2,  4],\n",
      "        [ 2, 10,  9,  3,  6,  1,  4,  3,  2],\n",
      "        [ 8,  3,  8,  8,  1,  2,  7,  5, 10],\n",
      "        [ 9,  9,  7,  4,  8,  9,  4,  8,  9],\n",
      "        [ 1,  1,  1,  3,  6,  3,  2,  1,  8],\n",
      "        [ 6,  2,  5,  7,  9,  2, 10,  1,  5],\n",
      "        [ 4,  1,  6,  8,  9,  9, 10,  9,  4],\n",
      "        [ 4,  8,  7,  9,  6,  4, 10,  1,  4],\n",
      "        [ 4,  1,  3, 10,  1,  9,  7,  8,  5],\n",
      "        [ 5,  8,  9,  3,  8,  3,  9,  2,  6],\n",
      "        [ 8,  8,  7, 10,  4,  3,  8,  8,  3],\n",
      "        [ 4,  4, 10, 10,  1,  6,  5,  6,  7],\n",
      "        [ 6,  1,  2,  8,  9,  1,  9,  7,  3]])\n",
      "tensor(391402.4375, grad_fn=<KlDivBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-12.2439,  -2.0432,  -4.0043,  ...,  -2.9572,  -4.1879,  -4.0169],\n",
      "         [-10.3995,  -1.7207,  -2.9050,  ...,  -1.6106,  -3.4019,  -3.5648],\n",
      "         [-11.0546,  -1.9456,  -2.9454,  ...,  -3.0831,  -4.1149,  -3.0242],\n",
      "         ...,\n",
      "         [-12.4393,  -1.7196,  -3.7912,  ...,  -3.2925,  -4.6297,  -4.0563],\n",
      "         [-11.1165,  -1.7953,  -2.3561,  ...,  -1.6216,  -3.4290,  -2.7587],\n",
      "         [-11.0054,  -1.1023,  -2.6034,  ...,  -1.8923,  -3.5524,  -3.9801]],\n",
      "\n",
      "        [[-12.5950,  -2.6382,  -3.4111,  ...,  -3.5086,  -4.6873,  -3.3329],\n",
      "         [ -9.9841,  -2.8181,  -2.7481,  ...,  -1.3355,  -3.3912,  -2.7073],\n",
      "         [-11.2497,  -3.4175,  -2.8210,  ...,  -1.8986,  -3.4739,  -2.9435],\n",
      "         ...,\n",
      "         [-11.4252,  -2.4923,  -2.4568,  ...,  -2.7353,  -3.6650,  -3.5153],\n",
      "         [-10.2964,  -2.0941,  -2.3796,  ...,  -1.7336,  -2.4531,  -3.0293],\n",
      "         [-10.5630,  -1.5368,  -2.1035,  ...,  -1.5091,  -2.9948,  -3.8950]],\n",
      "\n",
      "        [[-11.4207,  -1.6895,  -2.7983,  ...,  -2.5570,  -3.5512,  -3.4871],\n",
      "         [-10.4053,  -1.8775,  -2.0644,  ...,  -1.9080,  -3.1139,  -2.9515],\n",
      "         [-11.7682,  -1.5397,  -2.2898,  ...,  -1.6881,  -3.3136,  -2.5885],\n",
      "         ...,\n",
      "         [-10.6874,  -2.0386,  -2.1489,  ...,  -1.8998,  -3.3021,  -3.0563],\n",
      "         [ -9.9509,  -1.7487,  -2.5398,  ...,  -2.0144,  -2.6958,  -2.9916],\n",
      "         [-11.6236,  -2.0781,  -2.2962,  ...,  -2.4194,  -3.0933,  -2.8083]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-12.3207,  -2.8338,  -2.0259,  ...,  -2.3973,  -3.0188,  -2.3961],\n",
      "         [-10.9407,  -2.2718,  -2.1427,  ...,  -2.0039,  -3.1480,  -2.0282],\n",
      "         [-10.4626,  -2.3999,  -2.1433,  ...,  -1.8617,  -2.5825,  -2.5984],\n",
      "         ...,\n",
      "         [-10.2050,  -3.0348,  -1.1918,  ...,  -1.9675,  -3.3618,  -2.2263],\n",
      "         [-11.2821,  -3.5416,  -2.8197,  ...,  -0.8998,  -3.7828,  -1.8006],\n",
      "         [ -9.8162,  -2.5851,  -1.7328,  ...,  -1.5485,  -3.0648,  -2.5220]],\n",
      "\n",
      "        [[-11.5018,  -2.8465,  -1.9303,  ...,  -2.9578,  -2.4771,  -2.2814],\n",
      "         [-10.9241,  -2.9789,  -2.6970,  ...,  -2.8374,  -2.0653,  -2.4271],\n",
      "         [-10.2364,  -2.5105,  -1.7575,  ...,  -2.0048,  -2.4700,  -2.0084],\n",
      "         ...,\n",
      "         [-11.6354,  -2.5299,  -2.4346,  ...,  -2.3845,  -3.4484,  -1.8317],\n",
      "         [-11.2125,  -2.5520,  -2.2445,  ...,  -2.0897,  -3.0430,  -1.3744],\n",
      "         [ -9.2757,  -2.6437,  -1.7774,  ...,  -2.1469,  -2.0685,  -1.3548]],\n",
      "\n",
      "        [[-12.0118,  -2.7213,  -2.3251,  ...,  -2.3433,  -3.0563,  -2.5021],\n",
      "         [-10.1868,  -3.0069,  -2.2056,  ...,  -0.8889,  -3.2849,  -2.5743],\n",
      "         [-10.9261,  -2.3877,  -2.0029,  ...,  -1.7971,  -2.8186,  -2.4425],\n",
      "         ...,\n",
      "         [-11.8356,  -2.7179,  -2.9334,  ...,  -1.3730,  -2.3738,  -2.2214],\n",
      "         [-11.7916,  -2.3969,  -2.7058,  ...,  -2.5037,  -3.0094,  -2.3931],\n",
      "         [-11.3152,  -3.3209,  -2.2196,  ...,  -2.2148,  -2.7673,  -2.7385]]],\n",
      "       grad_fn=<LogSoftmaxBackward>) tensor([[ 8,  5,  4,  6,  2,  1,  4,  4,  5],\n",
      "        [ 3,  3,  5,  7, 10,  1,  2,  9,  4],\n",
      "        [10,  4,  9,  9,  8, 10,  8,  5,  7],\n",
      "        [ 1,  8,  1,  2, 10,  1,  2,  2,  2],\n",
      "        [ 9,  9,  7,  3,  5, 10,  1,  7,  6],\n",
      "        [10,  4,  5,  4,  3,  7,  1,  4,  7],\n",
      "        [10,  8,  1,  1,  2,  8,  8,  7,  8],\n",
      "        [ 1,  4,  6, 10,  7, 10,  9,  7,  1],\n",
      "        [ 7, 10,  1,  8,  9,  2,  2,  6,  2],\n",
      "        [ 7,  2,  3,  4, 10,  2,  2,  9,  8],\n",
      "        [ 2,  3,  2, 10,  4,  4,  5,  7,  4],\n",
      "        [ 6,  2,  7,  7,  3,  2,  9, 10,  9],\n",
      "        [ 5,  9, 10,  5,  8,  6,  9,  2,  5],\n",
      "        [ 7,  2,  7,  7,  1,  7,  2,  5,  2],\n",
      "        [ 3,  6,  3, 10,  5,  3,  8, 10, 10],\n",
      "        [ 6,  4, 10,  4,  6,  2,  6,  7,  7],\n",
      "        [ 2,  6,  8,  3,  4,  6,  1,  1,  5],\n",
      "        [ 1, 10,  6,  3,  5,  4,  4,  7,  2],\n",
      "        [10,  7,  1,  9,  7,  9,  8,  5,  3],\n",
      "        [ 8,  1,  3,  5,  4,  1,  6,  7,  6],\n",
      "        [ 5,  4, 10,  9,  5,  8,  3,  6,  1],\n",
      "        [ 5,  8,  7,  9,  1,  2,  4,  4,  7],\n",
      "        [ 2,  4,  2, 10,  2,  2,  4, 10,  2],\n",
      "        [ 5,  4,  3,  1,  7,  6,  5,  9,  3],\n",
      "        [ 2,  9,  3,  5,  4,  1,  4,  2,  5],\n",
      "        [ 4,  9,  9,  3,  5,  3,  3,  5,  9],\n",
      "        [ 1,  1,  6, 10,  7,  5,  3,  1,  8],\n",
      "        [ 4,  2,  9,  6,  4,  9,  3,  9,  9],\n",
      "        [ 1,  4,  4,  7,  5,  1,  6, 10,  6],\n",
      "        [ 7,  8,  2,  8,  6,  6,  1,  5,  5]])\n",
      "tensor(387289.8438, grad_fn=<KlDivBackward>)\n",
      "tensor([[[ -9.2947,  -1.1481,  -4.4263,  ...,  -3.0983,  -2.0959,  -3.1750],\n",
      "         [ -6.8887,  -2.0874,  -2.7800,  ...,  -3.2797,  -1.8914,  -3.0920],\n",
      "         [ -7.8638,  -1.6863,  -3.5538,  ...,  -3.1509,  -2.1472,  -2.7552],\n",
      "         ...,\n",
      "         [ -7.6620,  -2.1761,  -4.7081,  ...,  -2.8376,  -2.5288,  -2.6773],\n",
      "         [ -7.9013,  -0.8388,  -3.4777,  ...,  -2.4528,  -2.4201,  -2.4982],\n",
      "         [ -8.1077,  -2.1466,  -3.3538,  ...,  -3.4931,  -1.7123,  -3.8799]],\n",
      "\n",
      "        [[-10.0629,  -1.0917,  -3.9341,  ...,  -3.7255,  -2.2544,  -2.7663],\n",
      "         [ -8.2765,  -0.9712,  -3.7491,  ...,  -3.1036,  -2.0842,  -2.5992],\n",
      "         [ -7.9185,  -1.4362,  -3.1473,  ...,  -3.4840,  -1.8632,  -2.1786],\n",
      "         ...,\n",
      "         [ -7.4007,  -1.9986,  -2.7914,  ...,  -2.4692,  -1.6030,  -1.5199],\n",
      "         [ -7.0951,  -1.6236,  -3.3419,  ...,  -3.6130,  -2.0621,  -2.2266],\n",
      "         [ -8.6498,  -1.3713,  -4.2063,  ...,  -4.0097,  -2.1429,  -1.5882]],\n",
      "\n",
      "        [[ -8.5003,  -0.9445,  -3.7638,  ...,  -4.0759,  -2.2177,  -3.9752],\n",
      "         [ -9.4298,  -1.0767,  -3.6340,  ...,  -4.1752,  -1.8634,  -3.6944],\n",
      "         [ -8.0421,  -1.0503,  -2.7342,  ...,  -2.7099,  -2.1458,  -3.1837],\n",
      "         ...,\n",
      "         [ -8.1064,  -0.7381,  -2.7632,  ...,  -2.8163,  -2.1944,  -2.8617],\n",
      "         [ -7.8582,  -0.8937,  -3.1856,  ...,  -2.7335,  -2.5869,  -3.5332],\n",
      "         [ -8.2031,  -1.1905,  -3.8753,  ...,  -2.3410,  -2.9790,  -2.0945]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ -9.4867,  -1.8271,  -3.1240,  ...,  -4.6745,  -1.5683,  -2.7269],\n",
      "         [ -8.9046,  -1.2693,  -3.5701,  ...,  -3.0054,  -2.3855,  -2.1188],\n",
      "         [ -9.3380,  -1.7433,  -2.6790,  ...,  -4.1400,  -2.4944,  -2.9952],\n",
      "         ...,\n",
      "         [-10.1753,  -1.6387,  -3.1322,  ...,  -3.6736,  -2.1117,  -3.2121],\n",
      "         [ -9.2336,  -1.3951,  -2.5577,  ...,  -3.6860,  -2.4120,  -3.8333],\n",
      "         [-10.0621,  -1.6027,  -3.3374,  ...,  -3.8650,  -2.0888,  -2.7913]],\n",
      "\n",
      "        [[ -9.2018,  -2.0982,  -3.4222,  ...,  -5.1193,  -1.5894,  -2.9982],\n",
      "         [ -8.3790,  -1.4592,  -2.8173,  ...,  -3.5895,  -1.9361,  -3.1298],\n",
      "         [ -9.4461,  -1.6754,  -3.4694,  ...,  -5.0528,  -1.8502,  -2.4938],\n",
      "         ...,\n",
      "         [ -7.3436,  -2.7998,  -3.1711,  ...,  -3.8114,  -1.8308,  -1.2406],\n",
      "         [ -7.8009,  -1.6043,  -2.6474,  ...,  -3.9586,  -1.4755,  -1.9560],\n",
      "         [ -8.2746,  -1.9170,  -3.9582,  ...,  -3.7557,  -2.1071,  -1.3655]],\n",
      "\n",
      "        [[ -9.9661,  -1.7399,  -2.3428,  ...,  -3.6886,  -2.0460,  -2.5222],\n",
      "         [ -9.0846,  -1.9161,  -1.4418,  ...,  -4.2936,  -2.0962,  -2.2954],\n",
      "         [ -9.6881,  -1.0946,  -3.1082,  ...,  -3.6204,  -3.0822,  -2.9687],\n",
      "         ...,\n",
      "         [ -8.1733,  -1.9877,  -2.7131,  ...,  -2.8019,  -2.2914,  -1.4478],\n",
      "         [ -9.6695,  -1.7085,  -3.0020,  ...,  -2.9417,  -2.0676,  -2.2119],\n",
      "         [ -8.2856,  -0.9842,  -2.9845,  ...,  -3.4115,  -2.1568,  -1.9221]]],\n",
      "       grad_fn=<LogSoftmaxBackward>) tensor([[10,  3,  5, 10,  7,  2,  9,  8,  9],\n",
      "        [ 9,  4,  7,  8, 10,  7, 10,  2,  4],\n",
      "        [ 1,  9,  2,  4, 10,  9,  3,  2,  8],\n",
      "        [ 8,  5,  1,  2,  9, 10,  8,  6,  5],\n",
      "        [ 5,  2,  2,  1,  1,  7,  2,  7,  1],\n",
      "        [ 4,  1,  7,  4,  9,  1,  3,  7,  5],\n",
      "        [ 6, 10,  1,  1,  7,  2,  9,  1, 10],\n",
      "        [ 8,  6,  1,  6,  8,  9,  4,  4,  6],\n",
      "        [ 6,  4,  5,  9,  4,  2,  1,  1,  1],\n",
      "        [ 9,  2,  5,  8,  7,  6,  8,  1, 10],\n",
      "        [ 4,  4, 10, 10,  3,  3,  9,  6,  7],\n",
      "        [ 3,  7,  8,  4,  2,  8,  6,  9,  6],\n",
      "        [ 6,  3,  3,  9,  8,  2,  2,  2,  6],\n",
      "        [ 1,  6,  7, 10,  8,  2,  8,  1,  2],\n",
      "        [ 9,  7,  3,  8,  4,  8,  5,  6,  9],\n",
      "        [ 6, 10,  6,  2,  4,  5,  1,  7,  3],\n",
      "        [ 6,  3,  6,  6,  1,  3,  9,  3,  4],\n",
      "        [ 2,  2,  7,  5,  9,  5,  2,  5, 10],\n",
      "        [ 3,  9,  4,  4,  3, 10,  1,  4,  9],\n",
      "        [ 4,  5,  3,  4,  9,  4,  2,  7,  6],\n",
      "        [ 3,  9,  2,  6,  5,  5,  5,  5,  5],\n",
      "        [ 1,  5,  7,  7,  9,  1,  3,  7,  9],\n",
      "        [10,  8,  5,  8,  7,  3,  3,  6,  2],\n",
      "        [ 4,  8,  1,  4,  2,  4,  3,  9,  8],\n",
      "        [ 5,  3,  1,  9,  6,  9,  9,  1,  2],\n",
      "        [ 1,  5, 10,  8,  5,  4,  9,  4,  7],\n",
      "        [ 2,  7,  1, 10,  3,  1,  2,  6,  4],\n",
      "        [ 2,  3,  8,  7,  8,  3,  8,  6,  8],\n",
      "        [ 5,  1,  3,  6,  5,  2,  5,  2,  5],\n",
      "        [ 4,  6,  5,  9,  5,  2,  3,  9,  9]])\n",
      "tensor(392879.2812, grad_fn=<KlDivBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ -9.1052,  -2.2428,  -2.5777,  ...,  -3.7386,  -1.8165,  -2.2349],\n",
      "         [ -7.5599,  -2.6670,  -2.4536,  ...,  -1.6416,  -1.4798,  -2.7118],\n",
      "         [ -7.4797,  -2.5364,  -1.3139,  ...,  -2.0850,  -2.6401,  -2.7074],\n",
      "         ...,\n",
      "         [ -6.8819,  -1.9469,  -2.2590,  ...,  -2.3285,  -2.3416,  -2.7335],\n",
      "         [ -7.0576,  -3.2943,  -2.3534,  ...,  -2.3509,  -1.7811,  -1.8144],\n",
      "         [ -7.7797,  -2.0951,  -3.4601,  ...,  -3.1771,  -1.8270,  -2.2424]],\n",
      "\n",
      "        [[ -9.9063,  -2.4166,  -3.5781,  ...,  -2.3321,  -1.2041,  -3.4466],\n",
      "         [ -7.5711,  -2.1749,  -3.0413,  ...,  -2.6019,  -1.3060,  -2.7487],\n",
      "         [ -7.2986,  -3.2925,  -2.6669,  ...,  -1.8804,  -1.2586,  -2.4692],\n",
      "         ...,\n",
      "         [ -7.9600,  -1.7045,  -3.7339,  ...,  -1.5264,  -2.2428,  -2.4012],\n",
      "         [ -7.9277,  -2.0072,  -3.3829,  ...,  -1.6722,  -1.5308,  -2.7701],\n",
      "         [ -7.5210,  -3.5596,  -3.2201,  ...,  -2.0914,  -1.2236,  -2.0737]],\n",
      "\n",
      "        [[ -7.9487,  -0.7318,  -2.8386,  ...,  -3.6911,  -2.0667,  -3.8093],\n",
      "         [ -8.0341,  -1.3981,  -3.5836,  ...,  -3.1698,  -1.5699,  -3.1255],\n",
      "         [ -7.5752,  -0.7450,  -3.2590,  ...,  -3.2706,  -2.0587,  -2.7793],\n",
      "         ...,\n",
      "         [ -7.5036,  -1.3606,  -3.0757,  ...,  -4.3919,  -1.6303,  -3.2459],\n",
      "         [ -7.9366,  -0.9815,  -2.7317,  ...,  -3.5278,  -2.2060,  -2.4758],\n",
      "         [ -7.4377,  -1.3496,  -2.4984,  ...,  -3.1601,  -2.0414,  -2.9484]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-10.3997,  -2.5587,  -2.3074,  ...,  -3.9526,  -2.7907,  -3.2766],\n",
      "         [ -8.2721,  -2.1176,  -2.4437,  ...,  -3.1564,  -2.4359,  -2.6255],\n",
      "         [ -8.6279,  -2.3134,  -2.0430,  ...,  -4.4667,  -2.8140,  -2.7298],\n",
      "         ...,\n",
      "         [ -8.3829,  -2.1493,  -2.5165,  ...,  -3.3254,  -3.0680,  -2.2038],\n",
      "         [ -9.4149,  -2.7542,  -2.3190,  ...,  -3.2624,  -3.1916,  -2.2802],\n",
      "         [ -8.6912,  -2.3692,  -2.5304,  ...,  -3.6923,  -2.7428,  -2.5462]],\n",
      "\n",
      "        [[ -8.9531,  -2.9417,  -2.8217,  ...,  -3.2404,  -2.1723,  -2.5384],\n",
      "         [ -7.9813,  -2.9950,  -3.2486,  ...,  -3.7946,  -0.8765,  -2.9897],\n",
      "         [ -7.7228,  -2.9503,  -3.0266,  ...,  -3.5506,  -1.8187,  -2.6039],\n",
      "         ...,\n",
      "         [ -8.0247,  -3.0678,  -2.9318,  ...,  -3.8265,  -1.5976,  -3.1076],\n",
      "         [ -7.6036,  -2.1049,  -2.5619,  ...,  -2.8796,  -2.2419,  -2.6038],\n",
      "         [ -9.6777,  -2.9577,  -2.8853,  ...,  -4.1276,  -1.6544,  -2.2797]],\n",
      "\n",
      "        [[ -9.1223,  -2.0130,  -2.0563,  ...,  -2.8803,  -2.0683,  -3.7042],\n",
      "         [ -8.1499,  -1.2753,  -2.8184,  ...,  -3.0191,  -2.1086,  -3.4806],\n",
      "         [ -7.5965,  -1.6635,  -2.7138,  ...,  -2.2868,  -2.0817,  -2.7756],\n",
      "         ...,\n",
      "         [ -8.7605,  -1.9226,  -2.0737,  ...,  -2.9042,  -2.3539,  -2.8935],\n",
      "         [ -7.7915,  -2.2351,  -2.2101,  ...,  -3.0551,  -2.2373,  -3.1068],\n",
      "         [ -8.2975,  -1.5877,  -2.1475,  ...,  -2.7506,  -1.7223,  -3.4105]]],\n",
      "       grad_fn=<LogSoftmaxBackward>) tensor([[10, 10,  4,  7,  5,  8,  4,  2, 10],\n",
      "        [ 2,  4,  9,  2,  4,  3,  3,  7, 10],\n",
      "        [ 6, 10, 10, 10,  1,  9, 10,  6,  1],\n",
      "        [ 4,  4,  1,  6,  9,  8, 10,  7,  6],\n",
      "        [ 4, 10,  3,  3,  3,  3,  6,  6,  2],\n",
      "        [ 1,  9,  8,  7,  2,  3,  7,  7,  2],\n",
      "        [10,  7,  8,  7,  3,  9, 10, 10,  2],\n",
      "        [ 1,  8,  7,  8,  2, 10,  6,  7,  3],\n",
      "        [ 7,  5, 10,  1,  2,  3,  1,  1,  1],\n",
      "        [10,  4,  6,  9,  3,  5,  9,  7,  2],\n",
      "        [ 1,  4, 10,  9,  2,  3,  7,  6,  3],\n",
      "        [ 1, 10,  3,  9,  3,  5,  5,  4,  4],\n",
      "        [ 1, 10,  8,  8,  7,  4,  3,  3, 10],\n",
      "        [10,  1,  6,  7,  1,  4,  5,  7,  1],\n",
      "        [10,  9,  5,  4, 10,  2,  6,  5,  6],\n",
      "        [ 6,  8,  9,  8,  9,  6,  5,  2,  7],\n",
      "        [ 7,  6, 10,  9,  1,  9,  4,  7,  1],\n",
      "        [10,  8,  3,  5,  6,  3,  8,  1,  2],\n",
      "        [ 9,  6,  5, 10,  3,  1,  1,  1,  9],\n",
      "        [ 2,  2,  1,  2, 10,  9,  1,  3,  4],\n",
      "        [ 3,  4,  5,  4,  1,  5,  9,  2,  3],\n",
      "        [ 5,  4, 10,  3,  3,  2,  8,  3,  6],\n",
      "        [ 6,  6,  3,  4, 10,  5,  3,  4,  3],\n",
      "        [ 9,  3,  9,  5,  7,  2,  7,  3,  6],\n",
      "        [ 2,  9,  4, 10,  1,  1,  1,  3,  6],\n",
      "        [ 2,  3,  8,  3,  1,  3,  8,  8,  3],\n",
      "        [ 1,  6,  2,  4,  4,  5,  3,  6,  4],\n",
      "        [ 6,  9,  4,  7,  9,  7,  6,  7,  2],\n",
      "        [ 2,  2,  4,  2,  2,  4, 10,  1,  6],\n",
      "        [ 2,  8,  1,  9,  9,  1,  9,  2,  7]])\n",
      "tensor(395997.0625, grad_fn=<KlDivBackward>)\n",
      "tensor([[[-8.8628, -2.8878, -5.4520,  ..., -3.5803, -3.2912, -1.6505],\n",
      "         [-6.8061, -1.3833, -3.9825,  ..., -2.6426, -3.8679, -2.6221],\n",
      "         [-6.4264, -3.1007, -4.5552,  ..., -1.5381, -4.0178, -1.6904],\n",
      "         ...,\n",
      "         [-7.7552, -2.1522, -4.8543,  ..., -1.6332, -3.2998, -2.0556],\n",
      "         [-8.1913, -3.6895, -4.6326,  ..., -2.1908, -4.3389, -2.7408],\n",
      "         [-6.3211, -2.7713, -3.9720,  ..., -2.3794, -3.6715, -1.0824]],\n",
      "\n",
      "        [[-7.9224, -3.0065, -4.5669,  ..., -2.5027, -3.2123, -2.4746],\n",
      "         [-7.0053, -2.4951, -4.1576,  ..., -0.8662, -2.7231, -2.0989],\n",
      "         [-7.4290, -2.6135, -3.3134,  ..., -1.2678, -3.4574, -1.4281],\n",
      "         ...,\n",
      "         [-6.2829, -2.5165, -3.6498,  ..., -1.9824, -3.5297, -2.5940],\n",
      "         [-7.6224, -2.2490, -3.2030,  ..., -2.1571, -2.7266, -2.2976],\n",
      "         [-8.4632, -2.8093, -5.2382,  ..., -3.5950, -3.5338, -2.6680]],\n",
      "\n",
      "        [[-7.8931, -2.3738, -2.9182,  ..., -3.5272, -2.6276, -3.5777],\n",
      "         [-7.8366, -2.1714, -3.0342,  ..., -3.0019, -4.0049, -2.4763],\n",
      "         [-7.2975, -2.0124, -2.9585,  ..., -3.1460, -3.8802, -2.6995],\n",
      "         ...,\n",
      "         [-7.5098, -2.9697, -3.2036,  ..., -3.5962, -3.9211, -1.7374],\n",
      "         [-6.6636, -2.5813, -2.7160,  ..., -2.6377, -2.8901, -3.2644],\n",
      "         [-6.1110, -1.5996, -3.2485,  ..., -3.2775, -4.0260, -3.4453]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-6.9607, -2.2330, -2.8364,  ..., -3.9657, -2.4262, -2.1701],\n",
      "         [-6.9308, -1.7951, -2.1213,  ..., -3.1933, -3.1994, -3.6631],\n",
      "         [-7.1965, -2.4784, -2.0848,  ..., -2.8600, -2.5195, -2.5710],\n",
      "         ...,\n",
      "         [-8.6851, -2.3988, -2.3410,  ..., -3.3054, -3.0598, -1.9991],\n",
      "         [-6.8432, -1.4954, -2.1918,  ..., -3.4760, -3.2538, -2.7520],\n",
      "         [-8.5200, -2.6622, -2.4649,  ..., -2.8514, -2.8558, -2.7033]],\n",
      "\n",
      "        [[-7.8789, -2.6380, -2.7932,  ..., -3.5051, -2.2611, -2.1751],\n",
      "         [-8.5658, -2.3373, -2.3595,  ..., -3.5938, -2.8687, -1.3272],\n",
      "         [-7.0184, -2.2904, -2.1350,  ..., -2.0788, -3.0138, -1.8416],\n",
      "         ...,\n",
      "         [-6.6547, -2.3658, -1.8302,  ..., -2.5045, -2.1867, -2.2701],\n",
      "         [-7.0850, -3.0925, -2.2708,  ..., -3.0504, -3.2214, -2.4269],\n",
      "         [-8.4711, -1.6259, -2.3745,  ..., -2.7097, -2.7416, -2.0904]],\n",
      "\n",
      "        [[-7.9498, -3.1216, -4.1564,  ..., -2.5138, -2.7913, -2.4104],\n",
      "         [-7.7646, -2.2630, -3.9105,  ..., -1.8298, -3.0368, -1.9514],\n",
      "         [-6.7239, -3.1575, -3.1995,  ..., -2.2881, -2.9198, -2.2018],\n",
      "         ...,\n",
      "         [-6.2783, -2.9215, -3.2530,  ..., -1.4186, -3.0616, -2.3342],\n",
      "         [-7.3116, -2.3523, -3.2178,  ..., -2.7255, -2.5228, -2.1828],\n",
      "         [-6.4870, -2.9154, -3.8284,  ..., -1.9979, -2.5904, -1.9264]]],\n",
      "       grad_fn=<LogSoftmaxBackward>) tensor([[ 2, 10,  1,  7,  2,  3,  6, 10,  8],\n",
      "        [10, 10,  6,  7,  7,  9,  4,  1,  3],\n",
      "        [ 7,  5,  6,  9,  4,  5,  6,  2,  2],\n",
      "        [ 2,  2,  1,  2,  9,  8,  1,  4,  4],\n",
      "        [ 5,  4,  2,  7,  8,  7,  9,  3,  7],\n",
      "        [ 1,  6,  2,  5, 10,  1,  9,  5, 10],\n",
      "        [ 6, 10,  4,  6,  5, 10,  4,  2,  4],\n",
      "        [ 2,  4,  9,  3,  6,  5,  1,  5,  8],\n",
      "        [ 8,  4,  2,  1,  6,  7,  4,  3,  8],\n",
      "        [10,  9,  4,  2,  7,  9,  9,  6, 10],\n",
      "        [ 5,  1,  3,  7,  5,  6,  2,  9,  1],\n",
      "        [ 8,  2, 10,  7, 10, 10,  8,  7,  4],\n",
      "        [ 6,  9, 10,  7,  5,  5,  3,  2,  3],\n",
      "        [ 9,  3,  7,  7,  6,  8,  4,  2,  6],\n",
      "        [ 6,  4,  6,  3,  2,  3, 10,  3,  2],\n",
      "        [ 9,  7,  3,  8,  5, 10,  6,  9,  4],\n",
      "        [ 6,  2,  1,  1,  8, 10,  3,  7,  7],\n",
      "        [ 4,  7,  3,  2,  7,  9,  1,  8, 10],\n",
      "        [ 7,  9,  1,  8,  3,  8,  4,  1,  4],\n",
      "        [ 8,  7,  5,  9,  1,  5,  7,  5,  4],\n",
      "        [ 2,  4,  2,  1,  6,  1,  9, 10,  9],\n",
      "        [ 9,  3,  6,  1,  6,  4,  1,  4,  1],\n",
      "        [ 5,  7,  1,  7,  3,  1,  1,  8,  3],\n",
      "        [ 5,  4,  1,  5,  7,  5,  9,  7,  3],\n",
      "        [ 6,  2,  2,  7,  1,  9,  7,  6, 10],\n",
      "        [ 3,  4,  1,  8,  6, 10, 10,  1,  7],\n",
      "        [ 3, 10,  6,  3,  4, 10,  6,  2,  9],\n",
      "        [ 2,  1,  5,  9,  9,  5,  2,  6,  3],\n",
      "        [ 5,  8,  2,  5,  9,  3,  9,  3,  1],\n",
      "        [ 3, 10,  6,  6,  8,  8,  7, 10,  7]])\n",
      "tensor(393942.6562, grad_fn=<KlDivBackward>)\n"
     ]
    }
   ],
   "source": [
    "run_epoch(data_generation(vocab_size, 30, 20), model, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mah = MultiHeadAttention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.rand(5, 10, d_model)\n",
    "k = torch.rand(5, 10, d_model)\n",
    "v = torch.rand(5, 10, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mah(q,k,v,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
