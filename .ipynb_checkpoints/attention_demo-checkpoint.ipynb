{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arch units\n",
    "## Self Attention Unit\n",
    "## Multi Head Attention\n",
    "## Encode Decode Unit\n",
    "## Norm + Residual Layer\n",
    "## Feed Forward\n",
    "## Input Positional Encoding \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters from paper\n",
    "N = 2 # 6\n",
    "d_model = 512\n",
    "h = 8\n",
    "d_k = d_v = d_model//h\n",
    "d_ff = 2048 #128\n",
    "vocab_size = 11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        self.W_O = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask):\n",
    "        b, q_seq, _ = Q.size()\n",
    "        b, k_seq, _ = K.size()\n",
    "        query = self.W_Q(Q).view(b, q_seq, h, d_k) # view (b, q_seq, h, d)\n",
    "        key = self.W_K(K).view(b, k_seq, h, d_k) # view (b, k_seq, h, d)\n",
    "        value  = self.W_V(V).view(b, k_seq, h, d_k) # view (b, k_seq, h, d)\n",
    "        \n",
    "        query = query.transpose(1, 2).contiguous() # view (b, h, q_seq, d)\n",
    "        key = key.transpose(1, 2).contiguous() # view (b, h, k_seq, d)\n",
    "        value = value.transpose(1, 2).contiguous() # view (b, h, k_seq, d)\n",
    "        \n",
    "        qk = query.matmul(key.transpose(-2,-1))\n",
    "        scale_qk = qk/(math.sqrt(d_k)) # shape (b, h, q_seq, k_seq)\n",
    "        \n",
    "        if mask is not None: # mask size (b, 1, k_seq)\n",
    "            mask = mask.unsqueeze(1) # mask size (b, 1, 1, k_seq)\n",
    "            scale_qk = scale_qk.masked_fill(mask==0, 1e-9)\n",
    "        \n",
    "        softmax_qk = nn.functional.softmax(scale_qk, dim=-1) # (b, h, q_seq, k_seq)\n",
    "        weighted_value = softmax_qk.matmul(value) # (b, h, q_seq, d)\n",
    "        return self.W_O(weighted_value.transpose(2,1).contiguous().view(b, q_seq, h*d_k)) # (b, h, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_mod=d_model):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.d_mod = d_mod\n",
    "        # https://stackoverflow.com/questions/39095252/fail-to-implement-layer-normalization-with-keras\n",
    "        # https://stackoverflow.com/questions/50935345/understanding-torch-nn-parameter\n",
    "        self.alpha = nn.Parameter(torch.ones(d_mod))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_mod))\n",
    "    def forward(self, x, eps=1e-6):\n",
    "        u = x.mean(-1, keepdim=True)\n",
    "        sigma = x.std(-1, keepdim=True)\n",
    "        return self.alpha * (x - u)/(1/(sigma + eps)) + self.beta\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCell(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderCell, self).__init__()\n",
    "        self.attn = MultiHeadAttention()\n",
    "        self.norm_1 = LayerNorm()\n",
    "        self.pff = nn.Sequential(\n",
    "                    nn.Linear(d_model, d_ff),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(d_ff, d_model))\n",
    "        self.norm_2 = LayerNorm()\n",
    "    \n",
    "    def forward(self, x, src_mask=None):\n",
    "        print(\"x \", x)\n",
    "        x_norm_1 = self.norm_1(x + self.attn(x, x, x, src_mask))  # Layer 1\n",
    "        print(\"norm_1\", x_norm_1)\n",
    "#         return self.norm_2(x_norm_1 + self.pff(x_norm_1)) # Layer 2\n",
    "        alpha = self.norm_2(x_norm_1 + self.pff(x_norm_1))\n",
    "        print(\"alpha\", alpha)\n",
    "        return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderCell(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecoderCell, self).__init__()\n",
    "        self.attn = MultiHeadAttention()\n",
    "        self.norm_1 = LayerNorm()\n",
    "        self.attn = MultiHeadAttention()\n",
    "        self.norm_2 = LayerNorm()\n",
    "        self.pff = nn.Sequential(\n",
    "                    nn.Linear(d_model, d_ff),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(d_ff, d_model))\n",
    "        self.norm_3 = LayerNorm()\n",
    "        \n",
    "    def forward(self, x, enc, src_mask=None, trg_mask=None):\n",
    "        x_norm_1 = self.norm_1(x + self.attn(x, x, x, trg_mask))\n",
    "        x_norm_2 = self.norm_2(x_norm_1 + self.attn(x_norm_1, enc, enc, src_mask))\n",
    "        return self.norm_3(x_norm_2 + self.pff(x_norm_2)) # (b, seq, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderStack(nn.Module):\n",
    "    def __init__(self, N):\n",
    "        super(EncoderStack, self).__init__()\n",
    "        self.N = N\n",
    "        \n",
    "    def forward(self, x, src_mask):\n",
    "        cell = EncoderCell()\n",
    "        encoders = nn.ModuleList([copy.deepcopy(cell) for _ in range(self.N)])\n",
    "        \n",
    "        for enc in encoders:\n",
    "            x = enc(x, src_mask)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderStack(nn.Module):\n",
    "    def __init__(self, N):\n",
    "        super(DecoderStack, self).__init__()\n",
    "        self.N = N\n",
    "    \n",
    "    def forward(self, x, enc, src_mask, trg_mask):\n",
    "        cell = DecoderCell()\n",
    "        decoders = nn.ModuleList([copy.deepcopy(cell) for _ in range(self.N)])\n",
    "        for decdr in decoders:\n",
    "            x = decdr(x, enc, src_mask, trg_mask)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dpout=0.1, max_seq=50):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dpout)\n",
    "        \n",
    "        pe_matx = torch.zeros(max_seq, d_model)\n",
    "        position = torch.arange(0, max_seq, dtype=torch.float).unsqueeze(-1)\n",
    "        w_t = torch.exp(torch.arange(0, d_model, 2).float() * -math.log(10000)/d_model)\n",
    "        val = position * w_t\n",
    "        pe_matx[:, 0::2] = torch.sin(val)\n",
    "        pe_matx[:, 1::2] = torch.cos(val)\n",
    "        pe_matx = pe_matx.unsqueeze(1)\n",
    "        self.register_buffer(\"pe_matx\", pe_matx)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x - (batch, seq, emb), pe_matrix - (max_seq, 1, d_model)\n",
    "        x += self.pe_matx[:x.size(0), :]\n",
    "        return(self.dropout(x))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, embedd = True, log_softmx=True):\n",
    "        super(Transformer, self).__init__()\n",
    "#         self.W_in = nn.Linear(word_emb_dim, d_model)\n",
    "        self.encoder = EncoderStack(N)\n",
    "        self.decoder = DecoderStack(N)\n",
    "        # https://stats.stackexchange.com/questions/392213/understand-the-output-layer-of-transformer\n",
    "        self.W_out = nn.Linear(d_model, vocab_size)\n",
    "        self.sftmx = log_softmx\n",
    "        self.embedd = embedd\n",
    "        if self.embedd:\n",
    "            embed_x = EmbeddingLayer(vocab_size, d_model)\n",
    "            embed_y = EmbeddingLayer(vocab_size, d_model)\n",
    "            pe_x = PositionalEncoding(d_model)\n",
    "            pe_y = copy.deepcopy(pe_x)\n",
    "            self.enc_x = nn.Sequential(embed_x, pe_x)\n",
    "            self.enc_y = nn.Sequential(embed_y, pe_y)\n",
    "        \n",
    "    def forward(self, inp_x, inp_y, src_mask, trg_mask):\n",
    "        if self.embedd:\n",
    "            inp_x, inp_y = self.enc_x(inp_x), self.enc_y(inp_y)\n",
    "        enc_x = self.encoder(inp_x, src_mask)\n",
    "        dec_x = self.decoder(inp_y, enc_x, src_mask, trg_mask)\n",
    "        if self.sftmx:\n",
    "            return nn.functional.log_softmax(self.W_out(dec_x), dim=-1)\n",
    "        return self.W_out(dec_x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.reddit.com/r/MachineLearning/comments/bjgpt2/d_confused_about_using_masking_in_transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
    "class Batch:\n",
    "    def __init__(self, src, trg=None, pad=0): # size src, trg (b, seq)\n",
    "        self.src = src \n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        if trg is not None:\n",
    "            self.trg = trg[:,:-1] # size (b,0:seq-1)\n",
    "            self.trg_y = trg[:,1:] # size (b,1:seq)\n",
    "            self.trg_mask = self.std_mask(self.trg, pad)\n",
    "            self.ntokens = (self.trg_y != pad).data.sum() # size (1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def std_mask(tgt, pad):\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2) # size (b, 1, seq)\n",
    "        mask = torch.from_numpy(np.triu(np.ones((1,tgt.shape[-1],tgt.shape[-1])), k=1).astype('uint8')) == 0\n",
    "        return tgt_mask & mask # size (b, 1, seq) * (1, seq, seq) -> (b, seq, seq) \n",
    "    \n",
    "#     @staticmethod\n",
    "#     def subsequent_mask(size):\n",
    "#         return torch.from_numpy(np.triu(np.ones((1,size,size)), k=1).astype('uint8')) == 0 # size (1, seq, seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/pytorch/issues/7455    \n",
    "def labelSmoothingLoss(x, y, epsilon, padding_value=0, cls=1, d=-1):\n",
    "    # concat x, y batch as index_fill_ don't support vector dim > 1\n",
    "#     x = x.view(-1, x.size(-1))    \n",
    "    x=x.contiguous().view(-1, x.size(-1))\n",
    "    y=y.contiguous().view(-1)\n",
    "    \n",
    "    x_ = x.data.clone()\n",
    "    x_.fill_(epsilon / (x_.size(-1) - cls))\n",
    "    x_.scatter_(d, y.data.unsqueeze(-1), (1 - epsilon))\n",
    "    x_[:, padding_value] = 0\n",
    "    mask = torch.nonzero(y.data == padding_value)\n",
    "    if mask.dim() > 0:\n",
    "        x_.index_fill_(0, mask.squeeze(), 0.0)\n",
    "    return torch.mean(torch.sum(-x_*x), dim=d) # x_ is true distribution and x is prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init all parameters as we used deepcopy to save computation tym\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://nlp.seas.harvard.edu/2018/04/03/attention.html#synthetic-data\n",
    "def data_generation(V, batch, nbatches):\n",
    "    for i in range(nbatches):\n",
    "        data = torch.randint(1, V, size=(batch, 10))\n",
    "        data[:, 0] = 1\n",
    "        src = data.clone().detach()\n",
    "        trg = data.clone().detach()\n",
    "        yield Batch(src, trg, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://nlp.seas.harvard.edu/2018/04/03/attention.html#synthetic-data\n",
    "def run_epoch(data_itr, model, optimizer):\n",
    "    start = time.time()\n",
    "    total_token = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    \n",
    "    for i, batch in enumerate(data_itr):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outp = model(batch.src, batch.trg, batch.src_mask, batch.trg_mask)\n",
    "        loss = labelSmoothingLoss(outp, batch.trg_y, batch.ntokens)\n",
    "        \n",
    "        break\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss+=loss\n",
    "        total_token+=batch.ntokens\n",
    "        tokens+=batch.ntokens\n",
    "        \n",
    "        if i%30 == 1:\n",
    "            elapsed = time.time() - start\n",
    "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
    "                    (i, loss / batch.ntokens, tokens / elapsed))\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "#     return total_loss/total_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x  tensor([[[-0.0000,  1.2420,  2.6834,  ...,  2.0979, -2.0528, -1.1805],\n",
      "         [ 2.1321,  0.8550,  1.8131,  ...,  1.8008,  1.2272, -0.5109],\n",
      "         [ 2.6156,  3.6029, -1.1444,  ..., -0.8462,  0.3334,  3.3015],\n",
      "         ...,\n",
      "         [ 1.7315, -0.0000,  0.0000,  ..., -0.8977, -1.1905,  2.2879],\n",
      "         [-1.0748, -0.3836,  2.5188,  ...,  2.3092,  0.4682, -1.4502],\n",
      "         [ 0.0000, -0.1461,  1.0798,  ..., -0.8977, -1.1905,  2.2879]],\n",
      "\n",
      "        [[ 0.1791,  0.7312,  3.5966,  ...,  2.0979, -2.0527, -0.0000],\n",
      "         [-0.1398, -0.8943,  0.0000,  ...,  2.3092,  0.0000, -0.0000],\n",
      "         [-0.1398, -0.8943,  3.4320,  ...,  2.3092,  0.4683, -1.4502],\n",
      "         ...,\n",
      "         [ 3.0671,  0.3442,  2.7263,  ...,  1.8008,  1.2273, -0.5109],\n",
      "         [ 3.4505, -1.7813,  1.1816,  ..., -1.0663,  1.4006,  2.9691],\n",
      "         [ 3.0671,  0.3442,  2.7263,  ...,  0.0000,  1.2273, -0.5109]],\n",
      "\n",
      "        [[ 0.2544, -0.0000,  3.7239,  ...,  2.0979, -2.0525, -1.1805],\n",
      "         [ 3.5259, -0.0000,  1.3089,  ..., -1.0663,  1.4007,  0.0000],\n",
      "         [ 3.1425, -0.7185,  2.8536,  ...,  1.8008,  1.2274, -0.5109],\n",
      "         ...,\n",
      "         [ 3.1425, -0.0000,  2.8536,  ...,  1.8008,  1.2274, -0.5109],\n",
      "         [-0.7536,  0.8837, -0.5955,  ...,  3.7340, -1.3731,  2.8652],\n",
      "         [-0.4408, -0.4438, -0.5267,  ..., -1.0451,  0.0701,  0.1274]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.3067, -0.1937,  0.0000,  ...,  2.0979, -2.0497, -0.0000],\n",
      "         [ 2.7942, -1.5818,  1.9591,  ..., -0.8977, -1.1874,  0.0000],\n",
      "         [ 2.7942, -1.5818,  1.9591,  ..., -0.8977, -1.1874,  2.2879],\n",
      "         ...,\n",
      "         [ 0.9266, -1.9138,  2.8581,  ...,  3.1175, -1.5651,  0.6851],\n",
      "         [ 0.9266, -1.9138,  2.8581,  ...,  0.0000, -1.5651,  0.6851],\n",
      "         [-0.7012,  0.0000, -0.7567,  ...,  3.7339, -1.3702,  2.8652]],\n",
      "\n",
      "        [[-0.4549, -0.9387,  3.7426,  ...,  2.0979, -2.0496, -0.0000],\n",
      "         [ 2.9167,  0.0000, -0.0852,  ..., -0.0000,  0.0000,  3.3015],\n",
      "         [ 0.1650, -2.6587,  3.0379,  ...,  3.1175, -1.5650,  0.6851],\n",
      "         ...,\n",
      "         [ 2.9167,  1.4223, -0.0852,  ..., -0.8462,  0.3367,  3.3015],\n",
      "         [ 2.4331, -1.3257,  2.8723,  ...,  1.8008,  0.0000, -0.5109],\n",
      "         [ 2.4331, -1.3257,  2.8723,  ...,  1.8008,  1.2304, -0.5109]],\n",
      "\n",
      "        [[-1.4933, -0.7003,  3.0109,  ...,  2.0979, -2.0494, -1.1805],\n",
      "         [ 0.9941, -2.0884,  1.4072,  ..., -0.0000, -1.1871,  0.0000],\n",
      "         [-1.8122, -2.3258,  2.8463,  ...,  2.3092,  0.4715, -1.4502],\n",
      "         ...,\n",
      "         [ 1.3948, -1.0873,  2.1406,  ...,  1.8008,  1.2305, -0.5109],\n",
      "         [-1.4933, -0.7003,  0.0000,  ...,  2.0979, -2.0494, -1.1805],\n",
      "         [-0.0000, -0.8126, -1.2397,  ..., -1.0451,  0.0732,  0.1274]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "norm_1 tensor([[[-1.0536,  1.3318,  3.9408,  ...,  3.4980, -3.2862, -3.4761],\n",
      "         [ 2.4791,  0.5345,  1.9859,  ...,  2.7741,  2.0698, -2.5666],\n",
      "         [ 3.3906,  5.0554, -2.5151,  ..., -1.4135,  0.6106,  3.8331],\n",
      "         ...,\n",
      "         [ 1.7389, -0.6401, -0.5542,  ..., -1.1681, -1.7593,  2.2013],\n",
      "         [-2.6999, -1.0234,  3.4129,  ...,  4.1723,  0.9141, -3.7565],\n",
      "         [-0.8094, -0.8575,  1.1713,  ..., -1.1812, -1.6701,  2.0993]],\n",
      "\n",
      "        [[-1.1501,  0.1113,  4.3788,  ...,  3.0940, -3.4849, -2.1219],\n",
      "         [-1.6620, -2.1270, -0.6144,  ...,  3.7330, -0.3742, -2.0150],\n",
      "         [-1.6323, -2.0436,  5.0918,  ...,  3.9012,  0.4051, -4.1698],\n",
      "         ...,\n",
      "         [ 3.1909, -0.4714,  3.3320,  ...,  2.8932,  1.9589, -2.7850],\n",
      "         [ 3.9610, -4.1723,  0.8388,  ..., -1.6457,  1.9682,  2.8822],\n",
      "         [ 3.0972, -0.3979,  3.4636,  ...,  0.0515,  2.0491, -2.6917]],\n",
      "\n",
      "        [[-0.5042, -0.8015,  5.1559,  ...,  2.7529, -3.3570, -3.5288],\n",
      "         [ 4.6291, -0.8318,  1.1504,  ..., -2.6015,  1.9264, -1.7502],\n",
      "         [ 3.9214, -1.7513,  3.2742,  ...,  2.4542,  2.1480, -2.4852],\n",
      "         ...,\n",
      "         [ 3.8584, -0.6066,  3.2223,  ...,  2.3915,  2.0011, -2.4983],\n",
      "         [-2.4788,  0.7428, -2.1893,  ...,  5.5056, -2.6443,  3.1389],\n",
      "         [-2.0826, -1.5694, -2.1164,  ..., -2.2425, -0.4230, -1.3970]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0098, -1.0520, -0.5077,  ...,  3.1904, -3.6642, -0.7763],\n",
      "         [ 3.5922, -2.5565,  2.3078,  ..., -1.6119, -1.6327, -0.8368],\n",
      "         [ 3.6716, -2.5047,  2.3948,  ..., -1.5235, -1.7118,  2.6633],\n",
      "         ...,\n",
      "         [ 0.6370, -4.1295,  4.2947,  ...,  5.2060, -2.3516,  0.2992],\n",
      "         [ 0.7289, -4.0112,  4.1036,  ..., -0.0433, -2.5341,  0.1826],\n",
      "         [-2.3023, -0.2806, -2.1172,  ...,  5.2004, -2.1971,  3.7200]],\n",
      "\n",
      "        [[-1.7643, -1.6092,  5.5466,  ...,  2.5287, -3.2198, -0.4809],\n",
      "         [ 3.3999, -0.7073, -0.8304,  ..., -0.8838, -0.3351,  4.7543],\n",
      "         [-1.4360, -4.9628,  4.2856,  ...,  4.5458, -2.5237,  0.8607],\n",
      "         ...,\n",
      "         [ 3.5239,  1.6097, -0.8932,  ..., -2.3672,  0.3361,  4.8350],\n",
      "         [ 2.8552, -2.5124,  3.7965,  ...,  2.2432,  0.1062, -1.4056],\n",
      "         [ 2.8725, -2.5742,  3.6887,  ...,  2.0115,  2.3411, -1.5440]],\n",
      "\n",
      "        [[-2.8066, -1.1514,  4.0729,  ...,  3.1996, -3.7299, -3.5438],\n",
      "         [ 1.0974, -2.9892,  1.3527,  ..., -0.6041, -1.9557, -1.3087],\n",
      "         [-3.2056, -3.6045,  3.9850,  ...,  3.7434,  0.3449, -3.7292],\n",
      "         ...,\n",
      "         [ 1.8830, -1.8450,  2.6470,  ...,  2.6623,  2.0086, -2.4957],\n",
      "         [-2.9344, -1.1925, -0.8548,  ...,  3.1228, -3.6401, -3.5406],\n",
      "         [-0.6255, -1.7365, -3.5022,  ..., -2.1573, -0.3984, -1.1005]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "alpha tensor([[[ -2.8496,   3.0096,  10.6878,  ...,  10.9191, -11.6138, -10.0887],\n",
      "         [  6.2494,  -0.5510,   6.3921,  ...,   7.1750,   3.8960,  -5.6493],\n",
      "         [  8.2579,  12.2758,  -6.8234,  ...,  -4.4723,  -1.9153,  12.2202],\n",
      "         ...,\n",
      "         [  3.9618,  -2.3893,  -0.2389,  ...,  -2.0541,  -5.6996,   5.3782],\n",
      "         [ -8.6903,  -4.3222,   9.1932,  ...,  13.7640,   1.0869, -11.3624],\n",
      "         [ -2.4356,  -3.2588,   4.3218,  ...,  -1.5337,  -5.2310,   5.3986]],\n",
      "\n",
      "        [[ -2.3965,  -1.0162,  12.4134,  ...,   8.8612, -12.6999,  -5.8745],\n",
      "         [ -5.3848,  -7.8714,  -0.3702,  ...,  12.3492,  -2.7565,  -5.2586],\n",
      "         [ -5.7134,  -7.9167,  15.0927,  ...,  13.7574,  -1.2358, -12.4186],\n",
      "         ...,\n",
      "         [  8.3978,  -2.5567,   9.6394,  ...,   5.7296,   4.1594,  -5.6638],\n",
      "         [ 11.9443, -12.5816,   5.9880,  ...,  -3.3524,   3.5941,   6.7906],\n",
      "         [  7.9073,  -3.0411,  10.1242,  ...,  -0.5357,   4.4053,  -5.1044]],\n",
      "\n",
      "        [[ -1.2864,  -0.9366,  14.2820,  ...,   7.1492, -11.3072, -10.7532],\n",
      "         [ 14.0634,  -3.0156,   7.4772,  ...,  -6.0822,   3.6064,  -6.9207],\n",
      "         [  9.2060,  -6.3258,  10.2353,  ...,   5.5653,   4.1236,  -3.6635],\n",
      "         ...,\n",
      "         [ 10.0697,  -3.4087,   8.8716,  ...,   5.8228,   4.0696,  -4.8604],\n",
      "         [ -5.4538,  -0.6775,  -2.6627,  ...,  16.2687,  -8.6162,   9.9279],\n",
      "         [ -5.0626,  -4.8209,  -4.1887,  ...,  -5.7703,  -0.9150,  -3.8249]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[  0.3978,  -2.6968,  -2.0697,  ...,   8.9199, -13.9523,  -3.0972],\n",
      "         [  8.3692,  -5.8176,   6.6700,  ...,  -3.1491,  -3.6220,  -2.2655],\n",
      "         [  8.9557,  -6.4729,   7.4040,  ...,  -3.3512,  -4.8728,   6.2988],\n",
      "         ...,\n",
      "         [  1.4020, -12.7963,  15.8646,  ...,  14.7244,  -6.3026,   2.1073],\n",
      "         [  2.3302, -13.5774,  14.4448,  ...,   0.6086,  -6.5242,   0.9270],\n",
      "         [ -5.4022,  -1.1351,  -3.2743,  ...,  14.3808,  -6.8210,  10.9049]],\n",
      "\n",
      "        [[ -4.1556,  -3.6154,  14.8685,  ...,   6.3498, -11.2941,  -2.6017],\n",
      "         [  8.1237,  -3.4223,  -2.3067,  ...,  -3.2990,  -2.6489,  14.2855],\n",
      "         [ -4.2845, -15.9449,  15.9741,  ...,  12.5905,  -7.1163,   4.0114],\n",
      "         ...,\n",
      "         [  8.9390,   4.1228,  -2.2486,  ...,  -8.7977,  -2.2187,  15.2977],\n",
      "         [  7.4371,  -7.8366,  11.5071,  ...,   4.8182,  -0.5835,  -0.9913],\n",
      "         [  8.6959,  -7.8347,  12.0263,  ...,   3.1032,   5.2980,  -3.5350]],\n",
      "\n",
      "        [[ -7.4203,  -4.3605,  11.4879,  ...,   9.2481, -12.9155, -11.7564],\n",
      "         [  1.8797,  -8.1115,   4.7858,  ...,  -1.0460,  -5.1580,  -3.3552],\n",
      "         [ -9.8451, -10.9397,  11.6114,  ...,  12.5991,  -0.9494, -11.6379],\n",
      "         ...,\n",
      "         [  4.8537,  -5.9702,   8.5831,  ...,   6.0102,   4.4059,  -5.1857],\n",
      "         [ -7.6684,  -3.9567,  -2.3275,  ...,   8.0081, -13.4653, -10.9434],\n",
      "         [ -2.0737,  -5.4410,  -9.7707,  ...,  -6.3346,  -1.1613,  -3.6599]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "x  tensor([[[ -2.8496,   3.0096,  10.6878,  ...,  10.9191, -11.6138, -10.0887],\n",
      "         [  6.2494,  -0.5510,   6.3921,  ...,   7.1750,   3.8960,  -5.6493],\n",
      "         [  8.2579,  12.2758,  -6.8234,  ...,  -4.4723,  -1.9153,  12.2202],\n",
      "         ...,\n",
      "         [  3.9618,  -2.3893,  -0.2389,  ...,  -2.0541,  -5.6996,   5.3782],\n",
      "         [ -8.6903,  -4.3222,   9.1932,  ...,  13.7640,   1.0869, -11.3624],\n",
      "         [ -2.4356,  -3.2588,   4.3218,  ...,  -1.5337,  -5.2310,   5.3986]],\n",
      "\n",
      "        [[ -2.3965,  -1.0162,  12.4134,  ...,   8.8612, -12.6999,  -5.8745],\n",
      "         [ -5.3848,  -7.8714,  -0.3702,  ...,  12.3492,  -2.7565,  -5.2586],\n",
      "         [ -5.7134,  -7.9167,  15.0927,  ...,  13.7574,  -1.2358, -12.4186],\n",
      "         ...,\n",
      "         [  8.3978,  -2.5567,   9.6394,  ...,   5.7296,   4.1594,  -5.6638],\n",
      "         [ 11.9443, -12.5816,   5.9880,  ...,  -3.3524,   3.5941,   6.7906],\n",
      "         [  7.9073,  -3.0411,  10.1242,  ...,  -0.5357,   4.4053,  -5.1044]],\n",
      "\n",
      "        [[ -1.2864,  -0.9366,  14.2820,  ...,   7.1492, -11.3072, -10.7532],\n",
      "         [ 14.0634,  -3.0156,   7.4772,  ...,  -6.0822,   3.6064,  -6.9207],\n",
      "         [  9.2060,  -6.3258,  10.2353,  ...,   5.5653,   4.1236,  -3.6635],\n",
      "         ...,\n",
      "         [ 10.0697,  -3.4087,   8.8716,  ...,   5.8228,   4.0696,  -4.8604],\n",
      "         [ -5.4538,  -0.6775,  -2.6627,  ...,  16.2687,  -8.6162,   9.9279],\n",
      "         [ -5.0626,  -4.8209,  -4.1887,  ...,  -5.7703,  -0.9150,  -3.8249]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[  0.3978,  -2.6968,  -2.0697,  ...,   8.9199, -13.9523,  -3.0972],\n",
      "         [  8.3692,  -5.8176,   6.6700,  ...,  -3.1491,  -3.6220,  -2.2655],\n",
      "         [  8.9557,  -6.4729,   7.4040,  ...,  -3.3512,  -4.8728,   6.2988],\n",
      "         ...,\n",
      "         [  1.4020, -12.7963,  15.8646,  ...,  14.7244,  -6.3026,   2.1073],\n",
      "         [  2.3302, -13.5774,  14.4448,  ...,   0.6086,  -6.5242,   0.9270],\n",
      "         [ -5.4022,  -1.1351,  -3.2743,  ...,  14.3808,  -6.8210,  10.9049]],\n",
      "\n",
      "        [[ -4.1556,  -3.6154,  14.8685,  ...,   6.3498, -11.2941,  -2.6017],\n",
      "         [  8.1237,  -3.4223,  -2.3067,  ...,  -3.2990,  -2.6489,  14.2855],\n",
      "         [ -4.2845, -15.9449,  15.9741,  ...,  12.5905,  -7.1163,   4.0114],\n",
      "         ...,\n",
      "         [  8.9390,   4.1228,  -2.2486,  ...,  -8.7977,  -2.2187,  15.2977],\n",
      "         [  7.4371,  -7.8366,  11.5071,  ...,   4.8182,  -0.5835,  -0.9913],\n",
      "         [  8.6959,  -7.8347,  12.0263,  ...,   3.1032,   5.2980,  -3.5350]],\n",
      "\n",
      "        [[ -7.4203,  -4.3605,  11.4879,  ...,   9.2481, -12.9155, -11.7564],\n",
      "         [  1.8797,  -8.1115,   4.7858,  ...,  -1.0460,  -5.1580,  -3.3552],\n",
      "         [ -9.8451, -10.9397,  11.6114,  ...,  12.5991,  -0.9494, -11.6379],\n",
      "         ...,\n",
      "         [  4.8537,  -5.9702,   8.5831,  ...,   6.0102,   4.4059,  -5.1857],\n",
      "         [ -7.6684,  -3.9567,  -2.3275,  ...,   8.0081, -13.4653, -10.9434],\n",
      "         [ -2.0737,  -5.4410,  -9.7707,  ...,  -6.3346,  -1.1613,  -3.6599]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "norm_1 tensor([[[ -31.1057,   78.9237,   95.9785,  ...,   87.6511,  -55.8363,\n",
      "           -99.1601],\n",
      "         [  47.7931,  -26.9231,   34.8427,  ...,   56.3171,   31.1413,\n",
      "           -70.0567],\n",
      "         [  88.3304,   95.1467,  -47.8305,  ...,  -39.0689,    3.5793,\n",
      "            60.8422],\n",
      "         ...,\n",
      "         [  30.3123,  -13.1269,    8.6764,  ...,   -1.1587,  -41.6741,\n",
      "            35.0024],\n",
      "         [ -81.5760,  -16.4220,   72.8920,  ...,  104.0318,   17.3251,\n",
      "          -126.3744],\n",
      "         [ -15.6658,   -5.0531,   34.3858,  ...,    4.1732,  -33.8844,\n",
      "            17.6124]],\n",
      "\n",
      "        [[ -30.0860,   -8.2310,   68.1951,  ...,   53.9937,  -85.9927,\n",
      "           -77.0469],\n",
      "         [ -54.2482,  -40.0657,   16.0451,  ...,   83.7166,   -8.4421,\n",
      "           -78.1227],\n",
      "         [ -68.1880,  -31.2585,  143.8387,  ...,  108.6070,    7.3375,\n",
      "          -128.4774],\n",
      "         ...,\n",
      "         [  30.4492,    2.6326,   58.5005,  ...,   40.3357,   48.6928,\n",
      "           -67.5307],\n",
      "         [  82.6692,  -89.4431,   54.6833,  ...,  -23.0911,   50.7634,\n",
      "            38.1945],\n",
      "         [  39.5929,   -2.0546,   98.5497,  ...,   -3.1901,   37.0010,\n",
      "           -56.7984]],\n",
      "\n",
      "        [[  -1.7939,   -1.5655,  124.6570,  ...,   43.0561,  -73.7954,\n",
      "          -115.5609],\n",
      "         [ 122.0350,  -17.5651,   69.2719,  ...,  -59.4096,   44.2147,\n",
      "           -85.4290],\n",
      "         [  69.9983,  -17.4512,   72.8397,  ...,   35.1174,   61.0654,\n",
      "           -49.0689],\n",
      "         ...,\n",
      "         [  56.3566,   -6.8299,   60.1159,  ...,   33.2002,   64.0981,\n",
      "           -56.2732],\n",
      "         [ -30.1387,   14.3906,  -22.2790,  ...,  139.0941,  -72.1080,\n",
      "            65.3145],\n",
      "         [ -34.1755,  -20.6364,  -24.1578,  ...,  -48.7449,  -12.9401,\n",
      "           -57.5244]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[  19.0872,  -53.1055,  -33.7232,  ...,   58.5995, -137.1981,\n",
      "           -32.1790],\n",
      "         [  43.5454,  -11.2236,   18.8726,  ...,  -43.4941,  -25.8856,\n",
      "           -47.3868],\n",
      "         [  48.1540,  -30.6699,   42.6312,  ...,  -31.0991,  -31.9483,\n",
      "            23.0525],\n",
      "         ...,\n",
      "         [  -8.5102, -119.2170,  159.9449,  ...,  144.0172,  -19.2202,\n",
      "            22.6320],\n",
      "         [   8.5714, -131.6220,  131.6407,  ...,   19.6970,  -43.1771,\n",
      "            -2.3119],\n",
      "         [ -57.3064,    8.0523,  -41.8839,  ...,   88.6693,  -31.5028,\n",
      "            79.1954]],\n",
      "\n",
      "        [[ -22.5654,   -2.6222,  146.1605,  ...,   51.2821,  -88.0593,\n",
      "           -65.1748],\n",
      "         [  64.6271,   -7.4957,  -25.6024,  ...,  -37.6731,  -27.3054,\n",
      "            99.0300],\n",
      "         [ -63.6245, -135.9475,  136.5740,  ...,  102.0789,  -45.4341,\n",
      "            -2.9132],\n",
      "         ...,\n",
      "         [  75.5544,   56.7018,  -25.2262,  ...,  -90.8387,  -25.1224,\n",
      "           112.0150],\n",
      "         [  84.4491,  -71.7420,  129.0876,  ...,   45.8134,  -17.7251,\n",
      "           -40.9753],\n",
      "         [  85.5566,  -45.4615,  116.6804,  ...,   15.2000,   44.1748,\n",
      "           -72.1149]],\n",
      "\n",
      "        [[ -35.8558,  -20.2520,  115.6346,  ...,   96.2562,  -97.9821,\n",
      "          -147.1347],\n",
      "         [  35.9235,  -19.4257,   19.3994,  ...,  -27.3035,  -37.7407,\n",
      "           -63.1921],\n",
      "         [ -48.2129,  -58.7901,  132.0291,  ...,  112.1844,   -7.4599,\n",
      "          -147.1936],\n",
      "         ...,\n",
      "         [  68.9639,  -25.3056,   56.4774,  ...,   51.4497,   75.4403,\n",
      "           -89.3704],\n",
      "         [ -34.6739,  -14.3751,  -20.5701,  ...,   81.2165, -105.6107,\n",
      "          -146.6348],\n",
      "         [   0.5389,  -38.8498,  -93.9513,  ...,  -54.4175,   -4.4436,\n",
      "           -50.8100]]], grad_fn=<AddBackward0>)\n",
      "alpha tensor([[[-1.8280e+03,  4.7146e+03,  6.4969e+03,  ...,  6.7978e+03,\n",
      "          -5.6343e+03, -6.7796e+03],\n",
      "         [ 2.6021e+03, -2.6239e+03,  2.3427e+03,  ...,  2.6895e+03,\n",
      "           1.4137e+03, -3.2643e+03],\n",
      "         [ 6.0117e+03,  6.0382e+03, -3.4145e+03,  ..., -3.1497e+03,\n",
      "          -1.6725e+03,  5.9148e+03],\n",
      "         ...,\n",
      "         [ 1.1523e+03, -1.0473e+03,  7.0778e+02,  ...,  3.1997e+02,\n",
      "          -2.3822e+03,  1.5164e+03],\n",
      "         [-6.5874e+03, -1.7306e+03,  4.8053e+03,  ...,  7.7592e+03,\n",
      "           1.0110e+03, -8.8910e+03],\n",
      "         [-6.8075e+02, -8.3029e+02,  2.0297e+03,  ...,  5.9998e+02,\n",
      "          -2.0122e+03,  8.1587e+02]],\n",
      "\n",
      "        [[-1.4080e+03, -1.4191e+03,  4.5905e+03,  ...,  3.6077e+03,\n",
      "          -7.0856e+03, -4.7551e+03],\n",
      "         [-3.9290e+03, -3.9049e+03,  1.6609e+03,  ...,  6.5597e+03,\n",
      "          -1.1098e+03, -4.8420e+03],\n",
      "         [-5.1429e+03, -3.4268e+03,  1.0546e+04,  ...,  8.9343e+03,\n",
      "          -6.0158e+02, -9.1555e+03],\n",
      "         ...,\n",
      "         [ 1.8927e+03, -9.7297e+02,  3.4380e+03,  ...,  2.2633e+03,\n",
      "           2.5574e+03, -2.8180e+03],\n",
      "         [ 7.8583e+03, -7.0547e+03,  6.6212e+03,  ..., -3.2442e+02,\n",
      "           2.1603e+03,  2.4052e+03],\n",
      "         [ 2.3137e+03, -1.5457e+03,  6.6141e+03,  ..., -9.9521e+01,\n",
      "           1.6502e+03, -2.2397e+03]],\n",
      "\n",
      "        [[-2.5288e+01,  5.4689e+02,  8.2890e+03,  ...,  2.6521e+03,\n",
      "          -6.1045e+03, -7.8375e+03],\n",
      "         [ 1.0978e+04, -2.1838e+03,  7.8448e+03,  ..., -3.4727e+03,\n",
      "           2.0598e+03, -8.0372e+03],\n",
      "         [ 3.6555e+03, -1.7652e+03,  4.4389e+03,  ...,  1.7439e+03,\n",
      "           2.7934e+03, -1.3831e+03],\n",
      "         ...,\n",
      "         [ 3.5605e+03, -1.1889e+03,  3.5454e+03,  ...,  2.0830e+03,\n",
      "           3.2450e+03, -2.3731e+03],\n",
      "         [-1.0960e+03, -4.4909e+02,  4.4574e+00,  ...,  9.8273e+03,\n",
      "          -5.5393e+03,  5.0561e+03],\n",
      "         [-2.0130e+03, -1.2330e+03, -6.6341e+02,  ..., -2.9743e+03,\n",
      "          -8.0909e+02, -3.4678e+03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.3860e+03, -5.4741e+03, -3.4364e+03,  ...,  4.7818e+03,\n",
      "          -1.6297e+04, -2.7310e+03],\n",
      "         [ 2.0422e+03, -9.6658e+02,  1.5246e+03,  ..., -1.8630e+03,\n",
      "          -1.2581e+03, -2.1282e+03],\n",
      "         [ 2.3692e+03, -2.2128e+03,  2.5217e+03,  ..., -1.4703e+03,\n",
      "          -1.7959e+03,  1.2300e+03],\n",
      "         ...,\n",
      "         [-9.2822e+02, -1.3129e+04,  1.8498e+04,  ...,  1.4480e+04,\n",
      "          -1.8172e+03,  3.9268e+03],\n",
      "         [ 2.1892e+03, -1.4674e+04,  1.3396e+04,  ...,  2.9432e+03,\n",
      "          -3.4590e+03,  5.2684e+02],\n",
      "         [-3.1501e+03, -3.6535e+02, -2.6419e+03,  ...,  5.9927e+03,\n",
      "          -2.8959e+03,  5.7847e+03]],\n",
      "\n",
      "        [[-6.5422e+02, -3.4077e+02,  1.1217e+04,  ...,  2.8896e+03,\n",
      "          -9.7610e+03, -5.5666e+03],\n",
      "         [ 3.7163e+03, -1.4331e+03, -1.8264e+03,  ..., -3.2299e+03,\n",
      "          -3.4179e+03,  7.2928e+03],\n",
      "         [-5.5305e+03, -1.4840e+04,  1.4950e+04,  ...,  8.8312e+03,\n",
      "          -3.8948e+03,  9.4568e+02],\n",
      "         ...,\n",
      "         [ 5.2748e+03,  4.0517e+03, -1.5983e+03,  ..., -9.0265e+03,\n",
      "          -4.6679e+03,  9.9904e+03],\n",
      "         [ 7.2003e+03, -7.0433e+03,  1.1098e+04,  ...,  2.2559e+03,\n",
      "          -2.5783e+03, -3.9538e+02],\n",
      "         [ 8.0492e+03, -5.4013e+03,  1.1989e+04,  ..., -1.7388e+03,\n",
      "           3.1428e+03, -4.8847e+03]],\n",
      "\n",
      "        [[-2.1375e+03, -3.6404e+03,  9.3340e+03,  ...,  8.2681e+03,\n",
      "          -1.1503e+04, -1.3428e+04],\n",
      "         [ 1.6230e+03, -2.3449e+03,  1.7333e+03,  ..., -1.6690e+03,\n",
      "          -2.2974e+03, -3.5463e+03],\n",
      "         [-4.7554e+03, -6.2000e+03,  1.0931e+04,  ...,  1.0376e+04,\n",
      "          -2.6518e+03, -1.2947e+04],\n",
      "         ...,\n",
      "         [ 5.2174e+03, -3.3277e+03,  5.1260e+03,  ...,  2.3623e+03,\n",
      "           5.1019e+03, -5.4657e+03],\n",
      "         [-1.7572e+03, -2.1956e+03, -2.4973e+03,  ...,  5.5321e+03,\n",
      "          -1.1456e+04, -1.2021e+04],\n",
      "         [-5.1322e+02, -3.0260e+03, -6.0019e+03,  ..., -3.9400e+03,\n",
      "          -1.0631e+03, -3.5960e+03]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "run_epoch(data_generation(vocab_size, 30, 20), model, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "mah = MultiHeadAttention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.rand(5, 10, d_model)\n",
    "k = torch.rand(5, 10, d_model)\n",
    "v = torch.rand(5, 10, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3094,  0.0192,  0.1885,  ...,  0.2809,  0.1203, -0.1954],\n",
       "         [-0.3081,  0.0196,  0.1878,  ...,  0.2825,  0.1199, -0.1942],\n",
       "         [-0.3095,  0.0203,  0.1880,  ...,  0.2813,  0.1213, -0.1940],\n",
       "         ...,\n",
       "         [-0.3083,  0.0193,  0.1872,  ...,  0.2823,  0.1200, -0.1944],\n",
       "         [-0.3101,  0.0210,  0.1881,  ...,  0.2816,  0.1207, -0.1940],\n",
       "         [-0.3092,  0.0209,  0.1863,  ...,  0.2812,  0.1212, -0.1941]],\n",
       "\n",
       "        [[-0.4010,  0.0192,  0.1289,  ...,  0.2678,  0.1162, -0.1459],\n",
       "         [-0.4009,  0.0195,  0.1298,  ...,  0.2670,  0.1171, -0.1450],\n",
       "         [-0.4010,  0.0190,  0.1286,  ...,  0.2680,  0.1162, -0.1451],\n",
       "         ...,\n",
       "         [-0.3988,  0.0197,  0.1286,  ...,  0.2680,  0.1168, -0.1469],\n",
       "         [-0.3995,  0.0194,  0.1286,  ...,  0.2676,  0.1172, -0.1450],\n",
       "         [-0.4004,  0.0189,  0.1288,  ...,  0.2686,  0.1178, -0.1457]],\n",
       "\n",
       "        [[-0.3997,  0.0504,  0.1788,  ...,  0.1710,  0.1605, -0.1597],\n",
       "         [-0.3989,  0.0492,  0.1800,  ...,  0.1740,  0.1615, -0.1585],\n",
       "         [-0.3988,  0.0509,  0.1790,  ...,  0.1726,  0.1617, -0.1589],\n",
       "         ...,\n",
       "         [-0.3978,  0.0516,  0.1795,  ...,  0.1729,  0.1601, -0.1591],\n",
       "         [-0.3981,  0.0503,  0.1782,  ...,  0.1737,  0.1608, -0.1585],\n",
       "         [-0.4003,  0.0517,  0.1796,  ...,  0.1725,  0.1611, -0.1597]],\n",
       "\n",
       "        [[-0.3594,  0.0287,  0.1976,  ...,  0.2647,  0.1217, -0.2345],\n",
       "         [-0.3576,  0.0299,  0.1974,  ...,  0.2641,  0.1244, -0.2355],\n",
       "         [-0.3572,  0.0281,  0.1985,  ...,  0.2627,  0.1219, -0.2345],\n",
       "         ...,\n",
       "         [-0.3585,  0.0275,  0.1995,  ...,  0.2639,  0.1245, -0.2353],\n",
       "         [-0.3589,  0.0308,  0.1977,  ...,  0.2636,  0.1213, -0.2357],\n",
       "         [-0.3593,  0.0289,  0.1983,  ...,  0.2632,  0.1219, -0.2363]],\n",
       "\n",
       "        [[-0.3278,  0.0260,  0.2093,  ...,  0.2247,  0.0899, -0.1595],\n",
       "         [-0.3287,  0.0256,  0.2079,  ...,  0.2257,  0.0904, -0.1591],\n",
       "         [-0.3279,  0.0259,  0.2080,  ...,  0.2251,  0.0905, -0.1608],\n",
       "         ...,\n",
       "         [-0.3275,  0.0247,  0.2085,  ...,  0.2254,  0.0901, -0.1595],\n",
       "         [-0.3270,  0.0264,  0.2085,  ...,  0.2248,  0.0900, -0.1598],\n",
       "         [-0.3282,  0.0253,  0.2092,  ...,  0.2253,  0.0913, -0.1611]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mah(q,k,v,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
