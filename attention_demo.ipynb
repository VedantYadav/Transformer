{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arch units\n",
    "## Self Attention Unit\n",
    "## Multi Head Attention\n",
    "## Encode Decode Unit\n",
    "## Norm + Residual Layer\n",
    "## Feed Forward\n",
    "## Input Positional Encoding \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters from paper\n",
    "N = 2 # 6\n",
    "d_model = 512\n",
    "h = 8\n",
    "d_k = d_v = d_model//h\n",
    "d_ff = 2048 #128\n",
    "vocab_size = 11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        self.W_O = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask, dropout=None):\n",
    "        b, q_seq, _ = Q.size()\n",
    "        b, k_seq, _ = K.size()\n",
    "        query = self.W_Q(Q).view(b, q_seq, h, d_k) # view (b, q_seq, h, d)\n",
    "        key = self.W_K(K).view(b, k_seq, h, d_k) # view (b, k_seq, h, d)\n",
    "        value  = self.W_V(V).view(b, k_seq, h, d_k) # view (b, k_seq, h, d)\n",
    "        \n",
    "        query = query.transpose(1, 2).contiguous() # view (b, h, q_seq, d)\n",
    "        key = key.transpose(1, 2).contiguous() # view (b, h, k_seq, d)\n",
    "        value = value.transpose(1, 2).contiguous() # view (b, h, k_seq, d)\n",
    "        \n",
    "        qk = query.matmul(key.transpose(-2,-1))\n",
    "        scale_qk = qk/(math.sqrt(d_k)) # shape (b, h, q_seq, k_seq)\n",
    "        \n",
    "        if mask is not None: # mask size (b, 1, k_seq)\n",
    "            mask = mask.unsqueeze(1) # mask size (b, 1, 1, k_seq)\n",
    "            scale_qk = scale_qk.masked_fill(mask==0, 1e-9)\n",
    "        \n",
    "        softmax_qk = nn.functional.softmax(scale_qk, dim=-1) # (b, h, q_seq, k_seq)\n",
    "        if dropout is not None:\n",
    "            softmax_qk = self.dropout(softmax_qk)\n",
    "        weighted_value = softmax_qk.matmul(value) # (b, h, q_seq, d)\n",
    "        return self.W_O(weighted_value.transpose(2,1).contiguous().view(b, q_seq, h*d_k)) # (b, h, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_mod=d_model):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.d_mod = d_mod\n",
    "        # https://stackoverflow.com/questions/39095252/fail-to-implement-layer-normalization-with-keras\n",
    "        # https://stackoverflow.com/questions/50935345/understanding-torch-nn-parameter\n",
    "        self.alpha = nn.Parameter(torch.ones(d_mod))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_mod))\n",
    "    def forward(self, x, eps=1e-6):\n",
    "        u = x.mean(-1, keepdim=True)\n",
    "        sigma = x.std(-1, keepdim=True)\n",
    "        return self.alpha * (x - u)/(sigma + eps) + self.beta\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCell(nn.Module):\n",
    "    def __init__(self, dropout=0.1, Adropout=0.1):\n",
    "        super(EncoderCell, self).__init__()\n",
    "        self.attn = MultiHeadAttention(Adropout)\n",
    "        self.norm_1 = LayerNorm()\n",
    "        self.pff = nn.Sequential(\n",
    "                    nn.Linear(d_model, d_ff),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                    nn.Linear(d_ff, d_model))\n",
    "        self.norm_2 = LayerNorm()\n",
    "    \n",
    "    def forward(self, x, src_mask=None):\n",
    "        x_norm_1 = self.norm_1(x + self.attn(x, x, x, src_mask))  # Layer 1\n",
    "        return self.norm_2(x_norm_1 + self.pff(x_norm_1)) # Layer 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderCell(nn.Module):\n",
    "    def __init__(self, dropout=0.1, Ddropout=0.1):\n",
    "        super(DecoderCell, self).__init__()\n",
    "        self.attn = MultiHeadAttention(Ddropout)\n",
    "        self.norm_1 = LayerNorm()\n",
    "        self.attn = MultiHeadAttention(Ddropout)\n",
    "        self.norm_2 = LayerNorm()\n",
    "        self.pff = nn.Sequential(\n",
    "                    nn.Linear(d_model, d_ff),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                    nn.Linear(d_ff, d_model))\n",
    "        self.norm_3 = LayerNorm()\n",
    "        \n",
    "    def forward(self, x, enc, src_mask=None, trg_mask=None):\n",
    "        x_norm_1 = self.norm_1(x + self.attn(x, x, x, trg_mask))\n",
    "        x_norm_2 = self.norm_2(x_norm_1 + self.attn(x_norm_1, enc, enc, src_mask))\n",
    "        return self.norm_3(x_norm_2 + self.pff(x_norm_2)) # (b, seq, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderStack(nn.Module):\n",
    "    def __init__(self, N, Edropout=0.1, Adropout=0.1):\n",
    "        super(EncoderStack, self).__init__()\n",
    "        self.N = N\n",
    "        self.Edropout = Edropout\n",
    "        self.Adropout = Adropout\n",
    "        self.encoders = nn.ModuleList([EncoderCell(self.Edropout, self.Adropout) \\\n",
    "                                       for _ in range(self.N)])\n",
    " \n",
    "    def forward(self, x, src_mask):\n",
    "        for enc in self.encoders:\n",
    "            x = enc(x, src_mask)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderStack(nn.Module):\n",
    "    def __init__(self, N, Ddropout=0.1, Adropout=0.1):\n",
    "        super(DecoderStack, self).__init__()\n",
    "        self.N = N\n",
    "        self.Ddropout = Ddropout\n",
    "        self.Adropout = Adropout\n",
    "        self.decoders = nn.ModuleList([DecoderCell(self.Ddropout, self.Adropout) \\\n",
    "                                       for _ in range(self.N)])\n",
    "        \n",
    "    def forward(self, x, enc, src_mask, trg_mask):\n",
    "        for decdr in self.decoders:\n",
    "            x = decdr(x, enc, src_mask, trg_mask)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dpout=0.1, max_seq=50):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dpout)\n",
    "        \n",
    "        pe_matx = torch.zeros(max_seq, d_model, requires_grad=False)\n",
    "        position = torch.arange(0, max_seq, dtype=torch.float).unsqueeze(-1)\n",
    "        w_t = torch.exp(torch.arange(0, d_model, 2).float() * -math.log(10000)/d_model)\n",
    "        val = position * w_t\n",
    "        pe_matx[:, 0::2] = torch.sin(val)\n",
    "        pe_matx[:, 1::2] = torch.cos(val)\n",
    "        pe_matx = pe_matx.unsqueeze(1)\n",
    "        self.register_buffer(\"pe_matx\", pe_matx)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x - (batch, seq, emb), pe_matrix - (max_seq, 1, d_model)\n",
    "        x += self.pe_matx[:x.size(0), :]\n",
    "        return(self.dropout(x))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, embedd = True, dropout=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "#         self.W_in = nn.Linear(word_emb_dim, d_model)\n",
    "        self.encoderStack = EncoderStack(N, Adropout=dropout)\n",
    "        self.decoderStack = DecoderStack(N, Adropout=dropout)\n",
    "        # https://stats.stackexchange.com/questions/392213/understand-the-output-layer-of-transformer\n",
    "        self.W_out = nn.Linear(d_model, vocab_size)\n",
    "        self.embedd = embedd\n",
    "        if self.embedd:\n",
    "            embed_x = EmbeddingLayer(vocab_size, d_model)\n",
    "            embed_y = EmbeddingLayer(vocab_size, d_model)\n",
    "            pe_x = PositionalEncoding(d_model)\n",
    "            pe_y = copy.deepcopy(pe_x)\n",
    "            self.enc_x = nn.Sequential(embed_x, pe_x)\n",
    "            self.enc_y = nn.Sequential(embed_y, pe_y)\n",
    "        \n",
    "    def forward(self, inp_x, inp_y, src_mask, trg_mask, sftmx=True):\n",
    "        \n",
    "        dec_x = self.decoder(inp_y, self.encoder(inp_x, src_mask), \n",
    "                             src_mask, trg_mask)\n",
    "        \n",
    "        if sftmx:\n",
    "            return nn.functional.log_softmax(self.W_out(dec_x), dim=-1)\n",
    "        return self.W_out(dec_x)\n",
    "    \n",
    "    def encoder(self, inp_x, src_mask):\n",
    "        if self.embedd:\n",
    "            inp_x = self.enc_x(inp_x)\n",
    "        return self.encoderStack(inp_x, src_mask)\n",
    "    \n",
    "    def decoder(self, inp_y, enc_x, src_mask, trg_mask):\n",
    "        if self.embedd:\n",
    "            inp_y = self.enc_y(inp_y)\n",
    "        return self.decoderStack(inp_y, enc_x, src_mask, trg_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.reddit.com/r/MachineLearning/comments/bjgpt2/d_confused_about_using_masking_in_transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
    "class Batch:\n",
    "    def __init__(self, src, trg=None, pad=0): # size src, trg (b, seq)\n",
    "        self.src = src \n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        if trg is not None:\n",
    "            self.trg = trg[:,:-1] # size (b,0:seq-1)\n",
    "            self.trg_y = trg[:,1:] # size (b,1:seq)\n",
    "            self.trg_mask = self.std_mask(self.trg, pad)\n",
    "            self.ntokens = (self.trg_y != pad).data.sum() # size (1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def std_mask(tgt, pad):\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2) # size (b, 1, seq)\n",
    "        mask = torch.from_numpy(np.triu(np.ones((1,tgt.shape[-1],tgt.shape[-1])), k=1).astype('uint8')) == 0\n",
    "        return tgt_mask & mask # size (b, 1, seq) * (1, seq, seq) -> (b, seq, seq) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vedantyadav/opt/anaconda3/envs/nmt/lib/python3.5/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/pytorch/pytorch/issues/7455    \n",
    "kldivLoss = nn.KLDivLoss(size_average=False)\n",
    "# CELoss = nn.CrossEntropyLoss()\n",
    "\n",
    "def labelSmoothingLoss(x, y, epsilon=0.0, padding_value=0, cls=2, d=-1):\n",
    "    # concat x, y batch as index_fill_ don't support vector dim > 1\n",
    "#     x = x.view(-1, x.size(-1))    \n",
    "    x=x.contiguous().view(-1, x.size(-1))\n",
    "    y=y.contiguous().view(-1)\n",
    "    \n",
    "    x_ = x.data.clone()\n",
    "    x_.fill_(epsilon / (x_.size(-1) - cls))\n",
    "    x_.scatter_(d, y.data.unsqueeze(-1), (1 - epsilon))\n",
    "    x_[:, padding_value] = 0\n",
    "    mask = torch.nonzero(y.data == padding_value)\n",
    "    if mask.dim() > 0:\n",
    "        x_.index_fill_(0, mask.squeeze(), 0.0)\n",
    "#     print(x, x_)\n",
    "#     return CELoss(x, x_, )\n",
    "#     return torch.mean(torch.sum(-x_*x), dim=d) # x_ is true distribution and x is prediction\n",
    "    return kldivLoss(x, copy.deepcopy(x_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://nlp.seas.harvard.edu/2018/04/03/attention.html#synthetic-data\n",
    "def data_generation(V, batch, nbatches):\n",
    "    for i in range(nbatches):\n",
    "        data = torch.randint(1, V, size=(batch, 10))\n",
    "        data[:, 0] = 1\n",
    "        src = data.clone().detach()\n",
    "        trg = data.clone().detach()\n",
    "        yield Batch(src, trg, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://nlp.seas.harvard.edu/2018/04/03/attention.html#synthetic-data\n",
    "def run_epoch(data_itr, model, opt):\n",
    "    start = time.time()\n",
    "    total_token = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    \n",
    "    for i, batch in enumerate(data_itr):\n",
    "        opt.optimizer.zero_grad()\n",
    "        \n",
    "        outp = model(batch.src, batch.trg, batch.src_mask, batch.trg_mask)\n",
    "        loss = labelSmoothingLoss(outp, batch.trg_y)\n",
    "#         loss = loss/batch.ntokens\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "#         loss.data = loss.data * batch.ntokens\n",
    "\n",
    "        opt.step()\n",
    "        \n",
    "        total_loss+=loss\n",
    "        total_token+=batch.ntokens\n",
    "        tokens+=batch.ntokens\n",
    "        \n",
    "        if i%50 == 1:\n",
    "            elapsed = time.time() - start\n",
    "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
    "                    (i, loss / batch.ntokens, tokens / elapsed))\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "    return total_loss/total_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "            (self.model_size ** (-0.5) *\n",
    "            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init all parameters as we used deepcopy to save computation tym\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.Adam(model.parameters())\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "# https://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
    "model_opt = NoamOpt(d_model, 1, 400,\n",
    "        torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Step: 1 Loss: 3.009474 Tokens per Sec: 716.724121\n",
      "Epoch Step: 1 Loss: 1.846428 Tokens per Sec: 808.009216\n",
      "tensor(1.8439)\n",
      "Epoch Step: 1 Loss: 1.788058 Tokens per Sec: 755.007202\n",
      "Epoch Step: 1 Loss: 1.551715 Tokens per Sec: 807.668640\n",
      "tensor(1.5134)\n",
      "Epoch Step: 1 Loss: 1.517454 Tokens per Sec: 729.454773\n",
      "Epoch Step: 1 Loss: 1.463484 Tokens per Sec: 810.447144\n",
      "tensor(1.4462)\n",
      "Epoch Step: 1 Loss: 1.507231 Tokens per Sec: 758.847900\n",
      "Epoch Step: 1 Loss: 0.941215 Tokens per Sec: 797.798157\n",
      "tensor(0.9294)\n",
      "Epoch Step: 1 Loss: 0.945399 Tokens per Sec: 694.597107\n",
      "Epoch Step: 1 Loss: 0.927456 Tokens per Sec: 805.195007\n",
      "tensor(0.8918)\n",
      "Epoch Step: 1 Loss: 0.788258 Tokens per Sec: 752.071228\n",
      "Epoch Step: 1 Loss: 0.821191 Tokens per Sec: 764.545532\n",
      "tensor(0.8560)\n",
      "Epoch Step: 1 Loss: 0.765289 Tokens per Sec: 614.856995\n",
      "Epoch Step: 1 Loss: 0.754950 Tokens per Sec: 803.661316\n",
      "tensor(0.6905)\n",
      "Epoch Step: 1 Loss: 0.657492 Tokens per Sec: 759.692383\n",
      "Epoch Step: 1 Loss: 0.782052 Tokens per Sec: 802.910645\n",
      "tensor(0.7134)\n",
      "Epoch Step: 1 Loss: 0.848361 Tokens per Sec: 742.392395\n",
      "Epoch Step: 1 Loss: 0.603192 Tokens per Sec: 773.546021\n",
      "tensor(0.5805)\n",
      "Epoch Step: 1 Loss: 0.673964 Tokens per Sec: 712.214966\n",
      "Epoch Step: 1 Loss: 0.661500 Tokens per Sec: 719.338196\n",
      "tensor(0.6535)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    run_epoch(data_generation(vocab_size, 30, 20), model, model_opt)\n",
    "    model.eval()\n",
    "    print(run_epoch(data_generation(vocab_size, 30, 5), model, model_opt).data)\n",
    "    \n",
    "#     scheduler.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_epoch(data_generation(vocab_size, 30, 20), model, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    for i in range(max_len-1):\n",
    "        out = model.decode(memory, src_mask, \n",
    "                           Variable(ys), \n",
    "                           Variable(subsequent_mask(ys.size(1))\n",
    "                                    .type_as(src.data)))\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim = 1)\n",
    "        next_word = next_word.data[0]\n",
    "        ys = torch.cat([ys, \n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "    return ys\n",
    "\n",
    "model.eval()\n",
    "src = torch.tensor(torch.LongTensor([[1,2,3,4,5,6,7,8,9,10]]) )\n",
    "src_mask = torch.tensor(torch.ones(1, 1, 10) )\n",
    "print(greedy_decode(model, src, src_mask, max_len=10, start_symbol=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
