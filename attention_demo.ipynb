{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arch units\n",
    "## Self Attention Unit\n",
    "## Multi Head Attention\n",
    "## Encode Decode Unit\n",
    "## Norm + Residual Layer\n",
    "## Feed Forward\n",
    "## Input Positional Encoding \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters from paper\n",
    "word_emb_dim = 50\n",
    "N = 6\n",
    "d_model = 32 # 512\n",
    "h = 4 # 8\n",
    "d_k = d_v = d_model//h\n",
    "d_ff = 128 # 2048\n",
    "vocab_size = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        assert Q.shape[-1] == K.shape[-1] == V.shape[-1]\n",
    "        qk = torch.bmm(Q, K.transpose(-2, -1)) # Q & K size (b, seq, d_k)\n",
    "        scale_qk = qk/math.sqrt(d_k) # size (b, seq, seq)\n",
    "        if mask:\n",
    "            scale_qk = scale_qk.masked_fill(mask==0, 1e-9) # where mask is True replaced with 0\n",
    "        softmax_qk = nn.functional.softmax(scale_qk, dim=-1) # size (b, seq, seq)\n",
    "        return torch.bmm(softmax_qk, V) # size (b, seq, d_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k)\n",
    "        self.W_K = nn.Linear(d_model, d_k)\n",
    "        self.W_V = nn.Linear(d_model, d_k)\n",
    "        self.attn = Attention()\n",
    "        self.W_O = nn.Linear(h*d_v, d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        head = None\n",
    "        for _ in range(h):\n",
    "            attn_head = self.attn(self.W_Q(Q), self.W_K(K), self.W_V(V), mask=None)\n",
    "            if head != None:\n",
    "                head = torch.cat((head, attn_head), dim=-1)\n",
    "            else:\n",
    "                head = attn_head\n",
    "        # head size (b, seq, d_k*h)\n",
    "        return self.W_O(head) # size (b, seq, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_mod=d_model):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.d_mod = d_mod\n",
    "        # https://stackoverflow.com/questions/39095252/fail-to-implement-layer-normalization-with-keras\n",
    "        # https://stackoverflow.com/questions/50935345/understanding-torch-nn-parameter\n",
    "        self.alpha = nn.Parameter(torch.ones(d_mod))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_mod))\n",
    "    def forward(self, x, eps=1e-6):\n",
    "        u = x.mean(-1, keepdim=True)\n",
    "        sigma = x.std(-1, keepdim=True)\n",
    "        return self.alpha * (x - u)/(1/(sigma + eps)) + self.beta\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCell(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderCell, self).__init__()\n",
    "        self.attn = MultiHeadAttention()\n",
    "        self.norm_1 = LayerNorm()\n",
    "        self.pff = nn.Sequential(\n",
    "                    nn.Linear(d_model, d_ff),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(d_ff, d_model))\n",
    "        self.norm_2 = LayerNorm()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_norm_1 = self.norm_1(x + self.attn(x, x, x))  # Layer 1\n",
    "        return self.norm_2(x_norm_1 + self.pff(x_norm_1)) # Layer 2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderCell(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecoderCell, self).__init__()\n",
    "        self.attn = MultiHeadAttention()\n",
    "        self.norm_1 = LayerNorm()\n",
    "        self.attn = MultiHeadAttention()\n",
    "        self.norm_2 = LayerNorm()\n",
    "        self.pff = nn.Sequential(\n",
    "                    nn.Linear(d_model, d_ff),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(d_ff, d_model))\n",
    "        self.norm_3 = LayerNorm()\n",
    "        \n",
    "    def forward(self, x, enc, src_mask=None, trg_mask=None):\n",
    "        x_norm_1 = self.norm_1(x + self.attn(x, x, x, trg_mask))\n",
    "        x_norm_2 = self.norm_2(x_norm_1 + self.attn(x_norm_1, enc, enc, src_mask))\n",
    "        return self.norm_3(x_norm_2 + self.pff(x_norm_2)) # (b, seq, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.W_in = nn.Linear(word_emb_dim, d_model)\n",
    "        # based on N values\n",
    "        self.encoder_unit = EncoderCell()\n",
    "        self.decoder_unit = DecoderCell()\n",
    "        # https://stats.stackexchange.com/questions/392213/understand-the-output-layer-of-transformer\n",
    "        self.W_out = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, inp_x, inp_y):\n",
    "        inp_x, inp_y = inp_x/math.sqrt(d_model), inp_y/math.sqrt(d_model)\n",
    "        inp_x, inp_y = self.W_in(inp_x), self.W_in(inp_y) # (b, seq, word_embedding) -> (b, seq, d_model)\n",
    "        enc_x = self.encoder_unit(inp_x)\n",
    "        dec_x = self.decoder_unit(inp_y, enc_x)\n",
    "        return self.W_out(dec_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, word_emb_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.reddit.com/r/MachineLearning/comments/bjgpt2/d_confused_about_using_masking_in_transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
    "class Batch:\n",
    "    def __init__(self, src, trg=None, pad=0): # size src, trg (b, seq)\n",
    "        self.src = src \n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        if trg is not None:\n",
    "            self.trg = trg[:,:-1] # size (b,0:seq-1)\n",
    "            self.trg_y = trg[:,1:] # size (b,1:seq)\n",
    "            self.trg_mask = self.std_mask(self.trg, pad)\n",
    "            self.ntokens = (self.trg_y != pad).data.sum() # size (1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def std_mask(tgt, pad):\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2) # size (b, 1, seq)\n",
    "        return tgt_mask & subsequent_mask(tgt.shape[-1]) # size (b, 1, seq) * (1, seq, seq) -> (b, seq, seq) \n",
    "    \n",
    "    @staticmethod\n",
    "    def subsequent_mask(size):\n",
    "        return torch.from_numpy(np.triu(np.ones((1,size,size)), k=1).astype('uint8')) == 0 # size (1, seq, seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 5])"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2.2052e-01, 8.3386e-01, 2.0095e-01, 1.0000e-09, 3.4726e-02],\n",
       "         [4.5698e-01, 9.6899e-01, 5.4661e-01, 1.0000e-09, 5.8170e-01],\n",
       "         [7.3435e-01, 4.7575e-01, 6.9755e-01, 1.0000e-09, 6.0524e-01],\n",
       "         [8.3406e-01, 4.3392e-01, 7.2352e-01, 1.0000e-09, 4.2996e-02],\n",
       "         [3.6918e-01, 3.2248e-01, 6.3152e-01, 1.0000e-09, 2.7349e-01]],\n",
       "\n",
       "        [[8.4413e-01, 7.4469e-01, 8.5517e-01, 2.1864e-01, 3.6536e-01],\n",
       "         [7.1498e-02, 3.2991e-01, 6.4716e-01, 2.0532e-01, 9.3745e-01],\n",
       "         [9.9492e-01, 6.3234e-01, 5.1108e-01, 6.3176e-01, 4.2131e-01],\n",
       "         [4.9687e-01, 2.7079e-01, 7.6468e-01, 4.8283e-01, 2.2096e-01],\n",
       "         [5.8373e-01, 5.5180e-01, 5.6868e-03, 6.8920e-01, 5.6148e-02]],\n",
       "\n",
       "        [[3.7395e-02, 9.0728e-01, 1.7958e-01, 2.0664e-01, 2.6232e-01],\n",
       "         [4.3243e-01, 5.1016e-01, 4.2045e-01, 2.6085e-01, 2.8659e-02],\n",
       "         [3.8285e-01, 3.2779e-01, 7.3841e-01, 5.9082e-01, 2.3451e-01],\n",
       "         [9.3807e-01, 6.3969e-01, 9.0029e-01, 9.3782e-01, 6.3724e-01],\n",
       "         [6.2480e-01, 5.6339e-01, 9.1050e-01, 6.5331e-01, 1.2923e-01]]])"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn.masked_fill(b.src_mask==0, 1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "bool value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-273-44ccff6d5e5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: bool value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
