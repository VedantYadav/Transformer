{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arch units\n",
    "## Self Attention Unit\n",
    "## Multi Head Attention\n",
    "## Encode Decode Unit\n",
    "## Norm + Residual Layer\n",
    "## Feed Forward\n",
    "## Input Positional Encoding \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters from paper\n",
    "word_emb_dim = 50\n",
    "N = 6\n",
    "d_model = 32 # 512\n",
    "h = 4 # 8\n",
    "d_k = d_v = d_model//h\n",
    "d_ff = 128 # 2048\n",
    "vocab_size = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k)\n",
    "        self.W_K = nn.Linear(d_model, d_k)\n",
    "        self.W_V = nn.Linear(d_model, d_k)\n",
    "        self.W_O = nn.Linear(h*d_v, d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        head = None\n",
    "        for _ in range(h):\n",
    "            attn_head = attention(self.W_Q(Q), self.W_K(K), self.W_V(V), mask=None)\n",
    "            if head != None:\n",
    "                head = torch.cat((head, attn_head), dim=-1)\n",
    "            else:\n",
    "                head = attn_head\n",
    "        # head size (b, seq, d_k*h)\n",
    "        return self.W_O(head) # size (b, seq, d_model)\n",
    "\n",
    "    def attention(self, Q, K, V, mask=None):\n",
    "        assert Q.shape[-1] == K.shape[-1] or K.shape[-2] == V.shape[-2]\n",
    "        qk = torch.matmul(Q, K.transpose(-2, -1)) # Q & K size (b, seq, d_k)\n",
    "        scale_qk = qk/math.sqrt(d_k) # size (b, seq, seq)\n",
    "        if mask:\n",
    "            scale_qk = scale_qk.masked_fill(mask==0, 1e-9) # where mask is True replaced with 0\n",
    "        softmax_qk = nn.functional.softmax(scale_qk, dim=-1) # size (b, seq, seq)\n",
    "        return torch.matmul(softmax_qk, V) # size (b, seq, d_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_mod=d_model):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.d_mod = d_mod\n",
    "        # https://stackoverflow.com/questions/39095252/fail-to-implement-layer-normalization-with-keras\n",
    "        # https://stackoverflow.com/questions/50935345/understanding-torch-nn-parameter\n",
    "        self.alpha = nn.Parameter(torch.ones(d_mod))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_mod))\n",
    "    def forward(self, x, eps=1e-6):\n",
    "        u = x.mean(-1, keepdim=True)\n",
    "        sigma = x.std(-1, keepdim=True)\n",
    "        return self.alpha * (x - u)/(1/(sigma + eps)) + self.beta\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCell(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderCell, self).__init__()\n",
    "        self.attn = MultiHeadAttention()\n",
    "        self.norm_1 = LayerNorm()\n",
    "        self.pff = nn.Sequential(\n",
    "                    nn.Linear(d_model, d_ff),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(d_ff, d_model))\n",
    "        self.norm_2 = LayerNorm()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_norm_1 = self.norm_1(x + self.attn(x, x, x))  # Layer 1\n",
    "        return self.norm_2(x_norm_1 + self.pff(x_norm_1)) # Layer 2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderCell(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecoderCell, self).__init__()\n",
    "        self.attn = MultiHeadAttention()\n",
    "        self.norm_1 = LayerNorm()\n",
    "        self.attn = MultiHeadAttention()\n",
    "        self.norm_2 = LayerNorm()\n",
    "        self.pff = nn.Sequential(\n",
    "                    nn.Linear(d_model, d_ff),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(d_ff, d_model))\n",
    "        self.norm_3 = LayerNorm()\n",
    "        \n",
    "    def forward(self, x, enc, src_mask=None, trg_mask=None):\n",
    "        x_norm_1 = self.norm_1(x + self.attn(x, x, x, trg_mask))\n",
    "        x_norm_2 = self.norm_2(x_norm_1 + self.attn(x_norm_1, enc, enc, src_mask))\n",
    "        return self.norm_3(x_norm_2 + self.pff(x_norm_2)) # (b, seq, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.W_in = nn.Linear(word_emb_dim, d_model)\n",
    "        # based on N values\n",
    "        self.encoder_unit = EncoderCell()\n",
    "        self.decoder_unit = DecoderCell()\n",
    "        # https://stats.stackexchange.com/questions/392213/understand-the-output-layer-of-transformer\n",
    "        self.W_out = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, inp_x, inp_y):\n",
    "        inp_x, inp_y = inp_x/math.sqrt(d_model), inp_y/math.sqrt(d_model)\n",
    "        inp_x, inp_y = self.W_in(inp_x), self.W_in(inp_y) # (b, seq, word_embedding) -> (b, seq, d_model)\n",
    "        enc_x = self.encoder_unit(inp_x)\n",
    "        dec_x = self.decoder_unit(inp_y, enc_x)\n",
    "        return self.W_out(dec_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, word_emb_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.reddit.com/r/MachineLearning/comments/bjgpt2/d_confused_about_using_masking_in_transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
    "class Batch:\n",
    "    def __init__(self, src, trg=None, pad=0): # size src, trg (b, seq)\n",
    "        self.src = src \n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        if trg is not None:\n",
    "            self.trg = trg[:,:-1] # size (b,0:seq-1)\n",
    "            self.trg_y = trg[:,1:] # size (b,1:seq)\n",
    "            self.trg_mask = self.std_mask(self.trg, pad)\n",
    "            self.ntokens = (self.trg_y != pad).data.sum() # size (1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def std_mask(tgt, pad):\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2) # size (b, 1, seq)\n",
    "        return tgt_mask & subsequent_mask(tgt.shape[-1]) # size (b, 1, seq) * (1, seq, seq) -> (b, seq, seq) \n",
    "    \n",
    "    @staticmethod\n",
    "    def subsequent_mask(size):\n",
    "        return torch.from_numpy(np.triu(np.ones((1,size,size)), k=1).astype('uint8')) == 0 # size (1, seq, seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn.masked_fill(b.src_mask==0, 1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randint(0,4,(3,4,5)) * 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2., 2., 3., 3., 3.],\n",
       "         [0., 2., 0., 1., 3.],\n",
       "         [1., 0., 0., 3., 3.],\n",
       "         [0., 1., 2., 0., 3.]],\n",
       "\n",
       "        [[2., 2., 1., 2., 2.],\n",
       "         [1., 3., 2., 0., 3.],\n",
       "         [2., 3., 3., 1., 2.],\n",
       "         [3., 2., 1., 3., 0.]],\n",
       "\n",
       "        [[3., 1., 3., 2., 2.],\n",
       "         [0., 2., 2., 0., 3.],\n",
       "         [3., 3., 1., 1., 1.],\n",
       "         [2., 1., 0., 0., 1.]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = nn.Linear(5, 5 * h, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = l1(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2 = a1.view(3,4,h,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 4, 5])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2.transpose(1,2).conti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randint(0,4,(3,4,2,5,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.randint(0,4,(3,4,2,6,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 2, 5, 4])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.matmul(b).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
